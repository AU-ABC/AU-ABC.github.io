[
  {
    "objectID": "news/past/2024-08-20-session3.html",
    "href": "news/past/2024-08-20-session3.html",
    "title": "ABC.3: Third ABC session",
    "section": "",
    "text": "The third session of the ABC (Accessible Bioinformatics Cafe) will be on august 20th, at 12:30 in the hall of AIAS (the Aarhus Institute of Advanced Studies, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-08-20-session3.html#agenda",
    "href": "news/past/2024-08-20-session3.html#agenda",
    "title": "ABC.3: Third ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. This time we have\n\nR analysis of bulkRNA sequencing data"
  },
  {
    "objectID": "news/past/2024-08-20-session3.html#signing-up",
    "href": "news/past/2024-08-20-session3.html#signing-up",
    "title": "ABC.3: Third ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2024-10-03-session6.html",
    "href": "news/past/2024-10-03-session6.html",
    "title": "ABC.6: Sixth ABC session",
    "section": "",
    "text": "The fourth session of the ABC (Accessible Bioinformatics Cafe) will be on october 3rd, at 13:00 in building 1873 room 118A (University city Molecular Biology Building, room by the canteen, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-10-03-session6.html#agenda",
    "href": "news/past/2024-10-03-session6.html#agenda",
    "title": "ABC.6: Sixth ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. This time we have\n\ndownload of open datasets from online databases\n\nYou can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/past/2024-10-03-session6.html#signing-up",
    "href": "news/past/2024-10-03-session6.html#signing-up",
    "title": "ABC.6: Sixth ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2024-06-13-session1.html",
    "href": "news/past/2024-06-13-session1.html",
    "title": "ABC.1: First ABC session",
    "section": "",
    "text": "The very first session of the ABC (Accessible Bioinformatics Cafe) will launch on june 13th, at 13:00 in the hall of AIAS (the Aarhus Institute of Advanced Studies, see below).\nEveryone is welcome to join independently of coding skills and level. In this first meeting we will also distribute a survey, so that participants can give some ideas about expectations and topics of interest."
  },
  {
    "objectID": "news/past/2024-06-13-session1.html#purpose",
    "href": "news/past/2024-06-13-session1.html#purpose",
    "title": "ABC.1: First ABC session",
    "section": "Purpose",
    "text": "Purpose\nThe ABC aims to:\n\nSupport and guide in using bioinformatics and programming tools.\nProvide support for coding-related issues.\nEnhance the coding skills of participants in fundamental programming languages (R, python, bash command line)."
  },
  {
    "objectID": "news/past/2024-06-13-session1.html#who-can-benefit-from-the-abc",
    "href": "news/past/2024-06-13-session1.html#who-can-benefit-from-the-abc",
    "title": "ABC.1: First ABC session",
    "section": "Who can benefit from the ABC?",
    "text": "Who can benefit from the ABC?\nEveryone, really. Those that will particularly benefit from the ABC include:\n\nBiologists, bioinformaticians, and health scientists who wish to adopt coding-based solutions.\nScientists and students who wish to accelerate the coding-based analysis of various types of data (such as OMICs data and other large-scale data) in research across Aarhus University and Aarhus University Hospital.\nStudents and researchers who want an open environment where it is easy to get assistance and talk with others encountering similar issues (or maybe who already solved them!)"
  },
  {
    "objectID": "news/past/2024-06-13-session1.html#format",
    "href": "news/past/2024-06-13-session1.html#format",
    "title": "ABC.1: First ABC session",
    "section": "Format",
    "text": "Format\nEach 2 hour session of the ABC will feature:\n\nTopic Presentation: Insightful presentations on various bioinformatics and coding topics.\nOpen Floor Session: A hands-on segment where staff from the Bioinformatics Core Facility and the Health Data Science Sandbox will be available to assist with any coding or bioinformatics issues."
  },
  {
    "objectID": "news/upcoming/2024-10-10-usegenomedk.html",
    "href": "news/upcoming/2024-10-10-usegenomedk.html",
    "title": "Use GenomeDK workshop",
    "section": "",
    "text": "You are invited to an introductory workshop on october 10th, at 11:00 in building 1540-K26 (Biology department)."
  },
  {
    "objectID": "news/upcoming/2024-10-10-usegenomedk.html#agenda",
    "href": "news/upcoming/2024-10-10-usegenomedk.html#agenda",
    "title": "Use GenomeDK workshop",
    "section": "Agenda",
    "text": "Agenda\nThe topics for the workshop are all at introductory level, such as\n\nlogin and account setup\nfile system\ncommand line intro\nsoftware management\nprojects on GenomeDK\nfile transfer\njobs\n\nThere are no prerequisites apart from having a working account on GenomeDK with a complete 2-Factor authentication. Also, bring your laptop, so we can try out live some commands and exercises on the cluster.\nThere will be a lunch break 12-13, with lunch not included in the workshop. We will serve coffee and cake.\nThere is time for questions and problem-solving, so come with your issues if you are already a GDK user."
  },
  {
    "objectID": "news/upcoming/2024-10-10-usegenomedk.html#signing-up",
    "href": "news/upcoming/2024-10-10-usegenomedk.html#signing-up",
    "title": "Use GenomeDK workshop",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up through this very short online form:\n \n\n Sign up"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "",
    "text": "Incoming ABC\n\n\n\nThe next ABC is on October 24th, in building 1240-218, from 1pm to 3pm. Click the button below for more information and sign up to the event.\n \n\n Information + signup"
  },
  {
    "objectID": "index.html#concept",
    "href": "index.html#concept",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Concept",
    "text": "Concept\n\n\n\n\n\n\nHave you ever ended in a situation involving questions similar to those?\n\nWhere do I start to code on a command line?\n How do I learn the basic R/python commands?\n What the heck is this error?????\n\nDo you feel finding the right tools is like going through a maze of software installations, versions and incompatibilities?\n\n\n\nWhat follows is the natural desire of throwing your computer out of the window. Don’t worry! All the bioinformaticians/data scientists have gone through that, sometimes defined as part of the Kubler-Ross curve1, where frustration is the point in which coding becomes challenging and we might feel lost.\nThe idea behind ABC is to bridge you beyond frustration moments, while at the same time offering a relaxed environment and the possibility to learn new things, help and get to know each other:"
  },
  {
    "objectID": "index.html#who-can-benefit-from-the-abc",
    "href": "index.html#who-can-benefit-from-the-abc",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Who can benefit from the ABC?",
    "text": "Who can benefit from the ABC?\nEveryone, really. Those that will particularly benefit from the ABC include:\n\nBiologists, bioinformaticians, and health scientists who wish to adopt coding-based solutions.\nScientists and students who wish to accelerate the coding-based analysis of various types of data (such as OMICs data and other large-scale data) in research across Aarhus University and Aarhus University Hospital.\nStudents and researchers who want an open environment where it is easy to get assistance and talk with others encountering similar issues (or maybe who already solved them!)"
  },
  {
    "objectID": "index.html#format",
    "href": "index.html#format",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Format",
    "text": "Format\nEach ~2 hour session of the ABC will feature:\n\nTopic Presentation: Insightful presentations on various bioinformatics and coding topics.\nOpen Floor Session: A hands-on segment where staff from the Bioinformatics Core Facility and the Health Data Science Sandbox will be available to assist with any coding or bioinformatics issues."
  },
  {
    "objectID": "index.html#when-and-where",
    "href": "index.html#when-and-where",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "When and where",
    "text": "When and where\nLook at our Events calendar for more information about each meeting and upcoming workshops and courses."
  },
  {
    "objectID": "index.html#newsletter",
    "href": "index.html#newsletter",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Newsletter",
    "text": "Newsletter\nSubscribe to our newsletter to receive a few, non-invasive reminders for our meetings and events!"
  },
  {
    "objectID": "index.html#suggestions",
    "href": "index.html#suggestions",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Suggestions",
    "text": "Suggestions\nDo you have an input on making the ABC better with new tutorials or workshops we could organize? Please write it down in the form below, and we will see if we can implement something for the future editions of ABC.\n \n\n Give your input!"
  },
  {
    "objectID": "Documentation.html",
    "href": "Documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "While it is always worth remembering an event which offers free cake (especially when it recurs with a frequency like the ABC, as you could read here), you might want to read again what has been done and said after the sugar levels drop down.\nOur tutorials and instructions are self contained, thus useful for anyone, also people not attending a session.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.1: Install R+Rstudio, first R coding problems/solutions\n\n\n\nR\n\n\nRStudio\n\n\nDataframe\n\n\n\nThings worth remembering from the ABC.1\n\n\n\nJun 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.2: RStudio projects and Anaconda+Python\n\n\n\nR\n\n\nRStudio\n\n\nAnaconda\n\n\nPython\n\n\nenvironments\n\n\n\nThings worth remembering from the ABC.2\n\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.3: R analysis of bulkRNA expression matrix\n\n\n\nR\n\n\nbulkRNA\n\n\n\nThings worth remembering from the ABC.3\n\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.4: Introduction to bash language for bioinformatics\n\n\n\nbash\n\n\ncommand line\n\n\n\nSlides and bash intro at the ABC.4\n\n\n\nSep 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.5: open coding and tutorials, single cell dimensionality reduction\n\n\n\nscanpy\n\n\nprojection\n\n\nPCA\n\n\nUMAP\n\n\n\nABC.5: try our previous tutorials or a workshop on dimensionality reduction\n\n\n\nSep 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.6: Download from online databases, open coding session\n\n\n\nGEO\n\n\nSRA\n\n\nDownload\n\n\n\nLearn how to download (especially) GEO-SRA data\n\n\n\nOct 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop: Use GenomeDK\n\n\n\nGenomeDK\n\n\nbash\n\n\n\nIntroductive workshop to genomeDK\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "binfResources/R.html",
    "href": "binfResources/R.html",
    "title": "Useful tips for R",
    "section": "",
    "text": "Useful tips for R\nNothing yet\n\n\n\n\n\n\nthis is my tip\n\n\n\nhello I am Marie"
  },
  {
    "objectID": "binfResources/python.html",
    "href": "binfResources/python.html",
    "title": "Useful tips for python",
    "section": "",
    "text": "Useful tips for python"
  },
  {
    "objectID": "documentation/2024-10-10-usegdk.html",
    "href": "documentation/2024-10-10-usegdk.html",
    "title": "Workshop: Use GenomeDK",
    "section": "",
    "text": "We had a great workshop day, and for those of you who were not there, or want to see the slides again, here they are\n \n\n See the slides"
  },
  {
    "objectID": "documentation/2024-09-19-ABC5.html",
    "href": "documentation/2024-09-19-ABC5.html",
    "title": "ABC.5: open coding and tutorials, single cell dimensionality reduction",
    "section": "",
    "text": "Slides\nToday’s slides\n \n\n Download Slides \n\n \n\n\nTutorials\nYou can try our previous tutorials by looking at the Documentation page, or try a workshop one of us have been giving at the scVerse 2024 conference in Munich. The workshop is about dimensionality reduction methods for single cell data and contains a jupyter notebook to run python code on Google Colab.\n \n\n Go to the workshop page"
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html",
    "href": "documentation/2024-06-27-ABC2.html",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "",
    "text": "Conferences in the second half of 2024, introduction to Integrated Development Environments and Virtual Environments.\n \n\n Download Slides \n\n \nThe following tutorials show how to create a package environment and do some basic analysis in both R and python."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#project-creation-and-management-in-rstudio",
    "href": "documentation/2024-06-27-ABC2.html#project-creation-and-management-in-rstudio",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Project creation and management in RStudio",
    "text": "Project creation and management in RStudio\n1. Open RStudioo, go to the menu and select File –&gt; New Project...\n2. Choose New Directory –&gt; New Project (similarly you can create a project in an existing folder). Note that there is a third version including version control: here you can also control the history of all changes in the project files using version control tools (such as Git), but we are not talking about this for now.\n\n3. Name your project and select the directory where you want to save it. Click on use renv for this project. Finally click Create Project\n4. The project is now established. You can see some files are automatically created into the project directory.\n\n\n\n\n\n\nNote\n\n\n\nWhen you want to open your project the next time, you can either: - Open the project by double-clicking on the file in Windows Explorer/OSX Finder (e.g. MyProject.Rproj) - Open RStudio, go to the menu and select File –&gt; Open Project and browser to find and select the project that you want to open - Open RStudio, go to the menu, select File –&gt; Recent Projects, and select the project from the list of most recently opened projects\nSome of the things making it nice to work in projects in RStudio are that when a project is opened:\n\nA new R session (process) is started\nThe .RData file (saved data) in the project’s main directory is loaded\nThe .Rhistory file (command history) in the project’s main directory is loaded\nThe current working directory is set to the project directory\nPackages in the environment are loaded\nAnd other settings are restored to where they were the last time the project was closed\n\n\n\n\nProject structure\nA clean and organised project structure is super important. To ensure this, a good practice is to create subfolders like Data, Scripts, Output, and Docs within your project directory. You can do all this in the RStudio browser (bottom-right corner)."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#using-the-renv-package-to-manage-a-projects-environment",
    "href": "documentation/2024-06-27-ABC2.html#using-the-renv-package-to-manage-a-projects-environment",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Using the renv package to manage a project’s environment",
    "text": "Using the renv package to manage a project’s environment\nThe renv package allows you to create isolated and reproducible environments for your R projects, ensuring that the exact package versions and dependencies are used, ultimately facilitating reproducibility and collaboration. For an extended description, read more here. Dependencies are packages on which others rely on to work.\nKey features:\n\nEach project can have its own set of R packages and versions\nEnsures that the same package versions are used each time the project is run\nManages dependencies by automatically tracking the packages used in your project and their versions\nEnsures the same environment setup when sharing projects with collaborators\n\n\ntracking installed packages with renv\nYou do not need much to track all your packages. Simply create your files with R code and save them inside your project folder. While you code, you will necessarily install some packages which you need to create your program.\nWhen you are done, run in the Rstudio console the program renv::snapshot() - this will update the file renv.lock with a list of all installed packages and dependencies (open the file to see how it looks like). All the packages are installed inside the renv folder and will be loaded from there.\nNote that renv will track only packages that are used in your files with code - other packages which are installed but not used will not be tracked. Pretty smart!\n1. Now, try to install dplyr and ggplot2 using\ninstall.packages(c(\"dplyr\", \"ggplot2\"))\nin the RStudio console.\n2. Snapshot the environment by running\nrenv::snapshot()\nHave a look at the file renv.lock to see how it looks like. It should not show any package beyond renv: the file is updated only with packages used in the scripts you have in the project folder, so we will have to run the snapshot at the end of the tutorial where we use the packages!\n\n\n\n\n\n\nSharing projects\n\n\n\nWhen sharing your project, include the renv.lock and renv directory in the project folder. You can also include just the file renv.lock. Other users can use renv::activate() to activate the environment, and also renv::restore() if they need to install the packages (in case the folder renv is not provided).\nNote that if you provide the folder renv, this must be used in the same operating system. It might not be possible to use the environment folder create, for example, in Linux, on a windows or mac computer."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#kickstart-dplyr-analysis",
    "href": "documentation/2024-06-27-ABC2.html#kickstart-dplyr-analysis",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Kickstart dplyr analysis",
    "text": "Kickstart dplyr analysis\nThe dplyr package is part of the tidyverse, a set of packages all based on consistent and intuitive syntax/grammar for data manipulation, where the fundamental data structure is the data frame. dplyr is one of the most popular packages for data manipulation.\nWe already install the package in our environment and it is ready to use. You can use the following commands in the Console, or create an R script or an R markdown document.\nCreate a small dataset with gene expressions and some patient meta data:\n\n# Load dplyr\nlibrary(dplyr)\n\n# Create the dataframe\ndata &lt;- data.frame(\n  SampleID = c(\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"),\n  Gene1 = c(5.2, 6.3, 4.9, 7.2, 5.8),\n  Gene2 = c(3.8, 2.7, 3.5, 4.1, 3.9),\n  Gene3 = c(7.1, 8.5, 6.8, 9.2, 7.3),\n  Age = c(45, 52, 37, 50, 43),\n  Treatment = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n  Response = c(\"Responder\", \"Non-Responder\", \"Responder\", \"Non-Responder\", \"Responder\")\n)\n\n\ndata\n\n\nA data.frame: 5 × 7\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n\n\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n\n\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n\n\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n\n\n\n\n\nYou can apply many commands very intuitively with dplyr. For example, to select only the gene expressions:\n\nselected_data &lt;- select(data, Gene1, Gene2, Gene3)\n\nselected_data\n\n\nA data.frame: 5 × 3\n\n\nGene1\nGene2\nGene3\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n5.2\n3.8\n7.1\n\n\n6.3\n2.7\n8.5\n\n\n4.9\n3.5\n6.8\n\n\n7.2\n4.1\n9.2\n\n\n5.8\n3.9\n7.3\n\n\n\n\n\nYou can establish any filter based on the columns or combination of them. Filtering only on age of at least 50yo and rate of Gene1 and Gene2 larger than 2 is as below\n\nfiltered_data &lt;- filter(data, Age&gt;50 & Gene1/Gene2&gt;2)\n\nfiltered_data\n\n\nA data.frame: 1 × 7\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\n\n\n\nAdding new columns as a combination of others is also immediate\n\nmutated_data &lt;- mutate(data, GeneTotal = Gene1 + Gene2 + Gene3)\n\nmutated_data\n\n\nA data.frame: 5 × 8\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\nGeneTotal\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n16.1\n\n\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n17.5\n\n\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n15.2\n\n\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n20.5\n\n\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n17.0\n\n\n\n\n\nNow we introduce the pipe symbol %&gt;%. This combines with the dplyr functions such that subsequent operations between pipes reflect the way we enunciate them orally. For example: The dataframe data must be grouped by treatment and for each group summarise the Gene1 by its average. Arrange the result by the average of Gene1 in descending order.\n\n\n\n\n\n\ndataframes and tibbles\n\n\n\nNote how the output is no longer a dataframe, but a tibble. A tibble is a more “modern” version of the dataframe which is used in the tidyverse packages. You cannot really notice much difference when using them. To ensure you are using a tibble , you can always define a dataframe and transform it into a tibble using the command as_tibble. Read more about tibbles at this link.\n\n\n\ngrouped_data &lt;- data %&gt;% \n                group_by(Treatment) %&gt;% \n                summarise(avgGene1 = mean(Gene1)) %&gt;%\n                arrange( desc(avgGene1) )\n\ngrouped_data\n\n\nA tibble: 2 × 2\n\n\nTreatment\navgGene1\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nB\n6.75\n\n\nA\n5.30\n\n\n\n\n\nTo save your table in the Output folder as csv file:\n\nwrite.csv(grouped_data, \"Output/grouped.csv\")\n\nNow, if you ran all the commands in your console, create a new R script (File -&gt; New -&gt; R Script) and paste all the code used until now, then save it. If you instead used a Script or Markdown, save it. Move the script/markdown file into the Code folder, so you keep the files organized! Now run again\nrenv::snapshot()\nYou should be requested for permission to write some packages into renv.lock. Accept and look into the file, which not should be different and including new package names.\n\n\n\n\n\n\nwrap up\n\n\n\nNow you know how to manage a project in RStudio and the packages you need. dplyr and the other packages pivoting around tidyverse have a plethora of useful functionalities. A good place to start from is to use a table you need for your own work, and try out various things you can do. Some good resources for training are available as links below:\n\nA chapter of R for data science from the data carpentry. Go through it and look at other chapters as well, they are very instructive!\nA cheatsheet for all dplyr manipulation and for other important tidyverse packages + Rstudio. Keep some at your desk!"
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#install-anaconda-and-create-an-environment",
    "href": "documentation/2024-06-27-ABC2.html#install-anaconda-and-create-an-environment",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Install Anaconda and create an environment",
    "text": "Install Anaconda and create an environment\n1. Download Anaconda from the download page. Uso also the Sign In button of the webpage to create an account: this will enable you to use an AI-chat showing up while you code, and which can help you doing a lot of things.\n\n\n\n\n\n\nNote\n\n\n\nThis guide has been made with Windows, and works similarly with MacOS. For Linux, you will download a file with .sh extension, which you can execute in the command line with bash installer.sh.\n\n\n2. Once Anaconda is installed, open Anaconda Navigator. Log into Anaconda Cloud from the software, and eventually update Anaconda Navigator if asked.\n3. Your initial window would look similarly as below. What you can see is a suite of softwares. Some of those are installed and they are found in the anaconda environment called base (root), as shown in the red circle. It is good advice not to modify the base environment, because Anaconda itself is installed into it!\n\n4. We create a new environment with only few needed packages for our tutorial. Click on Environments on the toolbar (red circle). You can see the base (root) environment and a long list of packages it contains. Click on Create (blue circle).\n\n5. In the appearing window choose a name for the environment and select Python. Then click on Create. It takes a bit of time to create the environment.\n\n6. Select the environment. It will load a few packages (just Python and its essential dependencies), but we want to install new ones. Open the filtering menu (green circle above) and choose All to view all existing packages. Select the following packages: pandas, numpy, jupyterlab, anaconda-toolbox, seaborn and click Apply. A list of dependencies will be shown, and you have to accept that to continue. Wait for the installations to go through.\n7. Now select the environment again. There are much more packages installed and ready to use. Click on Home in the toolbar: you will see the softwares installed in your environment (choose Installed Application from the dropdown menu to see only the installed ones).\n\n\n\n\n\n\nwrap up\n\n\n\nYou can create various environments to keep specific versions of packages constant in separate data science projects. This ensures a high degree of reproducibility in your projects.\nAn environment can include the softwares needed to code (RStudio, jupyterLab are the most famous, but also others can be installed).\nWe will use the created environment to run some basic commands in python."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#python-basics",
    "href": "documentation/2024-06-27-ABC2.html#python-basics",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Python basics",
    "text": "Python basics\nHere we look at the python basics: variables, arrays and dataframes. We will immediately work with two packages: numpy and pandas, which make the use of arrays and dataframes very flexible.\n\nJupyterLab\nLaunch jupyterlab from the Anaconda Home, using the environment ABC2. You should be able to see Jupyterlab opened in your internet browser. It will look similar to the one below: it has a file browser, can show the opened files on different tabs, and has a coding window where the opened file is shown. When you first open jupyterlab you will instead see a Launcher, which gives you a choice for all the available things to do, usually Notebooks, Console, or Other, with related available languages.\n\nYou do not create projects in JupyterLab, you simply create folders instead.\n\nUse the browser to create a folder wherever you prefere, and organize it into subfolders: Code, Output, Data, Scripts. You can use the small button above the browser to create a folder, or simply the right-click options.\nAgain with a right click, create a notebook inside the Code folder\nWhen asked to choose a kernel, select python 3. A kernel includes the programming language and its packages as we installed them into the environment using Anaconda.\n{fig-align=“center”, width=300px}\n\nA notebook looks like this:\n\nThe gray area is a code cell, where you can write code. Select the left border out of a cell to\n\ntransform it into a Markdown cell where you can write text: key M of your keyboard\nadd a cell below: key B\nrun the cell and add one below: keys Shift+Enter\n\n\n\n\n\n\n\nTip\n\n\n\nA notebook looks a lot like a text file. When you open it, it will show all results and images from your code! You can share it to anyone without the need to run the code again for the recipient, if it is needed to simply show some results.\nThe webpage you are reading now has been created with a notebook!\n\n\nLet’s turn things into practice now. Transform the first cell into markdown (key M) and write a title with\n ## ABC2 is cool\nand press Shift+Enter. You will get a new cell below, and the text will be formatted from Markdown.\nNow we write some code in the new cell and press Shift+Enter to execute:\n\nimport numpy as np\nimport pandas as pd\n\nprint(\"Hello ABC\")\nx = np.arange(1,10)\ny = x**2\nprint(\"x\")\nprint(x)\nprint(\"y\")\nprint(y)\n\nHello ABC\nx\n[1 2 3 4 5 6 7 8 9]\ny\n[ 1  4  9 16 25 36 49 64 81]\n\n\nThings should look as below:\n\nThe little number on the left shows how many code steps you have been running so far. The code imports relevant libraries and shortens their name with np and pd, which makes coding easier and compact. Then we use the library np to define an array of numbers from 1 to 10, and we square this array assigning it to y. Finally we print the two arrays.\nAll variables are assigned with the = symbol and you can do all arithmetic operations:\n\nprint(2+3)\nprint(9*2)\nprint(5**2)\n\n5\n18\n25\n\n\n\n\nPandas dataframes\nwe procees looking into a small dataframe. Let’s create one using the pandas library\n\n# Create the dataframe\ndata = pd.DataFrame({\n    'SampleID': ['S1', 'S2', 'S3', 'S4', 'S5'],\n    'Gene1': [5.2, 6.3, 4.9, 7.2, 5.8],\n    'Gene2': [3.8, 2.7, 3.5, 4.1, 3.9],\n    'Gene3': [7.1, 8.5, 6.8, 9.2, 7.3],\n    'Age': [45, 52, 37, 50, 43],\n    'Treatment': ['A', 'B', 'A', 'B', 'A'],\n    'Response': ['Responder', 'Non-Responder', 'Responder', 'Non-Responder', 'Responder']\n})\n\n# View the dataframe\ndata\n\n\n\n\n\n\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n\n\n0\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n\n\n1\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\n2\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n\n\n3\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n\n\n4\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n\n\n\n\n\n\n\nYou can plot variables from the dataframe with the package seaborn. For example\n\nimport seaborn as sns\n\nsns.scatterplot(data=data,\n                x=\"Gene1\",\n                y=\"Gene2\",\n                hue=\"Response\",\n                size=\"Age\"\n               )\n\n\n\n\n\n\n\n\nIn the command above we used some options beyond x and y. Can you see what they match in the plot?\n\n\n\n\n\n\nTip\n\n\n\nThe seaborn package webpage has great examples for any kind of plot you desire!\n\n\nThere are lots of summary statistics already implemented in python. Below we calculate mean, median and standard deviation for the column Gene1 of the data frame and then we print them.\n\nx = data.Gene1\nmeanG1 = np.mean(x)\nmedianG1 = np.median(x)\nsdG1 = np.std(x)\n\nprint(\"mean, median and sd:\")\n[meanG1, medianG1, sdG1]\n\nmean, median and sd:\n\n\n[np.float64(5.88), np.float64(5.8), np.float64(0.8182909018191513)]\n\n\nThis was neat! Can you try to calculate the cumulative sum of the difference between Gene1 and Gene2?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answer is\nx = data.Gene1 - data.Gene2\ncsum = np.cumsum(x)\nprint(\"cumulative sum of Gene1 - Gene2:\")\nprint(csum)\n\n\n\n\n\nFunctions\nAlthough python and the packages you can find have almost everything you will need, sometimes you might need to define your own function. The syntax to do it is very easy: you define a function name, which then you will be able to use it. Below, there is a function taking an argument (arg1) and multiplying it by 5. The output needs to be explicit through the return() function.\n\ndef myFunction(arg1):\n    res = arg1 * 5\n    return(res)\n\nSuch a function works if the argument is a number, but also if it is an array!\n\nprint(\"with a number only\")\nprint( myFunction(5) )\nprint(\"with an array\")\nprint( myFunction(data.Gene1) )\n\nwith a number only\n25\nwith an array\n0    26.0\n1    31.5\n2    24.5\n3    36.0\n4    29.0\nName: Gene1, dtype: float64\n\n\nTry to make a function that takes three vectors, plots the first against the sum of the second and third, and returns the sum of all three vectors. Use the plot command we applied previously for help.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answer is\ndef simpleSumPlot(arg1, arg2, arg3):\n    arg23 = arg2 + arg3\n    arg123 = arg1 + arg2 + arg3\n\n    #plotting\n    fig = sns.scatterplot(x = arg1, \n                    y = arg2)\n    fig.set_title(\"Plot with my own function\")\n    fig\n\n    return(arg123)\nNow you can try this on vectors of the same length. We can use the ones in our data frame!\n\nsimpleSumPlot(arg1 = data.Gene1,\n             arg2 = data.Gene2,\n             arg3 = data.Gene3)\n\n\n\n\n\nRead and write files\nSave a dataframe with the command .to_csv\n\ndata.to_csv('../Ouput/data.csv')\n\nAnd read it again using\n\ndata2 = pd.read_csv('../Ouput/data.csv')\n\n\ndata2\n\n\n\n\n\n\n\n\nUnnamed: 0\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n\n\n0\n0\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n\n\n1\n1\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\n2\n2\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n\n\n3\n3\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n\n\n4\n4\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n\n\n\n\n\n\n\n\n\n\n\n\n\nwrap up\n\n\n\nYou learned how to use jupyterlab notebooks and how to do basic operations and plots with python."
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html",
    "href": "documentation/2024-06-13-ABC1.html",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "",
    "text": "Introductory slides about the ABC, and who is behind the Health Data Science Sandbox and the Core Bioinformatics Facility\n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#rstudio-layout",
    "href": "documentation/2024-06-13-ABC1.html#rstudio-layout",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Rstudio Layout",
    "text": "Rstudio Layout\n\nLaunch RStudio.\nFamiliarize yourself with the RStudio interface, which consists of:\n\nSource Pane: Where you write and edit scripts.\nConsole Pane: Where you run commands and see output.\nEnvironment/History Pane: Shows your workspace and command history.\nFiles/Plots/Packages/Help/Viewer Pane: Access your files, view plots, manage packages, get help, and view HTML output."
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#make-a-simple-script",
    "href": "documentation/2024-06-13-ABC1.html#make-a-simple-script",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Make a simple script",
    "text": "Make a simple script\n\nClick on File &gt; New File &gt; R Script to open a new script editor.\nInside the file you created, Write a simple script. Copy for example the script below:\n\n\nprint(\"Hello ABC\")\nx &lt;- 1:10\ny &lt;- x^2\nplot(x, y, type=\"b\", col=\"blue\")\n\n[1] \"Hello ABC\"\n\n\n\n\n\n\n\n\n\n\nTry to run the script (The small Run button). Two things should happen:\n\nAn output is shown in the console (bottom left)\nA plot is created and shown in the plotting window (bottom right)\nthe variables x and y are saved in your environment and can be seen in the variable explorer (top right). These variables can be used again since they exist in your computer’s memory.\n\n\n\n\n\n\n\n\nWorking directory\n\n\n\nEvery time you start working in R, this will be considering a working directory. Such directory is the reference point you are working in. For example, if you want to open a file, you need to know where it is in relation to your working directory, so that you can correctly write where it is. Write the command getwd() in the console and press Enter to see your current working directory."
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#some-basic-operations",
    "href": "documentation/2024-06-13-ABC1.html#some-basic-operations",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Some basic operations",
    "text": "Some basic operations\nIn R you can perform basic math operations by using the appropriate symbol. For example\n\n2+3\n9*2\n5^2\n\n5\n\n\n18\n\n\n25\n\n\nYou can assign variables (“objects”) using the symbol &lt;-. For example\n\nx &lt;- 5\nanothername &lt;- 3\nitCanBeAnything &lt;- \"It can be text\""
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#lets-try-some-exercises",
    "href": "documentation/2024-06-13-ABC1.html#lets-try-some-exercises",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Lets try some exercises!",
    "text": "Lets try some exercises!\nCreate a new script file or use the console to test some exercises below.\n\n1. Create a data frame\nWe will create a simple data frame. A data frame is nothing more than a table, where both rows and columns have labels, and can be easily accessed and manipulated. To create a small data frame, we can define its columns. We define each column through a vector with the function c(), where we can write values inside separated by a comma. Then we provide all vectors to the function data.frame, where we assign column names (Gene, Control, Treatment1, Treatment2).\n\n\n\n\n\n\nNote\n\n\n\nMake sure your vectors are all of the same length! Also, each vector usually contains values of the same type (for example only numbers or only text)\n\n\n\ngeneExpr &lt;- data.frame(\n  Gene = c(\"GeneA\", \"GeneB\", \"GeneC\"),\n  Control = c(10, 20, 30),\n  Treatment1 = c(15, 25, 35),\n  Treatment2 = c(100, 0, 250)\n)\n\ngeneExpr\n\n\nA data.frame: 3 × 4\n\n\nGene\nControl\nTreatment1\nTreatment2\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nGeneA\n10\n15\n100\n\n\nGeneB\n20\n25\n0\n\n\nGeneC\n30\n35\n250\n\n\n\n\n\nYou should be able to see the small data frame printed in output and also shown in the variable explorer.\n\n\n2. Plot from a data frame\nNow lets try to plot it Treatment 1 versus Treatment 2. The basic function for plotting in R is called plot. It takes as arguments the x axis and the y axis. It has other options which are not mandatory, such as style and color of the plot. Notice how we access the values in the columns using the $ sign.\n\nplot( x = geneExpr$Treatment1, \n      y = geneExpr$Treatment2 , \n      main=\"Expression per Treatment group\", \n      xlab = \"Treatment 1\", \n      ylab = \"Treatment 2\" , \n      col =\"blue\" , \n      type =\"b\")\n\n\n\n\n\n\n\n\nYou should be able to see a plot in the plotting window. In the command above we used many options beyond x and y. Can you see what they match in the plot?\n\n\n3. Summary statistics\nThere are lots of summary statistics already implemented in R. Below we calculate mean, median and standard deviation for the column Treatment1 of the data frame and then we print them.\n\nx &lt;- geneExpr$Treatment1\nmeanTr1 &lt;- mean(x)\nmedianTr1 &lt;- median(x)\nsdTr1 &lt;- sd(x)\n\nprint(\"mean, median and sd:\")\nprint(c(meanTr1, medianTr1, sdTr1))\n\n[1] \"mean, median and sd:\"\n[1] 25 25 10\n\n\nThis was neat! Can you try to calculate the cumulative sum of the difference between Treatment 1 and Treatment 2?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nx &lt;- geneExpr$Treatment1 - geneExpr$Treatment2\ncsum &lt;- cumsum(x)\nprint(\"cumulative sum of Tr1 - Tr2:\")\nprint(csum)\n\n[1] \"cumulative sum of Tr1 - Tr2:\"\n[1]  -85  -60 -275\n\n\n\n\n\n\n\n4. Define your own function!\nAlthough R and the packages you can find have almost everything you will need, sometimes you might need to define your own function. The syntax to do it is very easy: you assign a function to a name, which then you will be able to use. Below, there is a function taking an argument (arg1) and multiplying it by 5. The function commands need to be between the curly brackets, and what we want as output need to be explicit through the return() function.\n\nmyFunction &lt;- function(arg1){\n  \n  # Now arg1 is a variable of the function. \n  # You can write comments inside code blocks with #\n  output &lt;- arg1 * 5\n  return (output)\n\n}\n\nSuch a function works if the argument is a number, but also if it is a vector!\n\nprint(\"with a number only\")\nmyFunction(5)\nprint(\"with a vector\")\nmyFunction(geneExpr$Treatment1)\n\n[1] \"with a number only\"\n[1] \"with a vector\"\n\n\n25\n\n\n\n75125175\n\n\nTry to make a function that takes three vectors, plots the first against the sum of the second and third, and returns the sum of all three vectors. Use the plot command we applied previously for help.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsimpleSumPlot &lt;- function (arg1, arg2, arg3){\n  \n  arg23 &lt;- arg2 + arg3\n  arg123 &lt;- arg1 + arg2 + arg3\n\n  #plotting\n  plot( x = arg1, \n      y = arg23, \n      main=\"Plot with my own function\", \n      xlab = \"arg1\", \n      ylab = \"arg2+arg3\" , \n      col =\"red\", \n      type =\"o\" )\n\n  return (arg123)\n  \n}\n\nNow you can try this on vectors of the same length. We can use the ones in our data frame!\n\nsimpleSumPlot(arg1=geneExpr$Control, \n              arg2=geneExpr$Treatment1, \n              arg3=geneExpr$Treatment2)\n\n\n12545315\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Read files\nMany times we want to read files from excel or other formats. R has many ways to do this and if not there are always packages out there to help you read the format you have. For reading an excel file a great package is the readxl. To read a csv file there is already the R function read.csv().\n\n\n\n\n\n\nNote\n\n\n\nBut wait. what are packages? Each package consist of a set of R and other scripts that meet specific needs for the users of R. For example openxlsx reads from Excel files, which R cannot do on its own. There are thousands of packages out there, ranging all fields of science, and some have become very popular.\n\n\nTry to install the package openxlsx. You can use the command install.packages(\"openxlsx\") in your RStudio console. Otherwise, go on the bottom right panel and click Packages and Install like this\n\nNow you are ready to import an Excel file. To use the package, we can load it with library(openxlsx). Otherwise we need to write the package name before the command to use from it (as done below). You can get a file locally on your computer or from an URL as done in this example.\n\ndf &lt;- openxlsx::read.xlsx(\"https://github.com/AU-ABC/AU-ABC.github.io/raw/main/documentation/2024-06-13-instR/data/data.xlsx\", sheet=1)\ndf\n\n\nA data.frame: 3 × 3\n\n\n\nx\ny\nz\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n10\n30\n-1\n\n\n2\n20\n20\n0\n\n\n3\n30\n10\n1\n\n\n\n\n\nOnce you have a data frame, you can always save it. Remember, the path of the saved file is related to your current working directory! To save your data frame as a csv file, use\n\nwrite.table(geneExpr, \n            file = \"./myFirstDataFrame.csv\", \n            row.names = FALSE , \n            sep=\"\\t\")\n\nwhere we remove the labels for the rows and use the tab separator instead of the comma. To read the file again, simply use\n\ndf2 &lt;- read.csv(\"./myFirstDataFrame.csv\" , sep=\"\\t\")\ndf2\n\n\nA data.frame: 3 × 4\n\n\nGene\nControl\nTreatment1\nTreatment2\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nGeneA\n10\n15\n100\n\n\nGeneB\n20\n25\n0\n\n\nGeneC\n30\n35\n250\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the tab key of your keyboard to see a list of the available paths"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html",
    "href": "documentation/2024-08-20-ABC3.html",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "",
    "text": "Data formats and bulkRNA sequencing exercise\n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html#install-packages",
    "href": "documentation/2024-08-20-ABC3.html#install-packages",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "Install packages",
    "text": "Install packages\nFirst of all you need quite some packages for bulkRNA analysis. The following installations will also help in the fiture analysis tutorial where various different plots are explored. Note how you install some packages with install.packages (from the R default channel) and with BiocManager::install (from the BiocManager channel).”\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"pheatmap\")\ninstall.packages(\"ggrepel\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"readxl\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"pdftools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggridges\")\ninstall.packages(\"forcats\")\ninstall.packages(\"igraph\")\n\n# Install Bioconductor packages\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n    install.packages(\"BiocManager\")\n}\n\nBiocManager::install(\"edgeR\")\nBiocManager::install(\"DESeq2\")\nBiocManager::install(\"ComplexHeatmap\")\nBiocManager::install(\"clusterProfiler\")\nBiocManager::install(\"org.Hs.eg.db\")\nBiocManager::install(\"org.Mm.eg.db\")\nBiocManager::install(\"AnnotationDbi\")\nBiocManager::install(\"pathview\")\nBiocManager::install(\"DOSE\")\nBiocManager::install(\"enrichplot\")\nBiocManager::install(\"ReactomePA\")\nBiocManager::install(\"SummarizedExperiment\")\n\nWe should then load the needed packages\n\nlibrary(edgeR)\nlibrary(DESeq2)\nlibrary(ggplot2)\nlibrary(ComplexHeatmap)\nlibrary(clusterProfiler)\nlibrary(pheatmap)\nlibrary(org.Hs.eg.db)\nlibrary(org.Mm.eg.db)\nlibrary(AnnotationDbi)\nlibrary(ggrepel)\nlibrary(openxlsx)\nlibrary(readxl)\nlibrary(pathview)\nlibrary(tidyverse)\nlibrary(RColorBrewer)\nlibrary(DOSE)\nlibrary(enrichplot)\nlibrary(pdftools)\nlibrary(dplyr)\nlibrary(ggridges)\nlibrary(forcats)\nlibrary(ReactomePA)\nlibrary(igraph)\nlibrary(BiocManager)\n\nHere we download a data matrix which we created in advance. The file is compressed, so we also unzip it (converting it from compressed to normal reading format) before reading.\n\ndownload.file(\"https://github.com/AU-ABC/AU-ABC.github.io/raw/main/documentation/2024-08-20-ABC3/FeatureCountOutput.zip\", destfile = \"./FeatureCountOutput.zip\")\n\n\nunzip(\"./FeatureCountOutput.zip\")\n\n\ncountData &lt;- read.table(\"./FeatureCountOutput.txt\", header = TRUE, sep = \"\\t\")\n\nNow we have created the matrix Countdata and we check the rownames of the countMatrix. The names are simply numbers and not gene IDs or names. So we replace the row names with the countData$Geneid column of the dataset. We use the function “head” to view the first 5 names before and after substitution.\n\nhead(rownames(countData))\nrownames(countData) &lt;- countData$Geneid\nhead(rownames(countData))\n\n\n'1''2''3''4''5''6'\n\n\n\n'DDX11L1''WASH7P''MIR6859-1''MIR1302-2HG''MIR1302-2''FAM138A'\n\n\nIf we look at column names, we see a certain number of names columns. The first ones are gene IDs and genetic informations about chromosome, starting and ending position, strand and length. The rest is sample names.\n\ncolnames(countData)\n  \n\n\n'Geneid''Chr''Start''End''Strand''Length''X1_.bam''X2_.bam''X3_.bam''X4_.bam''X5_.bam''X6_.bam''X7_.bam''X8_.bam''X9_.bam'\n\n\nThe sample names does not make sense for a person who did not do the sequencing of this data, therefore we rename it to the scientific annotation.\n\n\n\n\n\n\nTip\n\n\n\nThe order the colnames(countData) is shown, is the order you have to follow with the renaming for the scientific annotation.\n\n\n\n sampleNames &lt;- c(\"WT_1\", \"WT_2\", \"WT_3\", \"DRUG_1\", \"DRUG_2\", \"DRUG_3\", \"DRUG+SUPPLEMENT_1\", \"DRUG+SUPPLEMENT_2\", \"DRUG+SUPPLEMENT_3\")\n\nThe countmatrix countData contains additional information such as gene name, gene length, exon start and more. These informations are found in columns from 1 to 6. This means, that we have to change the samplenames starting from column number 7. Before we print the column names before and after we do substitution\n\ncolnames(countData)[7:length(colnames(countData))]\ncolnames(countData)[7:length(colnames(countData))] &lt;- sampleNames\ncolnames(countData)\n\n\n'X1_.bam''X2_.bam''X3_.bam''X4_.bam''X5_.bam''X6_.bam''X7_.bam''X8_.bam''X9_.bam'\n\n\n\n'Geneid''Chr''Start''End''Strand''Length''WT_1''WT_2''WT_3''DRUG_1''DRUG_2''DRUG_3''DRUG+SUPPLEMENT_1''DRUG+SUPPLEMENT_2''DRUG+SUPPLEMENT_3'\n\n\nAny sequencing data is prone to noise. The kind of noise we remove here is arguably something of great discussion as to where and when to keep data. We can see there are a lot of genes by counting the rows of countData\n\n  nrow(countData) # This returns the number of genes in the countmatrix before removing noise.\n\n49771\n\n\nWe can also plot a simple histogram of how many counts each gene has in a specific sample, for example WT_1. We use the logarithm log(x+1), where x is the expression to have a readable plot. Note how we can see many genes with 0 expression.\n\nhist(log1p(countData$WT_1), breaks=30, main=\"WT_1 expression values\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is however not a very useful plot, because a gene can be overexpressed under other conditions, and underexpressed in the WT_1 sample. We can try to look at the gene expression over all individuals for each gene using again a histogram.\n\n\nwe still have a number of genes with little or no expression. Those are of very little relevance in any analysis\n\nsumExpr &lt;- rowSums( countData[,7:15] )\n\nhist( log1p(sumExpr), breaks=30, main=\"Sum of gene expression values on all samples\")\n\n\n\n\n\n\n\n\nAs an example, we print the fifth gene to see the number of counts. If none of the samples have 5 or more counts of this gene, then we remove the gene.\n\n  countData[5, 7:ncol(countData)] \n\n\nA data.frame: 1 × 9\n\n\n\nWT_1\nWT_2\nWT_3\nDRUG_1\nDRUG_2\nDRUG_3\nDRUG+SUPPLEMENT_1\nDRUG+SUPPLEMENT_2\nDRUG+SUPPLEMENT_3\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nMIR1302-2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nBelow we use subset to apply the filter. We remove data (genes) contains little to no information. It essentially removes a gene from the countmatrix if the gene has less than 5 counts in all the samples.\n\ncountData &lt;- subset(countData, rowSums(countData[, 7:ncol(countData)] &gt; 5) &gt; 0)\n\nWe removed a lot of genes!\n\nnrow(countData) #Returning the number of genes after removing noise.\n\n21473"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html#condition-matrix",
    "href": "documentation/2024-08-20-ABC3.html#condition-matrix",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "Condition matrix",
    "text": "Condition matrix\nThe samples are defined in some condition like WT, DRUG, DRUG+SUPPLEMENT or some other condition.\n\nconditions &lt;- c(rep(\"WT\", 3), rep(\"DRUG\", 3), rep(\"DRUG+SUPPLEMENT\", 3)) \n \n\nWe create a dataframe with the condition and sample information\n\ncoldata &lt;- data.frame(row.names = colnames(countData[0, 7:ncol(countData)]), conditions)\n\n\ncoldata\n\n\nA data.frame: 9 × 1\n\n\n\nconditions\n\n\n\n&lt;chr&gt;\n\n\n\n\nWT_1\nWT\n\n\nWT_2\nWT\n\n\nWT_3\nWT\n\n\nDRUG_1\nDRUG\n\n\nDRUG_2\nDRUG\n\n\nDRUG_3\nDRUG\n\n\nDRUG+SUPPLEMENT_1\nDRUG+SUPPLEMENT\n\n\nDRUG+SUPPLEMENT_2\nDRUG+SUPPLEMENT\n\n\nDRUG+SUPPLEMENT_3\nDRUG+SUPPLEMENT\n\n\n\n\n\nWe create a subset of the countData so that we have an object only containing only the counts of the samples.\n\ncountData_subset &lt;- countData[, -c(1:6)]\n\n\nhead(countData_subset)\n\n\nA data.frame: 6 × 9\n\n\n\nWT_1\nWT_2\nWT_3\nDRUG_1\nDRUG_2\nDRUG_3\nDRUG+SUPPLEMENT_1\nDRUG+SUPPLEMENT_2\nDRUG+SUPPLEMENT_3\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nWASH7P\n45\n45\n37\n109\n84\n107\n123\n128\n72\n\n\nMIR6859-1\n2\n1\n4\n7\n10\n5\n7\n6\n9\n\n\nLOC729737\n6\n3\n6\n9\n7\n5\n85\n81\n68\n\n\nLOC100996442\n70\n66\n61\n82\n58\n70\n127\n107\n100\n\n\nLOC127239154\n6\n7\n6\n23\n20\n19\n5\n5\n4\n\n\nLOC100132287\n2\n4\n1\n3\n2\n2\n4\n6\n4\n\n\n\n\n\nWe check if the sample names are correct. Both of these should return TRUE, otherwise there is some incorrect naming.\n\nall(colnames(countData_subset) %in% rownames(coldata))\nall(colnames(countData_subset) == rownames(coldata))\n\nTRUE\n\n\nTRUE"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html#applying-deseq2",
    "href": "documentation/2024-08-20-ABC3.html#applying-deseq2",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "Applying DeSeq2",
    "text": "Applying DeSeq2\nTime to use the DESeq2 normalization method to analyse your sequencing data. The commands below run DeSeq2 and\n\nbuilds a DeSeq2 object from the data matrix and condition matrix\napplyes DeSeq2\ntransforms the expressions by variance stabilization\n\n\ndds &lt;- DESeqDataSetFromMatrix(count = countData_subset, colData = coldata, design = ~conditions)\ndds &lt;- DESeq(dds)\nvsdata &lt;- vst(dds, blind=FALSE)\n\nWarning message in DESeqDataSet(se, design = design, ignoreRank):\n\"some variables in design formula are characters, converting to factors\"\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nestimating size factors\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nestimating dispersions\n\ngene-wise dispersion estimates\n\nmean-dispersion relationship\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nfinal dispersion estimates\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nfitting model and testing\n\n\n\nTo compute a variance stabilizing transformation is roughly similar to putting the data on the log2 scale, while also dealing with the sampling variability of low counts. It uses the design formula to calculate the within-group variability (if blind=FALSE) or the across-all-samples variability (if blind=TRUE). It does not use the design to remove variation in the data. It therefore does not remove variation that can be associated with batch or other covariates (nor does DESeq2 have a way to specify which covariates are nuisance and which are of interest).\nThe resulting object is called vsdata, which will be used for further analysis with DESeq2\n\nvsdata\n\nclass: DESeqTransform \ndim: 21473 9 \nmetadata(1): version\nassays(1): ''\nrownames(21473): WASH7P MIR6859-1 ... TRNT TRNP\nrowData names(26): baseMean baseVar ... maxCooks dispFit\ncolnames(9): WT_1 WT_2 ... DRUG+SUPPLEMENT_2 DRUG+SUPPLEMENT_3\ncolData names(2): conditions sizeFactor\n\n\n\n\n\n\n\n\nwrap up\n\n\n\nYou have read and preprocessed a bulk RNA expression matrix and ran an analysis with DeSeq2. Further steps concern visualizing and interpreting the results inferred by DeSeq2 on the data."
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html",
    "href": "documentation/2024-09-19-ABC6.html",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "",
    "text": "Today’s slides\n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html#geo-and-sra-data",
    "href": "documentation/2024-09-19-ABC6.html#geo-and-sra-data",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "GEO and SRA data",
    "text": "GEO and SRA data\nGEO (Gene Expression Omnibus) is a database which contains processed data from biological experiments. For example aligned files and analysis results, and supplementary files of various type. A GEO entry is called Series and contains Samples , Platforms, Supplementary data. Usually, there is an associate SRA number.\nA SRA (Sequence Read Archive) number contains the raw data for the samples contained in a GEO series. The download from GEO can happen by using the various alphanumerical identifiers to write a download link. SRA downloads need the software SRA-tools, and cannot be performed using a simple download link.\n\n\n\nA GEO Series, where highlighted in different colours there are the main data elements. The Series ID (red). the type of platform (green) for creating the samples (blue), and the SRA number for raw data (purple).\n\n\n\nDownload GEO data programmatically\nSometimes you need to download some supplementary files from GEO. This is pretty easy from the command line, and we show it with an example. We consider the GEO series in the figure above - go to the GEO home page and find it using the series number in the search bar of the website.\nNow, we use the guidelines of GEO for programmatic downloads. Here you have examples and descriptions on how to write the FTP1 address of all files in the series (excluding SRA files).\nTo look at the content of the series, use the command line and write\ncurl -l ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/\nnote how the address contains series, the series ID where the last three numbers are changed into nnn, and the series number itself. The output is a list of the content of the series:\nmatrix\nminiml\nsoft\nsuppl\nThe first three are folders containing matrix, miniml and soft files, which are plain text files used to describe the series. The fourth is the folder of supplementary material. Using curl -l we can see that it contains the supplementary file of the series:\ncurl -l ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/suppl\nGSE67303_DEG_cuffdiff.xlsx\nNow we can download the file using the whole URL with wget\nwget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/suppl/GSE67303_DEG_cuffdiff.xlsx\n\n\n\n\n\n\nTip\n\n\n\nDo you want to download a lot of supplementary files without one-by-one download by hand? Then wget -r will download recursively the content of the whole folder, for example:\nwget -r ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/suppl/\nSome webpages have so-called robots which avoid a user to download in an autopmated way too many datasets at once, to prevent overloading of the bandwidth. Bulk downloads are identified by, amongst others, how much time it takes to request a download. When a robot rejects your download attempt, you get an error message. In that case, wget can solve the problems with some extra dedicated options. Below you can see a bulk download from the PRIDE database, which uses robots:\nwget --random-wait \\\n     -r -p -e robots=off \\\n     -U mozilla \\\n     ftp.pride.ebi.ac.uk/pride/data/archive/2024/09/PXD056312/\nNote: the command above takes time because the data is quite big and is just for the sake of example, so don’t run it :-)\n\n\nYou can change the URL to show the supplementary data inside samples and inside platforms. In our example you can click on the platform/sample number on the webpage, and see if they contain any supplementary data. It is not the case of this GEO series, but for the sake of the example we will write down how to list the supplementary files inside the related FTP folders.\n\nFor the platforms, we need to substute series with platforms, and the same with the IDs, which will again have nnn for the last 3 digits: curl -l ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL19nnn/GPL19949/ you will notice there are only soft and miniml folders for the platform description, and no supplementary\nFor the samples, it works analogously, using sample IDs curl -l ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM1644nnn/GSM1644066/ you will notice there are only soft and miniml folders for the platform description\n\n\n\nDownload GEO and SRA data with geofetch\nDownloading entire GEO series and related raw data through SRA is always painful. Here comes into play a pretty useful package called geofetch (Khoroshevskyi et al. (2023)). This can be easily installed through conda on the command line\nconda create -n geofetch\nconda activate geofetch\nconda install -c bioconda -c conda-forge geofetch sra-tools\n\nYou can also find a guide to anaconda in our ABC2 tutorial, where the packages geofetch and sra-tools can be installed interactively. geofetch works also with windows, though some options are not available.\n\n\n\n\n\n\n\nsettings for sra-tools\n\n\n\nsra-tools creates very large files in .sra format into your home folder while downloading. This can be detrimental on a cluster, for example on GenomeDK, where your home is limited in 100GB size. sra files are then converted into the desired format. You can decide to download sra files in to a temporary folder or any other folder of your choice. For example you can do what follows to change the folder for sra files into /tmp\nmkdir -p ${HOME}/.ncbi\necho '/repository/user/main/public/root = \"/tmp\"' &gt; ${HOME}/.ncbi/user-settings.mkfg\n\n\n\nDownload\ngeofetch makes it easy to download data - the command is very basic and different options can personalize which content you want to download. The whole download contain also metadata which follows the PEP format, a FAIR-ready way of documenting data for reproducible access by other users (Sheffield et al. (2021), LeRoy et al. (2024)). The basic command has this form:\ngeofetch -i SRA_SERIES_ID\nand one can add other arguments to choose the type of data to be downloaded according to this table:\n\n\n\nArguments to choose what to download with geofetch. The first line with empty arguments corresponds to the basic geofetch command.\n\n\nWe can download metadata and raw sra data for the GEO series we have in the first figure of this tutorial with the basic geofetch command\ngeofetch -i GSE67303\nOnce you are done, you have the following files and folders:\nGSE67303\nGSE67303\n├── GSE67303_GSE.soft\n├── GSE67303_GSM.soft\n├── GSE67303_PEP\n│   ├── GSE67303_PEP_raw.csv\n│   └── GSE67303_PEP.yaml\n├── GSE67303_SRA.csv\n\nGSE67303_GSE.soft is a text file containing series information\nGSE67303_GSM.soft is a text file containing samples information\nGSE67303_SRA.csv is a comma-separated table with the sra file lists who was downloaded\nthe files in the subfolder GSE67303_PEP contains data info following PEP standard\n\nTry to do ls in the folder used as default for the download of sra data, such as\nls /tmp/sra/\nand you should see the downloaded data from the sra archive\nSRR1930183.sra  SRR1930184.sra  SRR1930185.sra  SRR1930186.sra\nNow you can use fasterq-dump to convert the sra data into fastq format. fasterq-dump builds up on the tool fastq-dump, but includes some default settings, such as splitting the fastq files into paired reads and ambiguous reads (only when you actually have paired reads), which is usually the standard.\nIn this example we choose the locally downloaded files in the sra download folder, and we choose the output folder inside where we have the metadata information.\nfasterq-dump /tmp/sra/*.sra -O GSE67303/raw -v\nNow you can see the raw data is also there\nls /tmp/sra/\nGSE67303\n├── GSE67303_GSE.soft\n├── GSE67303_GSM.soft\n├── GSE67303_PEP\n│   ├── GSE67303_PEP_raw.csv\n│   └── GSE67303_PEP.yaml\n├── GSE67303_SRA.csv\n└── raw\n    ├── SRR1930183_1.fastq\n    ├── SRR1930183_2.fastq\n    ├── SRR1930184_1.fastq\n    ├── SRR1930184_2.fastq\n    ├── SRR1930185_1.fastq\n    ├── SRR1930185_2.fastq\n    ├── SRR1930186_1.fastq\n    └── SRR1930186_2.fastq\nIf you want, you can empty the sra download folder\nrm /tmp/sra/*.sra\n\n\n\nDownload SRA data with fasterq-dump\nYou can also use fasterq-dump directly if you do not want any metadata. It can be used with only one sra run at the time, but below we show how you can download entire series runs or sample runs.\n\nOnly one SRA run\nDo you simply need a single raw file? In the webpage of the GEO series, click on the SRA code. A new window will show all the samples: choose one and find the SRA run number, which starts with SRR (in figure below).\n\nIn relation to figure above\nfasterq-dump SRR1930186 -v\n\n\nAll SRA runs of a GEO series\nClick on the SRA link inside the GEO series. When the list with all samples appears, click on Send to on the top-right button, choose File and Accession List. This will download a csv file which you can combine with fasterq-dump to download all runs:\nfasterq-dump `sed 1,1d ./Downloads/SraAccList.csv` -v\n\n\nAll SRA runs of a single sample\nAfter clicking on the SRA code of the GEO series, you can click on a specific sample and use the same method with the Send to button to download all the runs of a specific sample."
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html#fastq-dump-command",
    "href": "documentation/2024-09-19-ABC6.html#fastq-dump-command",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "fastq-dump command",
    "text": "fastq-dump command\nfastq-dump is a tool for downloading sequencing reads from NCBI’s Sequence Read Archive (SRA). These sequence reads will be downloaded as FASTQ files. How these FASTQ files are formatted depends on the fastq-dump options used.\n\nDownloading reads from the SRA using fastq-dump\nIn this example, we want to download FASTQ reads for a mate-pair library.\nfastq-dump --gzip --skip-technical --readids --read-filter pass --dumpbase --split-3 --clip --outdir path/to/reads/ SRR_ID\nIn this command…\n\n--gzip: Compress output using gzip. Gzip archived reads can be read directly by bowtie2.\n--skip-technical: Dump only biological reads, skip the technical reads.\n--readids or -I: Append read ID after spot ID as ‘accession.spot.readid’. With this flag, one sequence gets appended the ID .1 and the other .2. Without this option, pair-ended reads will have identical IDs.\n--read-filter pass: Only returns reads that pass filtering (without Ns).\n--dumpbase or -B: Formats sequence using base space (default for other than SOLiD). Included to avoid colourspace (in which pairs of bases are represented by numbers).\n--split-3 separates the reads into left and right ends. If there is a left end without a matching right end, or a right end without a matching left end, they will be put in a single file.\n--clip or -W: Some of the sequences in the SRA contain tags that need to be removed. This will remove those sequences.\n--outdir or -O: (Optional) Output directory, default is current working directory.\nSRR_ID: This is is the ID of the run from SRA to be downloaded. This ID begins with “SRR” and is followed by around seven digits (e.g. SRA1234567).\n\nOther options that can be used instead of --split-3:\n\n--split-files splits the FASTQ reads into two files: one file for mate 1s (...1), and another for mate 2s (..._2). This option will not mateless pairs into a third file.\n--split-spot splits the FASTQ reads into two (mate 1s and mate 2s) within one file. --split-spot gives you an 8-line fastq format where forward precedes reverse (see https://www.biostars.org/p/178586/#258378)."
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html#fasterq-dump-command",
    "href": "documentation/2024-09-19-ABC6.html#fasterq-dump-command",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "fasterq-dump command",
    "text": "fasterq-dump command\nfasterq-dump is a tool for downloading sequencing reads from NCBI’s Sequence Read Archive (SRA). These sequence reads will be downloaded as fastq files. fasterq-dump is a newer, streamlined alternative to fastq-dump; both of these programs are a part of sra-tools.\n\nfasterq-dump vs fastq-dump\nHere are a few of the differences between fastq-dump and fasterq-dump:\n\nIn fastq-dump, the flag --split-3 is required to separate paired reads into left and right ends. This is the default setting in fasterq-dump.\nThe fastq-dump flag --skip-technical is no longer required to skip technical reads in fasterq-dump. Instead, the flag --include-technical is required to include technical reads when using fasterq-dump.\nThere is no --gzip or --bzip2 flag in fasterq-dump to download compressed reads with fasterq-dump. However, FASTQ files downloaded using fasterq-dump can still be subsequently compressed.\n\nThe following commands are equivalent, but will be executed faster using fasterq-dump:\nfastq-dump SRR_ID --split-3 --skip-technical\nfasterq-dump SRR_ID\n\n\nDownloading reads from the SRA using fasterq-dump\nIn this example, we want to download FASTQ reads for a mate-pair library.\nfastq-dump --threads n --progress SRR_ID\nIn this command…\n\n--threads specifies the number (n) processors/threads to be used.\n--progress is an optional argument that displays a progress bar when the reads are being downloaded.\nSRR_ID is the ID of the run from the SRA to be downloaded. This ID begins with “SRR” and is followed by around seven digits (e.g. SRR1234567)."
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html",
    "href": "documentation/2024-09-03-ABC4.html",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "",
    "text": "The difficulty of learning bash is often underestimated by others, who expect people approaching bioinformatics to learn it automatically. Here we try to put together the first basic concepts and commands.\n\n\n\n\n\n\nWhy the bash command line?\n\n\n\nUsing the bash command line becomes quickly essential if you are doing bioinformatics.\nFirst of all, you might need it to access a computing cluster (for example, GenomeDK at Aarhus University), since most clusters runs on a UNIX-based operating system, such as Linux, using a bash command line.\nJust as important is the fact that on a command line you can very easily do operations on multiple and very large files, something you would not be able to do using, for example, R or python. Large sequences of operations can be automatized into pipelines (an advanced topic not for this tutorial).\nWith a command line you can run many small programs, compose them together, and organize them in a chain of commands. This type of program organization fits well with what a bioinformatics project consist of: many tools to be applied repetitevely on multiple large files, and organizing those programs in a specific sequence. An example could be aligning to a reference genome many raw bulk-RNA sequencing files: the alignment operation must be repeated many times, and when files are finished, they might need to be merged if they are from the same sample.\n\n\n\n\nWhen using a UNIX operating system (Linux, MacOs), everything on your computer fits one of two categories: processes and files.\n\nProcesses are running instances of a program, and a program is any executable file stored in your computer.\nA file is any collection of data (program, image, video, audio, …).\n\nThe terminal is a text-based interface where you can write commands in various languages, depending on your choice. A terminal looks rather primitive, but it is what you often use on a daily basis to perform bioinformatics operations or control computing clusters.\nWhenever we write a command on the terminal and press enter, we have a shell receiving the code we wrote through the terminal. The shell is the outer layer of the operating system, which inteprets the commands and communicates them to the kernel.\nThe kernel is the core of the operating system, managing the computer physical components (hardware) and interfacing them with the processes that need to run. In general, any program (browser, game, …) you open or action (moving files, renaming folders, …) you do on your computer ends up being a process managed by the kernel. This communication process is shown in Figure 1\n\n\n\n\n\n\nFigure 1: Communication scheme where the outer layer is a bash shell command, which the shell then communicate to the kernel, which in turn manages the hardware resources to make the program actually run. Note that there can be many languages for the UNIX shell: bash is the most popular, but others exist and are used (for example zsh on MacOs). Figure credit: InnoKrea.\n\n\n\n\n\n\nWe can roughly identify various levels of efficiency, manual work, speed, number and size of handled files when working with a command line, the typical languages like R and python, or bash pipelines:\n\n\n\n\n\n\n\n\n\n\nProgramming mode\nNr of files\nFile Size\noperational speed\nManual work\n\n\n\n\nR, python, …\nfrom 1 to 10s\nsmall\npackage-dependent\nA lot\n\n\nCommand line\nfrom 1 to 100s\n1-10s GB\nfast\nLow-moderate\n\n\nUnix Pipeline\nfrom 1 to many 1000s\nmany TB\nfast\nLow\n\n\n\nYou will see in this tutorial how we can use basic bash utilities and handle text files. Those files would take longer time to read in R and python and the code to modify them would be in general longer and slower.\n\n\n\n\n\n\nShell vs Python-R?\n\n\n\nBoth the bash language and python or R are interpreted languages. You shouldn’t consider them as opposed to each other, but rather as complementary. Bash can do many things python or R cannot do very efficiently: search for files, look for patterns in long text files, concatenate operations, create pipelines, be the preferred system language on computing clusters. Python or R are great to perform for example statistical analysis.\nSo the best way to approach all those languages is to harness their advantages, instead of consider one of them “worse” than the others.\n\n\nWhat is Bash? Help you with this link if necessary\n\n\n  a package     a version of the Linux operating system     a language and an interpreter     a compiler which works only on Linux and Mac   Submit"
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#some-terminology",
    "href": "documentation/2024-09-03-ABC4.html#some-terminology",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "",
    "text": "When using a UNIX operating system (Linux, MacOs), everything on your computer fits one of two categories: processes and files.\n\nProcesses are running instances of a program, and a program is any executable file stored in your computer.\nA file is any collection of data (program, image, video, audio, …).\n\nThe terminal is a text-based interface where you can write commands in various languages, depending on your choice. A terminal looks rather primitive, but it is what you often use on a daily basis to perform bioinformatics operations or control computing clusters.\nWhenever we write a command on the terminal and press enter, we have a shell receiving the code we wrote through the terminal. The shell is the outer layer of the operating system, which inteprets the commands and communicates them to the kernel.\nThe kernel is the core of the operating system, managing the computer physical components (hardware) and interfacing them with the processes that need to run. In general, any program (browser, game, …) you open or action (moving files, renaming folders, …) you do on your computer ends up being a process managed by the kernel. This communication process is shown in Figure 1\n\n\n\n\n\n\nFigure 1: Communication scheme where the outer layer is a bash shell command, which the shell then communicate to the kernel, which in turn manages the hardware resources to make the program actually run. Note that there can be many languages for the UNIX shell: bash is the most popular, but others exist and are used (for example zsh on MacOs). Figure credit: InnoKrea."
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#efficiency-and-speed",
    "href": "documentation/2024-09-03-ABC4.html#efficiency-and-speed",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "",
    "text": "We can roughly identify various levels of efficiency, manual work, speed, number and size of handled files when working with a command line, the typical languages like R and python, or bash pipelines:\n\n\n\n\n\n\n\n\n\n\nProgramming mode\nNr of files\nFile Size\noperational speed\nManual work\n\n\n\n\nR, python, …\nfrom 1 to 10s\nsmall\npackage-dependent\nA lot\n\n\nCommand line\nfrom 1 to 100s\n1-10s GB\nfast\nLow-moderate\n\n\nUnix Pipeline\nfrom 1 to many 1000s\nmany TB\nfast\nLow\n\n\n\nYou will see in this tutorial how we can use basic bash utilities and handle text files. Those files would take longer time to read in R and python and the code to modify them would be in general longer and slower.\n\n\n\n\n\n\nShell vs Python-R?\n\n\n\nBoth the bash language and python or R are interpreted languages. You shouldn’t consider them as opposed to each other, but rather as complementary. Bash can do many things python or R cannot do very efficiently: search for files, look for patterns in long text files, concatenate operations, create pipelines, be the preferred system language on computing clusters. Python or R are great to perform for example statistical analysis.\nSo the best way to approach all those languages is to harness their advantages, instead of consider one of them “worse” than the others.\n\n\nWhat is Bash? Help you with this link if necessary\n\n\n  a package     a version of the Linux operating system     a language and an interpreter     a compiler which works only on Linux and Mac   Submit"
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#terminal-and-folders-anatomy",
    "href": "documentation/2024-09-03-ABC4.html#terminal-and-folders-anatomy",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "Terminal and folders anatomy",
    "text": "Terminal and folders anatomy\nWhen you work in the terminal, you will always see a prompt which starts with something of this type\n\nwhich provides you\n\nusername (e.g. samuele)\ncomputer name (e.g. D55749)\ncurrent working directory where the user is working at the moment (e.g. ~, which is the short form for the home directory)\n\nIn MobaXTerm you see only date, time and current working directory highlighted with various colors.\n\nHome directory ~\nThe home directory, which can be written as ~, is usually of the form /home/username, and is private to the user (no other users of that computer can access it).\nWhen you open the terminal, you always start with current working directory as your home. Try to write and execute (pressing enter) the command\npwd\nand you will see the full home path (which is the folder structure leading to your home directory)\n\n\nCurrent working directory (cwd)\nEvery command you execute refers to your cwd. For example, write\nls\nand you will see the list of files in your cwd. Try to create an empty file now with\ntouch emptyFile.txt\nand create a folder, which will be inside our cwd:\nmkdir myFolder\nIf you use again the ls command, the new file and folder will show in the cwd.\nNow we want to download something from the internet, for which we have a download link. We are getting a raw sequenced dataset in fastq format, which is currently gzip-compressed. The wget command can be used for the download. Note that now we also add an option -O to provide the output file name as ./myFolder/data.fastq.gz, where the dot . stands for the cwd, followed by myFolder, followed by the file name.\nwget https://github.com/hartwigmedical/testdata/raw/master/100k_reads_hiseq/TESTX/TESTX_H7YRLADXX_S1_L001_R1_001.fastq.gz -O ./myFolder/data.fastq.gz\n\n\n\n\n\n\nWarning\n\n\n\nNot all utilities are installed in MobaXTerm. If you get an error, install wget with the command\napt-get -y install wget\nand then try the download again.\n\n\n\n\nPaths and navigating the directory tree\nWe have already been using directories and paths a lot, so it is time to polish some definitions. Files are organized in a directory tree, which restricted to the tutorial we are running looks like Figure 2. Here home is a root folder, which is one of the top-level folder of your computer, so it has / at the beginning of its name.\n/home contains a folder with your username, which is right now your cwd. Inside your cwd you have the empty file and a directory myFolder containing the data.\nDirectories and files are organized with a tree hierarchy, so that /home is the first level, username the second, and myFolder the third level. The path in the directory tree to data.fastq.gz is expressed as /home/username/myFolder/data.fastq.gz.\n\n\n\n\n\n\nFigure 2: Directory tree of the tutorial. The home folder is a root folder (top-level of the tree) so its name starts with the / symbol. Other folders and files are at subsequent branch levels of the root folder /home. Some folders specific to your computer might be missing from this scheme.\n\n\n\n\nAbsolute and relative path\nThe path /home/username/myFolder/data.fastq.gz is called absolute because independent of your cwd. Let’s try another absolute path: the root folder /usr contains all executable files of the bash utilities we are using in this tutorial. Such files are in the folder bin. Execute\n\nls /usr/bin/\nThe output is a long list of executable files and some folders (sometimes they also have different colors, depending on your terminal settings). If you scroll and look, you can find familiar names like wget, ls, and so on. Now, the path /usr/bin is independent of our cwd.\nOn the contrary, the path ./myFolder/data.tar.gz depends on the cwd, and is equivalent to write myFolder/data.tar.gz, because ./ is always included by default (. represents the cwd). All paths that do not start with a root folder are relative!. To look inside myFolder, we can write\n\nls myFolder/\nHow do you write the command above using absolute paths? Remember that ~ correspondes to /home/username\n\n\n  ls /home/myFolder/     ls ~/myFolder     ls .   Submit\n\n\n\n\n\n\nNavigating folders\nHow to change your cwd? Simply use the command change directory. For example, you might want to work inside myFolder. Simply write\n\ncd myFolder\nand verify with pwd the new working directory path. If we want to unzip the compressed data file, we simply use its relative path:\ngunzip data.fastq.gz\nUse ls to verify that you have a file with name data.fastq."
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#working-with-text-files",
    "href": "documentation/2024-09-03-ABC4.html#working-with-text-files",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "Working with (text) files",
    "text": "Working with (text) files\nMany files you use in bioinformatics are nothing else than text files which are written in a specific matter. This specific way of arranging the text in the files gives you many of the file formats you encounter when doing bioinformatics.\n\n\n\n\n\n\nNote\n\n\n\nSome file formats are encoded differently than with plain text, and cannot usually be seen with a text editor.\n\n\n\nLess for reading files\nThe first thing you want to do, is to check at how the content of a file looks like. The program less is perfect for this purpose: you can scroll text with the arrows, and use the keyboard to do operations like searching for text or jumping to a line. Try\n\nless data.fastq\nand see the format of your data, where each four lines provide a sequence. The very first sequence you see should be\n@HISEQ_HU01:89:H7YRLADXX:1:1101:1116:2123 1:N:0:ATCACG\nTCTGTGTAAATTACCCAGCCTCACGTATTCCTTTAGAGCAATGCAAAACAGACTAGACAAAAGGCTTTTAAAAGTCTA\nATCTGAGATTCCTGACCAAATGT\n+\nCCCFFFFFHHHHHJJJJJJJJJJJJHIJJJJJJJJJIJJJJJJJJJJJJJJJJJJJHIJGHJIJJIJJJJJHHHHHHH\nFFFFFFFEDDEEEEDDDDDDDDD\nHere the first line is informative of the sequence, the second is the sequence itself and then you have an empty line (symbol +), followed by the quality scores (encoded by letters according to this table).\nTry to scroll in all directions (with the arrow keys) to explore some lines in the file. If you want to exit from the text viewer, press q.\n\nTry to search online how to look for a specific word in a file with less. Then visualize the data with less, and try to find if there is any sequence ten adjacent Ns (which is, ten missing nucleotides). Then, answer the question below\nHow long is the shortest sequence of missing nucleotides not found anywhere in the file data.fastq?\n\n\n  30     25     19   Submit\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsually, a bash shell command can be found by searching for the command + man, for example less man. The manual of less lists all the many (many many) functionalities that can be useful to explore text files, which is always a good activity to do for double checking if things are in order.\nIn general: always check the manuals or write on the prompt the option --help (such as less --help). Usually you will understand a lot of what you can do and how from that.\n\n\n\n\nFile properties\nHow big is a file? How many lines does it have? When has it been created? You can use the good old ls with some extra options to list all files with some extra info:\nls -lah \nYou should get something like this depending on your computer:\ntotalt 6,3M\ndrwxr-xr-x+ 1 au612681 UsersGrp    0 28 aug 11:38 .\ndrwx------+ 1 au612681 UsersGrp    0 28 aug 10:38 ..\n-rw-r--r--+ 1 au612681 UsersGrp 6,3M 28 aug 11:38 data.fastq\nthe last line is clearly the data file, with creation date and size. Other infos about the file are group and user ownership and which rights we have on the file (we will not talk about these in this tutorial).\n\n\n\n\n\n\nNote\n\n\n\nLook also at the other two elements: . and .., which represents the current directory and the one above in the directory structure. Those link you to the directories, so one can always change to the directory above by using cd ..\nYou can combine multiple times .. to change directories multiple levels above. Use the terminal and execute the command cd ../../. Which folder does it bring you to?\n\n\n  /home     /home/myFolder     /usr/bin     /home/username/   Submit\n\n\n\n\n\nOh damn, now we changed cwd, and we cannot remember the path to the previous one where we have the data. We can use -, which brings us to the previous cwd!\ncd -\nNow, how many lines are there in your file? The command wc can show that to you. It has many options, but we are going to use the one to count lines in a file. As always, look for the manual or examples to see how you can use it in other many ways.\nwc -l data.fastq\nwill tell you that the file has 100000 lines, so you have 25000 sequences (each sequence is defined by 4 lines).\n\n\nCopy and Move\nNow we learn a few useful commands because we want to work on more than one file. We will create multiple copies of our file. To make a copy of the data file and called the new one dataCopy.fastq, do\ncp data.fastq dataCopy.fastq\nThen we will move data.fastq. You could move it in any folder, even in the current directory. If you move it into the current directory, you can use the mv command to simply change its name:\nmv data.fastq dataOriginal.fastq\nIf you now use ls -lah, you will see that you have two files of identical size and different creation dates. But… are those files identical? diff can tell you if you provide two file names. It will print out differences (or nothing if files are identical)\ndiff dataOriginal.fastq dataCopy.fastq\n\n\nWriting on a file\nYou can write something on a file using &gt;. This writes any output from a command which would appear on the screen into a file. For example the following commands prints out the first four lines of the data, but they will end up inside the file fourLines.fastq\nhead -4 dataOriginal.fastq &gt; fourLines.fastq\nTry to use\n\ncat fourLines.fastq\nto see those four lines printed out directly on the screen from the file.\n\n\n\n\n\n\nWarning\n\n\n\nUsing again &gt; to write on the same file will not add text to the file, but will overwrite it with the new text.\n\n\nLet’s append the first four lines of the data to its copy without overwriting it. This can be done using &gt;&gt;:\n\nhead -4 dataOriginal.fastq &gt;&gt; dataCopy.fastq\nWe want to run again the diff command to compare the two sequence files. Do you need to rewrite it? Of course not! Your bash saves the command history.\nKeep CTRL (or CMD on Mac) pressed, and in the meanwhile press r, then start writing diff. The old command will show up and you can run it pressing Enter. Alternatively, press the arrow UP to see the history of commands backward.\nIf you run the diff command on the two files you will see an output. This shows quite a lot of information and not only the sequence which is different:\n--- dataOriginal.fastq\n+++ dataCopy.fastq\n@@ -99998,3 +99998,7 @@\n\nmeans that there are additional lines in the second file (+++), missing in dataOriginal.fastq (- - -), and their coordinates.\n\n\nPiping\nA last important concept is the pipe. You can create small pipelines directly on the shell with the symbol |. Every time you use the pipe symbol, you get the output of a command and send it to the next command. For example, the grep command can find a pattern in a file in this way\ngrep NNNNN dataOriginal.fastq\nNow, what if we want to find that pattern in the first hundred sequences? Easy! We use head, and then pipe it into grep! When doing this, you do not need to provide a file to grep, and in that way it will read the output of the pipe.\nhead -400 dataOriginal.fastq | grep NNNNN\nThat was cool! In this case we have a small output on screen, but the output can be really huge as well. Maybe it makes sense to count how many sequences grep can find. Well, can’t really do it by hand! Pipe the result once more using wc as we did earlier!\nhead -400 dataOriginal.fastq | grep NNNNN | wc -l\nWait a second! That sequence absolutely needs to be saved into a file. How do you do it!\n\n\n  head -400 dataOriginal.fastq | grep NNNNN | sequence.fastq     head -400 dataOriginal.fastq | grep NNNNN &lt; sequence.fastq     sequence.fastq &lt; head -400 dataOriginal.fastq | grep NNNNN\n    head -400 dataOriginal.fastq | grep NNNNN &gt; sequence.fastq   Submit\n\n\n\n\n\nShortcuts and useful keys combinations\nThis was all for this very introductory tutorial. You have already seen how the bash shell is flexible, though requiring a bit of a steep learning curve.\nWe want to recap here useful shortcuts and key combinations for your day-to-day use of the bash shell\nHere’s a list of useful shortcuts and key combinations that you can use in the Bash shell to improve your efficiency:\n\nNavigating the Command Line\n\nCtrl + A: Move the cursor to the beginning of the line.\nCtrl + E: Move the cursor to the end of the line.\nAlt + F: Move the cursor forward one word.\nAlt + B: Move the cursor backward one word.\nCtrl + L: Clear the terminal screen\n\n\n\nEditing Text\n\nCtrl + U: Delete the text from the cursor to the beginning of the line.\nCtrl + K: Delete the text from the cursor to the end of the line.\nCtrl + W: Delete the word before the cursor.\nAlt + D: Delete the word after the cursor.\n\n\n\nHistory Navigation\n\nCtrl + R: Search through command history interactively (reverse-i-search).\nCtrl + P: Previous command in history (equivalent to the up arrow key).\nCtrl + N: Next command in history (equivalent to the down arrow key).\n!!: Re-run the last command.\n\n\n\nCommand Completion and Expansion\n\nTab: Auto-complete file names, directory names, or commands.\nAlt + .: Use the last argument of the previous command in the current command line.\n\n\n\nOther Useful Shortcuts\n\nCtrl + C: Terminate the current command (useful if things are stuck).\nCtrl + D: Log out of the current shell (equivalent to exit).\nCtrl + T: Swap the last two characters before the cursor.\n\n\n\n\n\n\n\nwrap up\n\n\n\nNow you are able to navigate the directory tree and visualize text files with less. You can also perform some basic operations, including piping commands, which is useful when handling large outputs which might end up directly on your screen."
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "About the Sandbox",
    "section": "",
    "text": "An infrastructure project for health data science training and research in Denmark\nThe National Health Data Science Sandbox project kicked off in 2021 with 5 years of funding via the Data Science Research Infrastructure initiative from the Novo Nordisk Foundation. Health data science experts at five Danish universities are contributing to the Sandbox with coordination from the Center for Health Data Science under lead PI Professor Anders Krogh. Data scientists hosted in the research groups of each PI are building infrastructure and training modules on Computerome and UCloud, the primary academic high performance computing (HPC) platforms in Denmark. If you have any questions or would like to get in touch with one of our data scientists, please contact us here.\n\n\n\n\n\nOur computational ‘sandbox’ allows data scientists to explore datasets, tools and analysis pipelines in the same high performance computing environments where real research projects are conducted. Rather than a single, hefty environment, we’re deploying modularized topical environments tailored for independent use on each HPC platform. We aim to support three key user groups based at Danish universities:\n\ntrainees: use our training modules to learn analysis techniques with some guidance and guardrails - for your data type of interest AND for general good practices for HPC environments\n\nresearchers: prototype your tools and algorithms with an array of good quality datasets that are GDPR compliant and free to access\neducators: develop your next course with computational assignments in the HPC environment your students will use for their research\n\nActivity developing independent training modules and hosting workshops has centered on UCloud, while collaborative construction of a flexible Course Platform has been completed on Computerome for use by the Sandbox and independent educators. Publicly sourced datasets are being used in training modules on UCloud, while generation of synthetic data is an ongoing project at Computerome. Sandbox resources are under active construction, so check out our other pages for the current status on HPC Access, Datasets, and Modules. We run workshops using completed training modules on a regular basis and provide active support for Sandbox-hosted courses through a slack workspace. See our Contact page for more information.\n\n\nPartner with the Sandbox\nThe Sandbox welcomes proposals for new courses, modules, and prototyping projects from researchers and educators. We’d like to partner with lecturers engaged with us in developing needed materials collaboratively - we would love to have input from subject experts or help promote exciting new tools and analysis methods via modules! Please contact us with your ideas at nhds_sandbox@sund.ku.dk.\n\nWe thank the Novo Nordisk Foundation for funding support. If you use the Sandbox for research or reference it in text or presentations, please acknowledge the Health Data Science Sandbox project and its funder the Novo Nordisk Foundation (grant number NNF20OC0063268)."
  },
  {
    "objectID": "binfResources/bash.html",
    "href": "binfResources/bash.html",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "awk '$2~/^-$/ { next } { print }' file_input &gt; file_output\n\n\n\nawk -F \"\\\"*,\\\"*\" '{print $M}' file.csv\n\n\n\nawk '{ $1=\"\"; $2=\"\"; print}' filename\n\n\n\nsed -n '10,20p' myFile\n\n\n\nsed -n -e 1p -e 2p  myFile\n\n\n\n sed -n '$L~$Mp' myFile\n\n\n\ncut -f1 20141020.combined_mask.whole_genomeV2.bed | sort -n | uniq -c\n\n\n\nsed -e \"s/string1/string2/\" myFile \n\n\n\nsort -k 3,3 myFile\n\n\n\ngrep -v -e \"pattern\" myFile &gt; newFile\n\n\ngrep -v -n -e \"pattern\" myFile | cut -f1 -d\":\"\n\ngrep -v -e \"pattern1\\|pattern2\" myFile &gt; newFile\n\n\n\n\n\n\n\necho $a $b $c | tr -s ' ' '\\t'\n\n\n\nperl -E \"say $a + $b\"\n\n\n\n\n\n\nls *.zip | xargs -n 1 basename -s .zip\n\n\n\nfind . -type f ! -name '*.gz' -exec gzip \"{}\" \\;\n\n\n\n\n\n\nwget -i file.txt\n\n\n\nfor var in `ls -d */`; do echo ${var::-1}; done"
  },
  {
    "objectID": "binfResources/bash.html#handling-text-files",
    "href": "binfResources/bash.html#handling-text-files",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "awk '$2~/^-$/ { next } { print }' file_input &gt; file_output\n\n\n\nawk -F \"\\\"*,\\\"*\" '{print $M}' file.csv\n\n\n\nawk '{ $1=\"\"; $2=\"\"; print}' filename\n\n\n\nsed -n '10,20p' myFile\n\n\n\nsed -n -e 1p -e 2p  myFile\n\n\n\n sed -n '$L~$Mp' myFile\n\n\n\ncut -f1 20141020.combined_mask.whole_genomeV2.bed | sort -n | uniq -c\n\n\n\nsed -e \"s/string1/string2/\" myFile \n\n\n\nsort -k 3,3 myFile\n\n\n\ngrep -v -e \"pattern\" myFile &gt; newFile\n\n\ngrep -v -n -e \"pattern\" myFile | cut -f1 -d\":\"\n\ngrep -v -e \"pattern1\\|pattern2\" myFile &gt; newFile"
  },
  {
    "objectID": "binfResources/bash.html#handling-variables",
    "href": "binfResources/bash.html#handling-variables",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "echo $a $b $c | tr -s ' ' '\\t'\n\n\n\nperl -E \"say $a + $b\""
  },
  {
    "objectID": "binfResources/bash.html#managing-files",
    "href": "binfResources/bash.html#managing-files",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "ls *.zip | xargs -n 1 basename -s .zip\n\n\n\nfind . -type f ! -name '*.gz' -exec gzip \"{}\" \\;"
  },
  {
    "objectID": "binfResources/bash.html#transfering-files",
    "href": "binfResources/bash.html#transfering-files",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "wget -i file.txt\n\n\n\nfor var in `ls -d */`; do echo ${var::-1}; done"
  },
  {
    "objectID": "binfResources/databases.html",
    "href": "binfResources/databases.html",
    "title": "Useful tips for databases",
    "section": "",
    "text": "Useful tips for databases\nNothing yet\n\n\n\n\n\n\nthis is my tip\n\n\n\nhello I am Marie"
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "Events calendar",
    "section": "",
    "text": "The ABC will routinely post new meetings and related workshops/courses of interest here.\n\n\n\n\n\n\n\n\n\n\n\n\nUse GenomeDK workshop\n\n\n\nWorkshop\n\n\nGenomeDK\n\n\nIntroduction\n\n\nHPC\n\n\ncluster\n\n\n\nAll you need to get up and running with GenomeDK\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nABC.7: Seventh ABC session\n\n\n\nCafe\n\n\nCoding\n\n\nBioinformatics\n\n\n\nNew tutorial, cake and coffee. And of course… help :)\n\n\n\nOct 24, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Event title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nEvent title\n\n\nDates\n\n\nLocation\n\n\nOrganizers\n\n\n\n\n\n\nABC.6: Sixth ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.5: Fifth ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.4: Fourth ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.3: Third ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.2: Second ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.1: First ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news/upcoming/2024-10-24-session7.html",
    "href": "news/upcoming/2024-10-24-session7.html",
    "title": "ABC.7: Seventh ABC session",
    "section": "",
    "text": "The seventh session of the ABC (Accessible Bioinformatics Cafe) will be on october 24th, at 13:00 - building 1240-218\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/upcoming/2024-10-24-session7.html#agenda",
    "href": "news/upcoming/2024-10-24-session7.html#agenda",
    "title": "ABC.7: Seventh ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. You can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/upcoming/2024-10-24-session7.html#signing-up",
    "href": "news/upcoming/2024-10-24-session7.html#signing-up",
    "title": "ABC.7: Seventh ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n https://lnkd.in/dRendtca"
  },
  {
    "objectID": "news/past/2024-06-27-session2.html",
    "href": "news/past/2024-06-27-session2.html",
    "title": "ABC.2: Second ABC session",
    "section": "",
    "text": "The second session of the ABC (Accessible Bioinformatics Cafe) will be on june 27th, at 13:00 in the hall of AIAS (the Aarhus Institute of Advanced Studies, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-06-27-session2.html#agenda",
    "href": "news/past/2024-06-27-session2.html#agenda",
    "title": "ABC.2: Second ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented) at each meeting. This time we have a choice of\n\nA small dplyr project managed with RStudio\nInstall Anaconda and python basics"
  },
  {
    "objectID": "news/past/2024-06-27-session2.html#signing-up",
    "href": "news/past/2024-06-27-session2.html#signing-up",
    "title": "ABC.2: Second ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake, click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2024-09-03-session4.html",
    "href": "news/past/2024-09-03-session4.html",
    "title": "ABC.4: Fourth ABC session",
    "section": "",
    "text": "The fourth session of the ABC (Accessible Bioinformatics Cafe) will be on september 3rd, at 13:00 in building 1231 room 320 (by the lake, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-09-03-session4.html#agenda",
    "href": "news/past/2024-09-03-session4.html#agenda",
    "title": "ABC.4: Fourth ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. This time we have\n\nintroduction to bash for bioinformatics\n\nYou can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/past/2024-09-03-session4.html#signing-up",
    "href": "news/past/2024-09-03-session4.html#signing-up",
    "title": "ABC.4: Fourth ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2024-09-19-session5.html",
    "href": "news/past/2024-09-19-session5.html",
    "title": "ABC.5: Fifth ABC session",
    "section": "",
    "text": "The fifth session of the ABC (Accessible Bioinformatics Cafe) will be on september 19th, at 13:00 in building 1231 room 114 (by the lake, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-09-19-session5.html#agenda",
    "href": "news/past/2024-09-19-session5.html#agenda",
    "title": "ABC.5: Fifth ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nWhat’s on our screens\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. For this meeting we do not have new tutorials, but will rely on the previous ones and on an interactive single cell workshop one of us have been giving at a conference.\nOf course, you can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/past/2024-09-19-session5.html#signing-up",
    "href": "news/past/2024-09-19-session5.html#signing-up",
    "title": "ABC.5: Fifth ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  }
]