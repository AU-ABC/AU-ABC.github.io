[
  {
    "objectID": "Documentation.html",
    "href": "Documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "While it is always worth remembering an event which offers free cake (especially when it recurs with a frequency like the ABC, as you could read here), you might want to read again what has been done and said after the sugar levels drop down.\nOur tutorials and instructions are self contained, thus useful for anyone, also people not attending a session.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.1: Install R+Rstudio, first R coding problems/solutions\n\n\n\nR\n\n\nRStudio\n\n\nDataframe\n\n\n\nThings worth remembering from the ABC.1\n\n\n\nJun 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.2: RStudio projects and Anaconda+Python\n\n\n\nR\n\n\nRStudio\n\n\nAnaconda\n\n\nPython\n\n\nenvironments\n\n\n\nThings worth remembering from the ABC.2\n\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.3: R analysis of bulkRNA expression matrix\n\n\n\nR\n\n\nbulkRNA\n\n\n\nThings worth remembering from the ABC.3\n\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.4: Introduction to bash language for bioinformatics\n\n\n\nbash\n\n\ncommand line\n\n\n\nSlides and bash intro at the ABC.4\n\n\n\nSep 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.5: open coding and tutorials, single cell dimensionality reduction\n\n\n\nscanpy\n\n\nprojection\n\n\nPCA\n\n\nUMAP\n\n\n\nABC.5: try our previous tutorials or a workshop on dimensionality reduction\n\n\n\nSep 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.6: Download from online databases, open coding session\n\n\n\nGEO\n\n\nSRA\n\n\nDownload\n\n\n\nLearn how to download (especially) GEO-SRA data\n\n\n\nOct 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop: Use GenomeDK\n\n\n\nGenomeDK\n\n\nbash\n\n\n\nIntroductive workshop to genomeDK\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.7: Practical guide to SLURM and job submissions on HPC\n\n\n\nSlurm\n\n\nJobs\n\n\nHPC\n\n\n\nLearn how to create jobs on HPC and troubleshoot bugs/problems\n\n\n\nOct 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.9: Graphical applications on GenomeDK\n\n\n\nJupyterlab\n\n\nRstudio\n\n\nLLM\n\n\nGenomeDK\n\n\n\nLearn how to launch graphical applications on GenomeDK and to integrate an LLM chatbot in Jupyterlab\n\n\n\nNov 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.10: Detailed bulkRNA data alignment and quality control\n\n\n\nbulkRNA\n\n\nalignment\n\n\nQC\n\n\nSTAR\n\n\n\nTry to align a bulk dataset and perform quality control!\n\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing\n\n\n\nscrna\n\n\nseurat\n\n\nR\n\n\n\nlearn about seurat object structure and preprocessing of scrnaSeq data\n\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.12: Pixi and Desktop on genomeDK, R-clusterProfiler analysis\n\n\n\nGenomeDK\n\n\npixi\n\n\nconda\n\n\npackage-environment\n\n\nR\n\n\nclusterProfiler\n\n\npathway-analysis\n\n\n\nPathway enrichment analysis with R package clusterProfiler\n\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.13: BulkRNA deconvolution\n\n\n\nDeconvolution\n\n\nomnideconv\n\n\nscRNA-seq\n\n\nbulkRNA-seq\n\n\n\nBulkRNA deconvolution with scRNA-seq data\n\n\n\nFeb 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "gethelp.html",
    "href": "gethelp.html",
    "title": "Behind the ABC - Who we are",
    "section": "",
    "text": "Behind the ABC - Who we are\nWe organize the cafe as a collaboration between the Danish Health Data Science Sandbox, the Core Bioinformatics Facility, and PhD students whose fields touch bioinformatics and health data science.\nOur shared email address for general enquiries about the cafe is abcafe au.dk (with the @ in between).\n\n\n\n\n\n\n\nPeople\nWhat do I do\n\n\n\n\n\nPer Qvist: Ass.Prof. at AU biomed, leads as well the Core Bioinformatics Facility. Most importantly, has been managing constant cake and drinks flow to the ABC, which has been making him invaluable to all participants!\n\n\n\nMarie Sønderstrup: Phd Student at The department of biomedicine. Is the inspirator of the ABC, after living through the deep frustration of understanding how to code.\n\n\n\nManuel Peral Vazquez: Phd Student at Department of Molecular Biology and Genetics - Cellular Health, Intervention, and Nutrition. He loves to dig into the amenities of the bash command line and trying out interesting things.\n\n\n\nSamuele Soraggi: Special consultant at the department of bioinformatics after a migration from the field of mathematics. He often throws himself to unknown buggy pieces of code with duct tape and a hammer until they work. When he does not succeed, he deletes them and empty the trash bin.\n\n\n\nDimitrios Pediotidis-Maniatis: Consultant at the bioinformatics core facility. Probably the person with the highest attendance to the ABC! He masters a lot of things in bash and R - test him out!\n\n\n\njacob Egemose Høgfeldt: He is also consultant at the bioinformatics core facility, and knows a great deal of bioinformatics and data science. He is the absolute go-to person for bulkRNA sequencing questions together with Dimitrios.\n\n\n\n\n\nBeyond the ABC - in depth bioinformatics support\nSome of us work at Aarhus University as consultants, providing support for various HPC and bioinformatics topics and analysis, in two different ways:\n\n\n\nWho\nHelp and contact\n\n\n\n\n\nSamuele from the Health Data Science Sandbox provides some free consultancy hours for technical issues, bioinformatics setup for coding and perform analysis, machine learning practices, both on PC and computing cluster, and organizes a wide array of workshops, both on demand and on own initiative. You can book a short meeting with Samuele here to introduce the problem and setup eventual solutions, or write at samuele birc.au.dk (with @ in between).\n\n\n \nDimitrios and Jacob work at the Bioinformatics Core Facility, led by Ass. Prof. Per Qvist. They offer expert assistance in a wide range of bioinformatics services, including data analysis, storage, visualization and advice on experiment design. The Bioinformatics Core Facility primarily operate on a fee for service model of data analysis but also offer full joint scientific collaborations and training. You can find more details, including project examples and pricing, on their website (https://biomed.au.dk/bioinformatics-core-facility). Feel free to reach out for a free initial consultation regarding your projects by emailing to bioinformatics [at] biomed.au.dk."
  },
  {
    "objectID": "news/upcoming/2025-06-04.html",
    "href": "news/upcoming/2025-06-04.html",
    "title": "Workshop: Pipelines on GenomeDK",
    "section": "",
    "text": "Are you a student or academic, current or future user of the GenomeDK high-performance computing (HPC) system? Or are you just curious about HPC and how you can utilize it in your work? Do you need to learn to use GenomeDK, or wish to become a better and more organized user?\nThen join us in this series of GenomeDK workshops where we guide you from a general introduction to advanced usage. Each workshop targets increasing difficulty levels, so you can join the sessions that fit you best. The third workshop is for creating reproducible pipelines.\nThis workshop will be hosted by Samuele Soraggi (BiRC, MBG) and Dan Søndergaard (GenomeDK)."
  },
  {
    "objectID": "news/upcoming/2025-06-04.html#signing-up",
    "href": "news/upcoming/2025-06-04.html#signing-up",
    "title": "Workshop: Pipelines on GenomeDK",
    "section": "Signing up",
    "text": "Signing up\nEach workshop is scheduled for four hours, including a 45 min. lunch break. There will be cake, coffee, and tea for attendants, so please remember to sign up! There are 30 seats available and a waiting list.\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2024-06-27-session2.html",
    "href": "news/past/2024-06-27-session2.html",
    "title": "ABC.2: Second ABC session",
    "section": "",
    "text": "The second session of the ABC (Accessible Bioinformatics Cafe) will be on june 27th, at 13:00 in the hall of AIAS (the Aarhus Institute of Advanced Studies, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-06-27-session2.html#agenda",
    "href": "news/past/2024-06-27-session2.html#agenda",
    "title": "ABC.2: Second ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented) at each meeting. This time we have a choice of\n\nA small dplyr project managed with RStudio\nInstall Anaconda and python basics"
  },
  {
    "objectID": "news/past/2024-06-27-session2.html#signing-up",
    "href": "news/past/2024-06-27-session2.html#signing-up",
    "title": "ABC.2: Second ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake, click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2025-01-30-session12.html",
    "href": "news/past/2025-01-30-session12.html",
    "title": "ABC.12: Accessible Bioinformatics Cafe",
    "section": "",
    "text": "The 12th session of the ABC (Accessible Bioinformatics Cafe) will be on january 30th, at 13:00-15:00 in building 1231-114\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2025-01-30-session12.html#agenda",
    "href": "news/past/2025-01-30-session12.html#agenda",
    "title": "ABC.12: Accessible Bioinformatics Cafe",
    "section": "Agenda",
    "text": "Agenda\nFor this ABC Dimitrios will show how to do pathways analysis in R using the package clusterProfiler"
  },
  {
    "objectID": "news/past/2025-01-30-session12.html#signing-up",
    "href": "news/past/2025-01-30-session12.html#signing-up",
    "title": "ABC.12: Accessible Bioinformatics Cafe",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up to ensure there is enough cake (and you are pretty sure you will show up): click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2024-06-13-session1.html",
    "href": "news/past/2024-06-13-session1.html",
    "title": "ABC.1: First ABC session",
    "section": "",
    "text": "The very first session of the ABC (Accessible Bioinformatics Cafe) will launch on june 13th, at 13:00 in the hall of AIAS (the Aarhus Institute of Advanced Studies, see below).\nEveryone is welcome to join independently of coding skills and level. In this first meeting we will also distribute a survey, so that participants can give some ideas about expectations and topics of interest."
  },
  {
    "objectID": "news/past/2024-06-13-session1.html#purpose",
    "href": "news/past/2024-06-13-session1.html#purpose",
    "title": "ABC.1: First ABC session",
    "section": "Purpose",
    "text": "Purpose\nThe ABC aims to:\n\nSupport and guide in using bioinformatics and programming tools.\nProvide support for coding-related issues.\nEnhance the coding skills of participants in fundamental programming languages (R, python, bash command line)."
  },
  {
    "objectID": "news/past/2024-06-13-session1.html#who-can-benefit-from-the-abc",
    "href": "news/past/2024-06-13-session1.html#who-can-benefit-from-the-abc",
    "title": "ABC.1: First ABC session",
    "section": "Who can benefit from the ABC?",
    "text": "Who can benefit from the ABC?\nEveryone, really. Those that will particularly benefit from the ABC include:\n\nBiologists, bioinformaticians, and health scientists who wish to adopt coding-based solutions.\nScientists and students who wish to accelerate the coding-based analysis of various types of data (such as OMICs data and other large-scale data) in research across Aarhus University and Aarhus University Hospital.\nStudents and researchers who want an open environment where it is easy to get assistance and talk with others encountering similar issues (or maybe who already solved them!)"
  },
  {
    "objectID": "news/past/2024-06-13-session1.html#format",
    "href": "news/past/2024-06-13-session1.html#format",
    "title": "ABC.1: First ABC session",
    "section": "Format",
    "text": "Format\nEach 2 hour session of the ABC will feature:\n\nTopic Presentation: Insightful presentations on various bioinformatics and coding topics.\nOpen Floor Session: A hands-on segment where staff from the Bioinformatics Core Facility and the Health Data Science Sandbox will be available to assist with any coding or bioinformatics issues."
  },
  {
    "objectID": "news/past/2024-08-20-session3.html",
    "href": "news/past/2024-08-20-session3.html",
    "title": "ABC.3: Third ABC session",
    "section": "",
    "text": "The third session of the ABC (Accessible Bioinformatics Cafe) will be on august 20th, at 12:30 in the hall of AIAS (the Aarhus Institute of Advanced Studies, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-08-20-session3.html#agenda",
    "href": "news/past/2024-08-20-session3.html#agenda",
    "title": "ABC.3: Third ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. This time we have\n\nR analysis of bulkRNA sequencing data"
  },
  {
    "objectID": "news/past/2024-08-20-session3.html#signing-up",
    "href": "news/past/2024-08-20-session3.html#signing-up",
    "title": "ABC.3: Third ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2024-11-28-session9.html",
    "href": "news/past/2024-11-28-session9.html",
    "title": "ABC.9: Accessible Bioinformatics Cafe",
    "section": "",
    "text": "The ninth session of the ABC (Accessible Bioinformatics Cafe) will be on november 28th, at 13:00-15:00 in building 1873-118A\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-11-28-session9.html#agenda",
    "href": "news/past/2024-11-28-session9.html#agenda",
    "title": "ABC.9: Accessible Bioinformatics Cafe",
    "section": "Agenda",
    "text": "Agenda\nFor this ABC we show how to use graphical applications on GenomeDK, and specifically show Rstudio and Jupyterlab."
  },
  {
    "objectID": "news/past/2024-11-28-session9.html#signing-up",
    "href": "news/past/2024-11-28-session9.html#signing-up",
    "title": "ABC.9: Accessible Bioinformatics Cafe",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up to ensure there is enough cake (and you are pretty sure you will show up): click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2024-10-24-session7.html",
    "href": "news/past/2024-10-24-session7.html",
    "title": "ABC.7: Seventh ABC session",
    "section": "",
    "text": "The seventh session of the ABC (Accessible Bioinformatics Cafe) will be on october 24th, at 13:00 - building 1240-218\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-10-24-session7.html#agenda",
    "href": "news/past/2024-10-24-session7.html#agenda",
    "title": "ABC.7: Seventh ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. You can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/past/2024-10-24-session7.html#signing-up",
    "href": "news/past/2024-10-24-session7.html#signing-up",
    "title": "ABC.7: Seventh ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign-up"
  },
  {
    "objectID": "news/past/2024-09-19-session5.html",
    "href": "news/past/2024-09-19-session5.html",
    "title": "ABC.5: Fifth ABC session",
    "section": "",
    "text": "The fifth session of the ABC (Accessible Bioinformatics Cafe) will be on september 19th, at 13:00 in building 1231 room 114 (by the lake, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-09-19-session5.html#agenda",
    "href": "news/past/2024-09-19-session5.html#agenda",
    "title": "ABC.5: Fifth ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nWhat’s on our screens\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. For this meeting we do not have new tutorials, but will rely on the previous ones and on an interactive single cell workshop one of us have been giving at a conference.\nOf course, you can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/past/2024-09-19-session5.html#signing-up",
    "href": "news/past/2024-09-19-session5.html#signing-up",
    "title": "ABC.5: Fifth ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2024-10-03-session6.html",
    "href": "news/past/2024-10-03-session6.html",
    "title": "ABC.6: Sixth ABC session",
    "section": "",
    "text": "The fourth session of the ABC (Accessible Bioinformatics Cafe) will be on october 3rd, at 13:00 in building 1873 room 118A (University city Molecular Biology Building, room by the canteen, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-10-03-session6.html#agenda",
    "href": "news/past/2024-10-03-session6.html#agenda",
    "title": "ABC.6: Sixth ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. This time we have\n\ndownload of open datasets from online databases\n\nYou can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/past/2024-10-03-session6.html#signing-up",
    "href": "news/past/2024-10-03-session6.html#signing-up",
    "title": "ABC.6: Sixth ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "binfResources/bash.html",
    "href": "binfResources/bash.html",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "awk '$2~/^-$/ { next } { print }' file_input &gt; file_output\n\n\n\nawk -F \"\\\"*,\\\"*\" '{print $M}' file.csv\n\n\n\nawk '{ $1=\"\"; $2=\"\"; print}' filename\n\n\n\nsed -n '10,20p' myFile\n\n\n\nsed -n -e 1p -e 2p  myFile\n\n\n\n sed -n '$L~$Mp' myFile\n\n\n\ncut -f1 20141020.combined_mask.whole_genomeV2.bed | sort -n | uniq -c\n\n\n\nsed -e \"s/string1/string2/\" myFile \n\n\n\nsort -k 3,3 myFile\n\n\n\ngrep -v -e \"pattern\" myFile &gt; newFile\n\n\ngrep -v -n -e \"pattern\" myFile | cut -f1 -d\":\"\n\ngrep -v -e \"pattern1\\|pattern2\" myFile &gt; newFile\n\n\n\n\n\n\n\necho $a $b $c | tr -s ' ' '\\t'\n\n\n\nperl -E \"say $a + $b\"\n\n\n\n\n\n\nls *.zip | xargs -n 1 basename -s .zip\n\n\n\nfind . -type f ! -name '*.gz' -exec gzip \"{}\" \\;\n\n\n\n\n\n\nwget -i file.txt\n\n\n\nfor var in `ls -d */`; do echo ${var::-1}; done"
  },
  {
    "objectID": "binfResources/bash.html#handling-text-files",
    "href": "binfResources/bash.html#handling-text-files",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "awk '$2~/^-$/ { next } { print }' file_input &gt; file_output\n\n\n\nawk -F \"\\\"*,\\\"*\" '{print $M}' file.csv\n\n\n\nawk '{ $1=\"\"; $2=\"\"; print}' filename\n\n\n\nsed -n '10,20p' myFile\n\n\n\nsed -n -e 1p -e 2p  myFile\n\n\n\n sed -n '$L~$Mp' myFile\n\n\n\ncut -f1 20141020.combined_mask.whole_genomeV2.bed | sort -n | uniq -c\n\n\n\nsed -e \"s/string1/string2/\" myFile \n\n\n\nsort -k 3,3 myFile\n\n\n\ngrep -v -e \"pattern\" myFile &gt; newFile\n\n\ngrep -v -n -e \"pattern\" myFile | cut -f1 -d\":\"\n\ngrep -v -e \"pattern1\\|pattern2\" myFile &gt; newFile"
  },
  {
    "objectID": "binfResources/bash.html#handling-variables",
    "href": "binfResources/bash.html#handling-variables",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "echo $a $b $c | tr -s ' ' '\\t'\n\n\n\nperl -E \"say $a + $b\""
  },
  {
    "objectID": "binfResources/bash.html#managing-files",
    "href": "binfResources/bash.html#managing-files",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "ls *.zip | xargs -n 1 basename -s .zip\n\n\n\nfind . -type f ! -name '*.gz' -exec gzip \"{}\" \\;"
  },
  {
    "objectID": "binfResources/bash.html#transfering-files",
    "href": "binfResources/bash.html#transfering-files",
    "title": "Useful tips for day-to-day bash",
    "section": "",
    "text": "wget -i file.txt\n\n\n\nfor var in `ls -d */`; do echo ${var::-1}; done"
  },
  {
    "objectID": "binfResources/python.html",
    "href": "binfResources/python.html",
    "title": "Useful tips for python",
    "section": "",
    "text": "Useful tips for python"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "",
    "text": "Incoming Events\n\n\n\nThe 14th ABC will take place on march 13th, 2025, 13:00-15:00. Drop in for help or for following our tutorials!\n \n\n Information + signup"
  },
  {
    "objectID": "index.html#concept",
    "href": "index.html#concept",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Concept",
    "text": "Concept\n\n\n\n\n\n\nHave you ever ended in a situation involving questions similar to those?\n\nWhere do I start to code on a command line?\n How do I learn the basic R/python commands?\n What the heck is this error?????\n\nDo you feel finding the right tools is like going through a maze of software installations, versions and incompatibilities?\n\n\n\nWhat follows is the natural desire of throwing your computer out of the window. Don’t worry! All the bioinformaticians/data scientists have gone through that, sometimes defined as part of the Kubler-Ross curve1, where frustration is the point in which coding becomes challenging and we might feel lost.\nThe idea behind ABC is to bridge you beyond frustration moments, while at the same time offering a relaxed environment and the possibility to learn new things, help and get to know each other:"
  },
  {
    "objectID": "index.html#who-can-benefit-from-the-abc",
    "href": "index.html#who-can-benefit-from-the-abc",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Who can benefit from the ABC?",
    "text": "Who can benefit from the ABC?\nEveryone, really. Those that will particularly benefit from the ABC include:\n\nBiologists, bioinformaticians, and health scientists who wish to adopt coding-based solutions.\nScientists and students who wish to accelerate the coding-based analysis of various types of data (such as OMICs data and other large-scale data) in research across Aarhus University and Aarhus University Hospital.\nStudents and researchers who want an open environment where it is easy to get assistance and talk with others encountering similar issues (or maybe who already solved them!)"
  },
  {
    "objectID": "index.html#format",
    "href": "index.html#format",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Format",
    "text": "Format\nEach ~2 hour session of the ABC will feature:\n\nTopic Presentation: Insightful presentations on various bioinformatics and coding topics.\nOpen Floor Session: A hands-on segment where staff from the Bioinformatics Core Facility and the Health Data Science Sandbox will be available to assist with any coding or bioinformatics issues."
  },
  {
    "objectID": "index.html#when-and-where",
    "href": "index.html#when-and-where",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "When and where",
    "text": "When and where\nLook at our Events calendar for more information about each meeting and upcoming workshops and courses."
  },
  {
    "objectID": "index.html#newsletter",
    "href": "index.html#newsletter",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Newsletter",
    "text": "Newsletter\nSubscribe to our newsletter to receive a few, non-invasive reminders for our meetings and events!"
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "Welcome to the Accessible Bioinformatics Cafe",
    "section": "Funding",
    "text": "Funding\n\n\n\nFor all 2025 the cafe is supported by the Danish Data Science Foundation!"
  },
  {
    "objectID": "documentation/2024-09-19-ABC5.html",
    "href": "documentation/2024-09-19-ABC5.html",
    "title": "ABC.5: open coding and tutorials, single cell dimensionality reduction",
    "section": "",
    "text": "Slides\nToday’s slides\n \n\n Download Slides \n\n \n\n\nTutorials\nYou can try our previous tutorials by looking at the Documentation page, or try a workshop one of us have been giving at the scVerse 2024 conference in Munich. The workshop is about dimensionality reduction methods for single cell data and contains a jupyter notebook to run python code on Google Colab.\n \n\n Go to the workshop page"
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html",
    "href": "documentation/2024-09-03-ABC4.html",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "",
    "text": "The difficulty of learning bash is often underestimated by others, who expect people approaching bioinformatics to learn it automatically. Here we try to put together the first basic concepts and commands.\n\n\n\n\n\n\nWhy the bash command line?\n\n\n\nUsing the bash command line becomes quickly essential if you are doing bioinformatics.\nFirst of all, you might need it to access a computing cluster (for example, GenomeDK at Aarhus University), since most clusters runs on a UNIX-based operating system, such as Linux, using a bash command line.\nJust as important is the fact that on a command line you can very easily do operations on multiple and very large files, something you would not be able to do using, for example, R or python. Large sequences of operations can be automatized into pipelines (an advanced topic not for this tutorial).\nWith a command line you can run many small programs, compose them together, and organize them in a chain of commands. This type of program organization fits well with what a bioinformatics project consist of: many tools to be applied repetitevely on multiple large files, and organizing those programs in a specific sequence. An example could be aligning to a reference genome many raw bulk-RNA sequencing files: the alignment operation must be repeated many times, and when files are finished, they might need to be merged if they are from the same sample.\n\n\n\n\nWhen using a UNIX operating system (Linux, MacOs), everything on your computer fits one of two categories: processes and files.\n\nProcesses are running instances of a program, and a program is any executable file stored in your computer.\nA file is any collection of data (program, image, video, audio, …).\n\nThe terminal is a text-based interface where you can write commands in various languages, depending on your choice. A terminal looks rather primitive, but it is what you often use on a daily basis to perform bioinformatics operations or control computing clusters.\nWhenever we write a command on the terminal and press enter, we have a shell receiving the code we wrote through the terminal. The shell is the outer layer of the operating system, which inteprets the commands and communicates them to the kernel.\nThe kernel is the core of the operating system, managing the computer physical components (hardware) and interfacing them with the processes that need to run. In general, any program (browser, game, …) you open or action (moving files, renaming folders, …) you do on your computer ends up being a process managed by the kernel. This communication process is shown in Figure 1\n\n\n\n\n\n\nFigure 1: Communication scheme where the outer layer is a bash shell command, which the shell then communicate to the kernel, which in turn manages the hardware resources to make the program actually run. Note that there can be many languages for the UNIX shell: bash is the most popular, but others exist and are used (for example zsh on MacOs). Figure credit: InnoKrea.\n\n\n\n\n\n\nWe can roughly identify various levels of efficiency, manual work, speed, number and size of handled files when working with a command line, the typical languages like R and python, or bash pipelines:\n\n\n\n\n\n\n\n\n\n\nProgramming mode\nNr of files\nFile Size\noperational speed\nManual work\n\n\n\n\nR, python, …\nfrom 1 to 10s\nsmall\npackage-dependent\nA lot\n\n\nCommand line\nfrom 1 to 100s\n1-10s GB\nfast\nLow-moderate\n\n\nUnix Pipeline\nfrom 1 to many 1000s\nmany TB\nfast\nLow\n\n\n\nYou will see in this tutorial how we can use basic bash utilities and handle text files. Those files would take longer time to read in R and python and the code to modify them would be in general longer and slower.\n\n\n\n\n\n\nShell vs Python-R?\n\n\n\nBoth the bash language and python or R are interpreted languages. You shouldn’t consider them as opposed to each other, but rather as complementary. Bash can do many things python or R cannot do very efficiently: search for files, look for patterns in long text files, concatenate operations, create pipelines, be the preferred system language on computing clusters. Python or R are great to perform for example statistical analysis.\nSo the best way to approach all those languages is to harness their advantages, instead of consider one of them “worse” than the others.\n\n\nWhat is Bash? Help you with this link if necessary\n\n\n  a package     a version of the Linux operating system     a language and an interpreter     a compiler which works only on Linux and Mac   Submit"
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#some-terminology",
    "href": "documentation/2024-09-03-ABC4.html#some-terminology",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "",
    "text": "When using a UNIX operating system (Linux, MacOs), everything on your computer fits one of two categories: processes and files.\n\nProcesses are running instances of a program, and a program is any executable file stored in your computer.\nA file is any collection of data (program, image, video, audio, …).\n\nThe terminal is a text-based interface where you can write commands in various languages, depending on your choice. A terminal looks rather primitive, but it is what you often use on a daily basis to perform bioinformatics operations or control computing clusters.\nWhenever we write a command on the terminal and press enter, we have a shell receiving the code we wrote through the terminal. The shell is the outer layer of the operating system, which inteprets the commands and communicates them to the kernel.\nThe kernel is the core of the operating system, managing the computer physical components (hardware) and interfacing them with the processes that need to run. In general, any program (browser, game, …) you open or action (moving files, renaming folders, …) you do on your computer ends up being a process managed by the kernel. This communication process is shown in Figure 1\n\n\n\n\n\n\nFigure 1: Communication scheme where the outer layer is a bash shell command, which the shell then communicate to the kernel, which in turn manages the hardware resources to make the program actually run. Note that there can be many languages for the UNIX shell: bash is the most popular, but others exist and are used (for example zsh on MacOs). Figure credit: InnoKrea."
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#efficiency-and-speed",
    "href": "documentation/2024-09-03-ABC4.html#efficiency-and-speed",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "",
    "text": "We can roughly identify various levels of efficiency, manual work, speed, number and size of handled files when working with a command line, the typical languages like R and python, or bash pipelines:\n\n\n\n\n\n\n\n\n\n\nProgramming mode\nNr of files\nFile Size\noperational speed\nManual work\n\n\n\n\nR, python, …\nfrom 1 to 10s\nsmall\npackage-dependent\nA lot\n\n\nCommand line\nfrom 1 to 100s\n1-10s GB\nfast\nLow-moderate\n\n\nUnix Pipeline\nfrom 1 to many 1000s\nmany TB\nfast\nLow\n\n\n\nYou will see in this tutorial how we can use basic bash utilities and handle text files. Those files would take longer time to read in R and python and the code to modify them would be in general longer and slower.\n\n\n\n\n\n\nShell vs Python-R?\n\n\n\nBoth the bash language and python or R are interpreted languages. You shouldn’t consider them as opposed to each other, but rather as complementary. Bash can do many things python or R cannot do very efficiently: search for files, look for patterns in long text files, concatenate operations, create pipelines, be the preferred system language on computing clusters. Python or R are great to perform for example statistical analysis.\nSo the best way to approach all those languages is to harness their advantages, instead of consider one of them “worse” than the others.\n\n\nWhat is Bash? Help you with this link if necessary\n\n\n  a package     a version of the Linux operating system     a language and an interpreter     a compiler which works only on Linux and Mac   Submit"
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#terminal-and-folders-anatomy",
    "href": "documentation/2024-09-03-ABC4.html#terminal-and-folders-anatomy",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "Terminal and folders anatomy",
    "text": "Terminal and folders anatomy\nWhen you work in the terminal, you will always see a prompt which starts with something of this type\n\nwhich provides you\n\nusername (e.g. samuele)\ncomputer name (e.g. D55749)\ncurrent working directory where the user is working at the moment (e.g. ~, which is the short form for the home directory)\n\nIn MobaXTerm you see only date, time and current working directory highlighted with various colors.\n\nHome directory ~\nThe home directory, which can be written as ~, is usually of the form /home/username, and is private to the user (no other users of that computer can access it).\nWhen you open the terminal, you always start with current working directory as your home. Try to write and execute (pressing enter) the command\npwd\nand you will see the full home path (which is the folder structure leading to your home directory)\n\n\nCurrent working directory (cwd)\nEvery command you execute refers to your cwd. For example, write\nls\nand you will see the list of files in your cwd. Try to create an empty file now with\ntouch emptyFile.txt\nand create a folder, which will be inside our cwd:\nmkdir myFolder\nIf you use again the ls command, the new file and folder will show in the cwd.\nNow we want to download something from the internet, for which we have a download link. We are getting a raw sequenced dataset in fastq format, which is currently gzip-compressed. The wget command can be used for the download. Note that now we also add an option -O to provide the output file name as ./myFolder/data.fastq.gz, where the dot . stands for the cwd, followed by myFolder, followed by the file name.\nwget https://github.com/hartwigmedical/testdata/raw/master/100k_reads_hiseq/TESTX/TESTX_H7YRLADXX_S1_L001_R1_001.fastq.gz -O ./myFolder/data.fastq.gz\n\n\n\n\n\n\nWarning\n\n\n\nNot all utilities are installed in MobaXTerm. If you get an error, install wget with the command\napt-get -y install wget\nand then try the download again.\n\n\n\n\nPaths and navigating the directory tree\nWe have already been using directories and paths a lot, so it is time to polish some definitions. Files are organized in a directory tree, which restricted to the tutorial we are running looks like Figure 2. Here home is a root folder, which is one of the top-level folder of your computer, so it has / at the beginning of its name.\n/home contains a folder with your username, which is right now your cwd. Inside your cwd you have the empty file and a directory myFolder containing the data.\nDirectories and files are organized with a tree hierarchy, so that /home is the first level, username the second, and myFolder the third level. The path in the directory tree to data.fastq.gz is expressed as /home/username/myFolder/data.fastq.gz.\n\n\n\n\n\n\nFigure 2: Directory tree of the tutorial. The home folder is a root folder (top-level of the tree) so its name starts with the / symbol. Other folders and files are at subsequent branch levels of the root folder /home. Some folders specific to your computer might be missing from this scheme.\n\n\n\n\nAbsolute and relative path\nThe path /home/username/myFolder/data.fastq.gz is called absolute because independent of your cwd. Let’s try another absolute path: the root folder /usr contains all executable files of the bash utilities we are using in this tutorial. Such files are in the folder bin. Execute\n\nls /usr/bin/\nThe output is a long list of executable files and some folders (sometimes they also have different colors, depending on your terminal settings). If you scroll and look, you can find familiar names like wget, ls, and so on. Now, the path /usr/bin is independent of our cwd.\nOn the contrary, the path ./myFolder/data.tar.gz depends on the cwd, and is equivalent to write myFolder/data.tar.gz, because ./ is always included by default (. represents the cwd). All paths that do not start with a root folder are relative!. To look inside myFolder, we can write\n\nls myFolder/\nHow do you write the command above using absolute paths? Remember that ~ correspondes to /home/username\n\n\n  ls /home/myFolder/     ls ~/myFolder     ls .   Submit\n\n\n\n\n\n\nNavigating folders\nHow to change your cwd? Simply use the command change directory. For example, you might want to work inside myFolder. Simply write\n\ncd myFolder\nand verify with pwd the new working directory path. If we want to unzip the compressed data file, we simply use its relative path:\ngunzip data.fastq.gz\nUse ls to verify that you have a file with name data.fastq."
  },
  {
    "objectID": "documentation/2024-09-03-ABC4.html#working-with-text-files",
    "href": "documentation/2024-09-03-ABC4.html#working-with-text-files",
    "title": "ABC.4: Introduction to bash language for bioinformatics",
    "section": "Working with (text) files",
    "text": "Working with (text) files\nMany files you use in bioinformatics are nothing else than text files which are written in a specific matter. This specific way of arranging the text in the files gives you many of the file formats you encounter when doing bioinformatics.\n\n\n\n\n\n\nNote\n\n\n\nSome file formats are encoded differently than with plain text, and cannot usually be seen with a text editor.\n\n\n\nLess for reading files\nThe first thing you want to do, is to check at how the content of a file looks like. The program less is perfect for this purpose: you can scroll text with the arrows, and use the keyboard to do operations like searching for text or jumping to a line. Try\n\nless data.fastq\nand see the format of your data, where each four lines provide a sequence. The very first sequence you see should be\n@HISEQ_HU01:89:H7YRLADXX:1:1101:1116:2123 1:N:0:ATCACG\nTCTGTGTAAATTACCCAGCCTCACGTATTCCTTTAGAGCAATGCAAAACAGACTAGACAAAAGGCTTTTAAAAGTCTA\nATCTGAGATTCCTGACCAAATGT\n+\nCCCFFFFFHHHHHJJJJJJJJJJJJHIJJJJJJJJJIJJJJJJJJJJJJJJJJJJJHIJGHJIJJIJJJJJHHHHHHH\nFFFFFFFEDDEEEEDDDDDDDDD\nHere the first line is informative of the sequence, the second is the sequence itself and then you have an empty line (symbol +), followed by the quality scores (encoded by letters according to this table).\nTry to scroll in all directions (with the arrow keys) to explore some lines in the file. If you want to exit from the text viewer, press q.\n\nTry to search online how to look for a specific word in a file with less. Then visualize the data with less, and try to find if there is any sequence ten adjacent Ns (which is, ten missing nucleotides). Then, answer the question below\nHow long is the shortest sequence of missing nucleotides not found anywhere in the file data.fastq?\n\n\n  30     25     19   Submit\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsually, a bash shell command can be found by searching for the command + man, for example less man. The manual of less lists all the many (many many) functionalities that can be useful to explore text files, which is always a good activity to do for double checking if things are in order.\nIn general: always check the manuals or write on the prompt the option --help (such as less --help). Usually you will understand a lot of what you can do and how from that.\n\n\n\n\nFile properties\nHow big is a file? How many lines does it have? When has it been created? You can use the good old ls with some extra options to list all files with some extra info:\nls -lah \nYou should get something like this depending on your computer:\ntotalt 6,3M\ndrwxr-xr-x+ 1 au612681 UsersGrp    0 28 aug 11:38 .\ndrwx------+ 1 au612681 UsersGrp    0 28 aug 10:38 ..\n-rw-r--r--+ 1 au612681 UsersGrp 6,3M 28 aug 11:38 data.fastq\nthe last line is clearly the data file, with creation date and size. Other infos about the file are group and user ownership and which rights we have on the file (we will not talk about these in this tutorial).\n\n\n\n\n\n\nNote\n\n\n\nLook also at the other two elements: . and .., which represents the current directory and the one above in the directory structure. Those link you to the directories, so one can always change to the directory above by using cd ..\nYou can combine multiple times .. to change directories multiple levels above. Use the terminal and execute the command cd ../../. Which folder does it bring you to?\n\n\n  /home     /home/myFolder     /usr/bin     /home/username/   Submit\n\n\n\n\n\nOh damn, now we changed cwd, and we cannot remember the path to the previous one where we have the data. We can use -, which brings us to the previous cwd!\ncd -\nNow, how many lines are there in your file? The command wc can show that to you. It has many options, but we are going to use the one to count lines in a file. As always, look for the manual or examples to see how you can use it in other many ways.\nwc -l data.fastq\nwill tell you that the file has 100000 lines, so you have 25000 sequences (each sequence is defined by 4 lines).\n\n\nCopy and Move\nNow we learn a few useful commands because we want to work on more than one file. We will create multiple copies of our file. To make a copy of the data file and called the new one dataCopy.fastq, do\ncp data.fastq dataCopy.fastq\nThen we will move data.fastq. You could move it in any folder, even in the current directory. If you move it into the current directory, you can use the mv command to simply change its name:\nmv data.fastq dataOriginal.fastq\nIf you now use ls -lah, you will see that you have two files of identical size and different creation dates. But… are those files identical? diff can tell you if you provide two file names. It will print out differences (or nothing if files are identical)\ndiff dataOriginal.fastq dataCopy.fastq\n\n\nWriting on a file\nYou can write something on a file using &gt;. This writes any output from a command which would appear on the screen into a file. For example the following commands prints out the first four lines of the data, but they will end up inside the file fourLines.fastq\nhead -4 dataOriginal.fastq &gt; fourLines.fastq\nTry to use\n\ncat fourLines.fastq\nto see those four lines printed out directly on the screen from the file.\n\n\n\n\n\n\nWarning\n\n\n\nUsing again &gt; to write on the same file will not add text to the file, but will overwrite it with the new text.\n\n\nLet’s append the first four lines of the data to its copy without overwriting it. This can be done using &gt;&gt;:\n\nhead -4 dataOriginal.fastq &gt;&gt; dataCopy.fastq\nWe want to run again the diff command to compare the two sequence files. Do you need to rewrite it? Of course not! Your bash saves the command history.\nKeep CTRL (or CMD on Mac) pressed, and in the meanwhile press r, then start writing diff. The old command will show up and you can run it pressing Enter. Alternatively, press the arrow UP to see the history of commands backward.\nIf you run the diff command on the two files you will see an output. This shows quite a lot of information and not only the sequence which is different:\n--- dataOriginal.fastq\n+++ dataCopy.fastq\n@@ -99998,3 +99998,7 @@\n\nmeans that there are additional lines in the second file (+++), missing in dataOriginal.fastq (- - -), and their coordinates.\n\n\nPiping\nA last important concept is the pipe. You can create small pipelines directly on the shell with the symbol |. Every time you use the pipe symbol, you get the output of a command and send it to the next command. For example, the grep command can find a pattern in a file in this way\ngrep NNNNN dataOriginal.fastq\nNow, what if we want to find that pattern in the first hundred sequences? Easy! We use head, and then pipe it into grep! When doing this, you do not need to provide a file to grep, and in that way it will read the output of the pipe.\nhead -400 dataOriginal.fastq | grep NNNNN\nThat was cool! In this case we have a small output on screen, but the output can be really huge as well. Maybe it makes sense to count how many sequences grep can find. Well, can’t really do it by hand! Pipe the result once more using wc as we did earlier!\nhead -400 dataOriginal.fastq | grep NNNNN | wc -l\nWait a second! That sequence absolutely needs to be saved into a file. How do you do it!\n\n\n  head -400 dataOriginal.fastq | grep NNNNN | sequence.fastq     head -400 dataOriginal.fastq | grep NNNNN &lt; sequence.fastq     sequence.fastq &lt; head -400 dataOriginal.fastq | grep NNNNN\n    head -400 dataOriginal.fastq | grep NNNNN &gt; sequence.fastq   Submit\n\n\n\n\n\nShortcuts and useful keys combinations\nThis was all for this very introductory tutorial. You have already seen how the bash shell is flexible, though requiring a bit of a steep learning curve.\nWe want to recap here useful shortcuts and key combinations for your day-to-day use of the bash shell\nHere’s a list of useful shortcuts and key combinations that you can use in the Bash shell to improve your efficiency:\n\nNavigating the Command Line\n\nCtrl + A: Move the cursor to the beginning of the line.\nCtrl + E: Move the cursor to the end of the line.\nAlt + F: Move the cursor forward one word.\nAlt + B: Move the cursor backward one word.\nCtrl + L: Clear the terminal screen\n\n\n\nEditing Text\n\nCtrl + U: Delete the text from the cursor to the beginning of the line.\nCtrl + K: Delete the text from the cursor to the end of the line.\nCtrl + W: Delete the word before the cursor.\nAlt + D: Delete the word after the cursor.\n\n\n\nHistory Navigation\n\nCtrl + R: Search through command history interactively (reverse-i-search).\nCtrl + P: Previous command in history (equivalent to the up arrow key).\nCtrl + N: Next command in history (equivalent to the down arrow key).\n!!: Re-run the last command.\n\n\n\nCommand Completion and Expansion\n\nTab: Auto-complete file names, directory names, or commands.\nAlt + .: Use the last argument of the previous command in the current command line.\n\n\n\nOther Useful Shortcuts\n\nCtrl + C: Terminate the current command (useful if things are stuck).\nCtrl + D: Log out of the current shell (equivalent to exit).\nCtrl + T: Swap the last two characters before the cursor.\n\n\n\n\n\n\n\nwrap up\n\n\n\nNow you are able to navigate the directory tree and visualize text files with less. You can also perform some basic operations, including piping commands, which is useful when handling large outputs which might end up directly on your screen."
  },
  {
    "objectID": "documentation/2024-10-10-usegdk.html",
    "href": "documentation/2024-10-10-usegdk.html",
    "title": "Workshop: Use GenomeDK",
    "section": "",
    "text": "We had a great workshop day, and for those of you who were not there, or want to see the slides again, here they are\n \n\n See the slides"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html",
    "href": "documentation/2024-10-24-ABC7.html",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "Today we will talk about job submission on genomeDK using SLURM and SCREEN/TMUX.\n\n\n\nWhat-is-SLURM\nInteractive-Jobs\nSubmitting Batch Jobs\nUnderstanding Paths\nConda in Batch Jobs\nResource Estimation\nJob-Priority\nMonitoring-Jobs\nCommon-Errors\nUsing-SCREEN-TMUX\nConclusion\n\n\n\n\n“Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.”\nThere are other managing systems as well but genomeDK uses SLURM so this is the one we learn. You do not need to know a lot about slurm but you do need to know how to interact with it. We submit our jobs through slurm.\nThere are 2 types of jobs in general: Interactive jobs and batch jobs.\n\n\n\nNode: A single machine in the cluster.\nJob: A set of instructions or tasks that SLURM executes.\nPartition: A logical grouping of nodes (like a queue) based on resource types or usage policies.\n\n\n\n\nSLURM has several basic commands that help users interact with the cluster:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsinfo\nDisplays information about the nodes in the cluster.\n\n\nsqueue\nDisplays a list of jobs currently in the queue.\n\n\nsbatch\nSubmits a job script to the queue for execution.\n\n\nscancel\nCancels a running or pending job.\n\n\nscontrol\nUsed for controlling jobs, partitions, or nodes.\n\n\n\n\n\n\n\nIn interactive jobs you request from slurm to give you some resources and it assigns you a node with those resources. Now you can run any command you want on the shell or run scripts and they will use that node with the specific resource allocation you asked. It is called interactive because you do not need to submit your code beforehand but you can change this in real time. To ask for an interactive job you use the command srun.\n\n\nTo run an interactive job, you use the srun command. Here’s a basic example to request an interactive session:\nsrun --account myProjectname -c 2 --mem 6g --time 03:00:00 --pty bash\nIn this example we have requested resources under the account myProjectname. We request 2 cpus per task, 6g of ram for 3 hours. --pty bash tells that we want to be using a bash terminal.\nRemember that you always have to specify an account. This is because genomeDK uses projects tied to accounts to track resource usage and by default we all have a very small quota to run stuff outside of genomeDK.\nBe aware that downloading files does not require resources so it best be done outside of an interactive job!\n\n\n\n\nTo submit a job, you need to write a job script and submit it using sbatch. Here is an example of a simple SLURM script:\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --ntasks=1               # Run a single task\n#SBATCH --time=01:00:00          # Time limit (hh:mm:ss)\n#SBATCH --mem=1G                 # Memory required (1 GB)\n\n# Run the job\npython my_script.py\nBatch scripts always have to start with the shebang. #!/bin/bash: This line, known as the shebang, indicates which interpreter should be used to run the script. In this case, it specifies the bash shell located in the /bin directory. The shebang must be the very first line of the script and begins with #!\nTo submit a job to an HPC cluster managed by SLURM, you need to write a job script and submit it using the sbatch command. However, before submitting the script, you must ensure that it is executable. This can be done using the chmod command.\n\n\nTo make your job script executable, use the following command:\nchmod +x mybatchscript.sh\nThen you can execute your script by running:\nsbatch mybatchscript.sh\n\n\n\nIn the script below we specify an output. This is where everything that would be printed(echoed) in the terminal goes. We also specified an error file. Since this is not an interactive job, if it has any problem it would not show on your screen. To catch usefull information we store them in the error file. Lastly I added the --array. This will make the batch script to submit multiple version of it using different TASK ID. Here from 0 to 4. So for example if you wanted to run the same script for multiple different inputs in parallel this is a good option.\n#!/bin/bash\n#SBATCH --account=myProjectname             # Specify the project/account name\n#SBATCH --job-name=test_job_array            # Job name\n#SBATCH --output=output_%A_%a.txt           # Standard output log, with job and array ID\n#SBATCH --error=error_%A_%a.log              # Standard error log, with job and array ID\n#SBATCH --array=0-4                           # Job array with indices from 0 to 4\n#SBATCH --ntasks=1                           # Run a single task\n#SBATCH --time=01:00:00                      # Time limit (hh:mm:ss)\n#SBATCH --mem=1G                             # Memory required (1 GB)\n\n# Get the job array index\narray_index=$SLURM_ARRAY_TASK_ID\n\n# Here we run a python script that accepts an input --index\npython my_script.py --index $array_index\n\n\n\n\nUnderstanding how SLURM handles file paths is crucial for reliable job execution. When you submit a job, SLURM needs to know where to find your input files and where to write your output. This might seem straightforward, but it’s actually one of the most common sources of confusion and errors in job submissions.\n\n\nWhen you submit a job using sbatch, SLURM needs to understand two important things:\n\nWhere to find your input files\nWhere to write your output files\n\nThe tricky part is that these locations are determined by something called the “launch directory” - but what exactly is that?\n\n\n\nThe launch directory is super important - it’s basically the starting point for all your relative paths. Here’s how it works:\n\nIt’s the directory you’re in when you run the sbatch command\nBy default, SLURM uses this as the working directory for your job\nAll relative paths in your script will be based on this directory\nYou can override it using --chdir, but be careful with that!\n\nFor example, if you’re in /home/username/projects/ when you run sbatch, that becomes your launch directory. Think of it as SLURM’s “You are here” marker.\n\n\n\nA lot of your projects might(and should) look like this:\n/home/username/projects/\n    ├── batch_job_script.sh\n    ├── data/\n    │   ├── raw_data/\n    │   │   └── input.csv\n    │   └── processed_data/\n    ├── scripts/\n    │   └── analysis_script.py\n    └── results/\n        └── output/\nNow you have a few options for referencing these files in your batch script:\n\n\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --job-name=data_analysis\n#SBATCH --output=results/output/job_%j.out\n#SBATCH --error=results/output/job_%j.err\n\n# This works because paths are relative to launch directory\npython scripts/analysis_script.py \\\n    --input data/raw_data/input.csv \\\n    --output data/processed_data/results.csv\n\n\n\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --job-name=data_analysis\n#SBATCH --output=/home/username/projects/results/output/job_%j.out\n#SBATCH --error=/home/username/projects/results/output/job_%j.err\n\n# This works anywhere but is less portable\npython /home/username/projects/scripts/analysis_script.py \\\n    --input /home/username/projects/data/raw_data/input.csv \\\n    --output /home/username/projects/data/processed_data/results.csv\n\n\n\n\nSometimes you might want your job to run from a different directory. You can do this with --chdir:\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --job-name=process_data\n#SBATCH --output=job_%j.out\n#SBATCH --error=job_%j.err\n#SBATCH --time=01:00:00\n#SBATCH --mem=1G\n#SBATCH --chdir=/home/username/projects/data/raw_data\n\n# Now paths are relative to the raw_data directory\npython ../../scripts/analysis_script.py \\\n    --input input.csv \\\n    --output ../processed_data/results.csv\n\n\n\nHere are some tips to make your life easier when dealing with paths in SLURM:\n\nUse Project-Based Organization\n\nKeep all related files under one project directory\nUse a consistent directory structure\nDocument your directory layout\n\nPath Variables in Scripts\n#!/bin/bash\n#SBATCH --account=myProjectname\n\n# Define paths at the start of your script\nPROJECT_DIR=\"$HOME/projects/my_project\"\nDATA_DIR=\"${PROJECT_DIR}/data\"\nSCRIPT_DIR=\"${PROJECT_DIR}/scripts\"\n\n# Use these variables in your commands\npython ${SCRIPT_DIR}/analysis.py \\\n    --input ${DATA_DIR}/input.csv\nCommon Pitfalls to Avoid\n\nDon’t assume the current directory\nAlways test paths with a small job first\nBe careful with spaces in paths\n\nDebugging Path Issues\n#!/bin/bash\n#SBATCH --account=myProjectname\n\n# Add these for debugging\necho \"Working directory: $PWD\"\necho \"Script location: $0\"\nls -la  # List files in working directory\n\n\n\n\n\nWhen submitting batch jobs it is important to use the correct conda environment for your needs.\n\n\nWhen submitting a Slurm job, your current shell’s conda environment is not inherited by the batch job. This is because:\n\nBatch jobs start in a fresh shell session\nShell initialization files (.bashrc, .bash_profile) may not be automatically sourced\n\n\n\n\nThere are several methods to activate conda environments in your Slurm scripts, each with its own advantages and considerations. I have mainly used these two which are also probably the best(?):\n\n\n#!/bin/bash\n#SBATCH [your parameters]\n\n# Initialize conda for bash shell\neval \"$(conda shell.bash hook)\"\nconda activate my_env_name\nAdvantages:\n\nProperly initializes conda’s shell functions\nWorks with conda’s auto-activation features\nMaintains conda’s internal state correctly\n\nConsiderations:\n\nRequires conda to be in the system PATH\nSlightly longer initialization time\n\n\n\n\n#!/bin/bash\n#SBATCH [your parameters]\n\n# Source conda's profile script\nsource /path/to/conda/etc/profile.d/conda.sh\nconda activate my_env_name\nAdvantages:\n\nWorks even if conda isn’t in PATH\nReliable across different conda installations\nProper initialization of conda functions\n\nConsiderations:\n\nRequires knowing the exact path to your conda installation\nPath might vary across clusters\n\nCommon conda installation paths:\n\n/opt/conda/etc/profile.d/conda.sh\n$HOME/miniconda3/etc/profile.d/conda.sh\n$HOME/anaconda3/etc/profile.d/conda.sh\n\n\n\n\n\n\nAlways explicitly activate environments in your job script:\n#!/bin/bash\n#SBATCH --job-name=my_analysis\n#SBATCH --output=output_%j.log\n\n# Initialize conda\neval \"$(conda shell.bash hook)\"\nconda activate your_env_name\n\npython script.py\nCheck environment activation:\n#!/bin/bash\n#SBATCH [your parameters]\n\neval \"$(conda shell.bash hook)\"\nconda activate your_env_name\n\n# Print environment info for debugging\necho \"Active conda env: $CONDA_DEFAULT_ENV\"\necho \"Python path: $(which python)\"\n\n\n\n\nCommon issues and solutions:\n\nEnvironment Not Found:\n\nVerify the environment exists: conda env list\nCheck for typos in environment name\nUse absolute paths if necessary\n\nConda Command Not Found:\n\nAdd conda’s initialization to your script:\n# Add to beginning of script\nexport PATH=\"/path/to/conda/bin:$PATH\"\n\nPackage Import Errors:\n\nVerify environment contents: conda list\nEnsure environment was created on a compatible system\nCheck for system-specific dependencies\n\n\n\n\n\n\nOne tricky part of using SLURM is knowing how many resources to request. Here’s how to figure it out:\n\n\nAfter your job completes, use sacct to see what resources it actually used:\nsacct -j &lt;jobid&gt; --format=JobID,JobName,MaxRSS,MaxVMSize,CPUTime\nYou can also use the extremely useful jobinfo &lt;job_id&lt;&gt; That will show something like\nName                : Myjobname\nUser                : dipe\nAccount             : BioinformaticsCore\nPartition           : normal\nNodes               : s21n42\nCores               : 1\nGPUs                : 0\nState               : PENDING (AssocMaxWallDurationPerJobLimit)\n\n\n\n\nStart with a reasonable estimate (like 4GB)\nCheck MaxRSS from sacct to see actual usage\nAdd 20% buffer to account for variations\nIf your job fails with “OUT OF MEMORY”, increase by 50%\n\n\n\n\n\nMore CPUs ≠ Faster Job (depends on your code)\nCheck CPU efficiency with sacct\nFor most Python/R scripts, start with 1-2 CPUs\nParallel jobs might need 4-16 CPUs\n\n\n\n\nYour job’s position in the queue depends on several factors:\n\n\n\n\nFair share (based on recent usage)\nJob size (smaller jobs might start sooner)\nWait time (priority increases with time)\nProject priority\n\n\n\n\n\nUse appropriate partition\nDon’t request more resources than needed\nSubmit shorter jobs when possible\nUse job arrays for many small tasks\n\n\n\n\n\nHere are two of the common SLURM errors and what they mean:\n\n\nslurmstepd: error: Exceeded step memory limit\n\nYour job needed more memory than requested\nSolution: Increase –mem value\n\n\n\n\nCANCELLED AT 2024-01-01T12:00:00 DUE TO TIME LIMIT\n\nJob didn’t finish in time\nSolution: Increase –time value or optimize code\n\n\n\n\n\nHave you ever started a process on genomeDK only to be required to physically move, close your laptop and loose all your progress? Tools like screen and tmux can help manage these issues processes by allowing you to create detachable terminal sessions. I personally use screen so we will talk about that with sbatch and srun. Screen and tmux are already installed for everyone on genomeDK.\nscreen is a terminal multiplexer that enables you to create multiple shell sessions within a single terminal window. You can detach from a session and reattach later, allowing your processes to continue running in the background even if you disconnect from the terminal.\n\n\nBasically everytime you are doing something interactive. Lets say you want to download some file but you also want to keep working on something else. You can open a screen session, start the download there, detach from the session and keep doing whatever else you where doing while the file is downloading in that other session.\n\n\nTechnically everything being done needs at least some resources but these are very minimal and unless you open hundrends of sessions at once (why would you?) there is no problem having multiple sessions at the same time.\n# Create new named session\nscreen -S download-genomes\n\n# Detach from session\nCtrl + A, D\n\n# List sessions\nscreen -ls\n\n# Reattach to session\nscreen -r download-genomes\n\n\n\n\n\nThe documentation for genomeDK. When you don’t remember something go there first: https://genome.au.dk/docs/overview/\nA bit more advanced, the slurm website: https://slurm.schedmd.com/overview.html\n\n\n\nNow you know the basics of SLURM and how to:\n\nSubmit interactive and batch jobs\nMonitor your jobs\nHandle common issues\nUse screen for session management\n\nRemember:\n\nAlways specify your account\nRequest appropriate resources\nUse screen for long sessions\nCheck job output and errors\n\nTHE END!"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#table-of-contents",
    "href": "documentation/2024-10-24-ABC7.html#table-of-contents",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "What-is-SLURM\nInteractive-Jobs\nSubmitting Batch Jobs\nUnderstanding Paths\nConda in Batch Jobs\nResource Estimation\nJob-Priority\nMonitoring-Jobs\nCommon-Errors\nUsing-SCREEN-TMUX\nConclusion"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#what-is-slurm",
    "href": "documentation/2024-10-24-ABC7.html#what-is-slurm",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "“Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.”\nThere are other managing systems as well but genomeDK uses SLURM so this is the one we learn. You do not need to know a lot about slurm but you do need to know how to interact with it. We submit our jobs through slurm.\nThere are 2 types of jobs in general: Interactive jobs and batch jobs.\n\n\n\nNode: A single machine in the cluster.\nJob: A set of instructions or tasks that SLURM executes.\nPartition: A logical grouping of nodes (like a queue) based on resource types or usage policies.\n\n\n\n\nSLURM has several basic commands that help users interact with the cluster:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsinfo\nDisplays information about the nodes in the cluster.\n\n\nsqueue\nDisplays a list of jobs currently in the queue.\n\n\nsbatch\nSubmits a job script to the queue for execution.\n\n\nscancel\nCancels a running or pending job.\n\n\nscontrol\nUsed for controlling jobs, partitions, or nodes."
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#interactive-jobs",
    "href": "documentation/2024-10-24-ABC7.html#interactive-jobs",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "In interactive jobs you request from slurm to give you some resources and it assigns you a node with those resources. Now you can run any command you want on the shell or run scripts and they will use that node with the specific resource allocation you asked. It is called interactive because you do not need to submit your code beforehand but you can change this in real time. To ask for an interactive job you use the command srun.\n\n\nTo run an interactive job, you use the srun command. Here’s a basic example to request an interactive session:\nsrun --account myProjectname -c 2 --mem 6g --time 03:00:00 --pty bash\nIn this example we have requested resources under the account myProjectname. We request 2 cpus per task, 6g of ram for 3 hours. --pty bash tells that we want to be using a bash terminal.\nRemember that you always have to specify an account. This is because genomeDK uses projects tied to accounts to track resource usage and by default we all have a very small quota to run stuff outside of genomeDK.\nBe aware that downloading files does not require resources so it best be done outside of an interactive job!"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#submitting-batch-jobs",
    "href": "documentation/2024-10-24-ABC7.html#submitting-batch-jobs",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "To submit a job, you need to write a job script and submit it using sbatch. Here is an example of a simple SLURM script:\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --ntasks=1               # Run a single task\n#SBATCH --time=01:00:00          # Time limit (hh:mm:ss)\n#SBATCH --mem=1G                 # Memory required (1 GB)\n\n# Run the job\npython my_script.py\nBatch scripts always have to start with the shebang. #!/bin/bash: This line, known as the shebang, indicates which interpreter should be used to run the script. In this case, it specifies the bash shell located in the /bin directory. The shebang must be the very first line of the script and begins with #!\nTo submit a job to an HPC cluster managed by SLURM, you need to write a job script and submit it using the sbatch command. However, before submitting the script, you must ensure that it is executable. This can be done using the chmod command.\n\n\nTo make your job script executable, use the following command:\nchmod +x mybatchscript.sh\nThen you can execute your script by running:\nsbatch mybatchscript.sh\n\n\n\nIn the script below we specify an output. This is where everything that would be printed(echoed) in the terminal goes. We also specified an error file. Since this is not an interactive job, if it has any problem it would not show on your screen. To catch usefull information we store them in the error file. Lastly I added the --array. This will make the batch script to submit multiple version of it using different TASK ID. Here from 0 to 4. So for example if you wanted to run the same script for multiple different inputs in parallel this is a good option.\n#!/bin/bash\n#SBATCH --account=myProjectname             # Specify the project/account name\n#SBATCH --job-name=test_job_array            # Job name\n#SBATCH --output=output_%A_%a.txt           # Standard output log, with job and array ID\n#SBATCH --error=error_%A_%a.log              # Standard error log, with job and array ID\n#SBATCH --array=0-4                           # Job array with indices from 0 to 4\n#SBATCH --ntasks=1                           # Run a single task\n#SBATCH --time=01:00:00                      # Time limit (hh:mm:ss)\n#SBATCH --mem=1G                             # Memory required (1 GB)\n\n# Get the job array index\narray_index=$SLURM_ARRAY_TASK_ID\n\n# Here we run a python script that accepts an input --index\npython my_script.py --index $array_index"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#understanding-paths",
    "href": "documentation/2024-10-24-ABC7.html#understanding-paths",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "Understanding how SLURM handles file paths is crucial for reliable job execution. When you submit a job, SLURM needs to know where to find your input files and where to write your output. This might seem straightforward, but it’s actually one of the most common sources of confusion and errors in job submissions.\n\n\nWhen you submit a job using sbatch, SLURM needs to understand two important things:\n\nWhere to find your input files\nWhere to write your output files\n\nThe tricky part is that these locations are determined by something called the “launch directory” - but what exactly is that?\n\n\n\nThe launch directory is super important - it’s basically the starting point for all your relative paths. Here’s how it works:\n\nIt’s the directory you’re in when you run the sbatch command\nBy default, SLURM uses this as the working directory for your job\nAll relative paths in your script will be based on this directory\nYou can override it using --chdir, but be careful with that!\n\nFor example, if you’re in /home/username/projects/ when you run sbatch, that becomes your launch directory. Think of it as SLURM’s “You are here” marker.\n\n\n\nA lot of your projects might(and should) look like this:\n/home/username/projects/\n    ├── batch_job_script.sh\n    ├── data/\n    │   ├── raw_data/\n    │   │   └── input.csv\n    │   └── processed_data/\n    ├── scripts/\n    │   └── analysis_script.py\n    └── results/\n        └── output/\nNow you have a few options for referencing these files in your batch script:\n\n\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --job-name=data_analysis\n#SBATCH --output=results/output/job_%j.out\n#SBATCH --error=results/output/job_%j.err\n\n# This works because paths are relative to launch directory\npython scripts/analysis_script.py \\\n    --input data/raw_data/input.csv \\\n    --output data/processed_data/results.csv\n\n\n\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --job-name=data_analysis\n#SBATCH --output=/home/username/projects/results/output/job_%j.out\n#SBATCH --error=/home/username/projects/results/output/job_%j.err\n\n# This works anywhere but is less portable\npython /home/username/projects/scripts/analysis_script.py \\\n    --input /home/username/projects/data/raw_data/input.csv \\\n    --output /home/username/projects/data/processed_data/results.csv\n\n\n\n\nSometimes you might want your job to run from a different directory. You can do this with --chdir:\n#!/bin/bash\n#SBATCH --account=myProjectname\n#SBATCH --job-name=process_data\n#SBATCH --output=job_%j.out\n#SBATCH --error=job_%j.err\n#SBATCH --time=01:00:00\n#SBATCH --mem=1G\n#SBATCH --chdir=/home/username/projects/data/raw_data\n\n# Now paths are relative to the raw_data directory\npython ../../scripts/analysis_script.py \\\n    --input input.csv \\\n    --output ../processed_data/results.csv\n\n\n\nHere are some tips to make your life easier when dealing with paths in SLURM:\n\nUse Project-Based Organization\n\nKeep all related files under one project directory\nUse a consistent directory structure\nDocument your directory layout\n\nPath Variables in Scripts\n#!/bin/bash\n#SBATCH --account=myProjectname\n\n# Define paths at the start of your script\nPROJECT_DIR=\"$HOME/projects/my_project\"\nDATA_DIR=\"${PROJECT_DIR}/data\"\nSCRIPT_DIR=\"${PROJECT_DIR}/scripts\"\n\n# Use these variables in your commands\npython ${SCRIPT_DIR}/analysis.py \\\n    --input ${DATA_DIR}/input.csv\nCommon Pitfalls to Avoid\n\nDon’t assume the current directory\nAlways test paths with a small job first\nBe careful with spaces in paths\n\nDebugging Path Issues\n#!/bin/bash\n#SBATCH --account=myProjectname\n\n# Add these for debugging\necho \"Working directory: $PWD\"\necho \"Script location: $0\"\nls -la  # List files in working directory"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#conda-in-batch-jobs",
    "href": "documentation/2024-10-24-ABC7.html#conda-in-batch-jobs",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "When submitting batch jobs it is important to use the correct conda environment for your needs.\n\n\nWhen submitting a Slurm job, your current shell’s conda environment is not inherited by the batch job. This is because:\n\nBatch jobs start in a fresh shell session\nShell initialization files (.bashrc, .bash_profile) may not be automatically sourced\n\n\n\n\nThere are several methods to activate conda environments in your Slurm scripts, each with its own advantages and considerations. I have mainly used these two which are also probably the best(?):\n\n\n#!/bin/bash\n#SBATCH [your parameters]\n\n# Initialize conda for bash shell\neval \"$(conda shell.bash hook)\"\nconda activate my_env_name\nAdvantages:\n\nProperly initializes conda’s shell functions\nWorks with conda’s auto-activation features\nMaintains conda’s internal state correctly\n\nConsiderations:\n\nRequires conda to be in the system PATH\nSlightly longer initialization time\n\n\n\n\n#!/bin/bash\n#SBATCH [your parameters]\n\n# Source conda's profile script\nsource /path/to/conda/etc/profile.d/conda.sh\nconda activate my_env_name\nAdvantages:\n\nWorks even if conda isn’t in PATH\nReliable across different conda installations\nProper initialization of conda functions\n\nConsiderations:\n\nRequires knowing the exact path to your conda installation\nPath might vary across clusters\n\nCommon conda installation paths:\n\n/opt/conda/etc/profile.d/conda.sh\n$HOME/miniconda3/etc/profile.d/conda.sh\n$HOME/anaconda3/etc/profile.d/conda.sh\n\n\n\n\n\n\nAlways explicitly activate environments in your job script:\n#!/bin/bash\n#SBATCH --job-name=my_analysis\n#SBATCH --output=output_%j.log\n\n# Initialize conda\neval \"$(conda shell.bash hook)\"\nconda activate your_env_name\n\npython script.py\nCheck environment activation:\n#!/bin/bash\n#SBATCH [your parameters]\n\neval \"$(conda shell.bash hook)\"\nconda activate your_env_name\n\n# Print environment info for debugging\necho \"Active conda env: $CONDA_DEFAULT_ENV\"\necho \"Python path: $(which python)\"\n\n\n\n\nCommon issues and solutions:\n\nEnvironment Not Found:\n\nVerify the environment exists: conda env list\nCheck for typos in environment name\nUse absolute paths if necessary\n\nConda Command Not Found:\n\nAdd conda’s initialization to your script:\n# Add to beginning of script\nexport PATH=\"/path/to/conda/bin:$PATH\"\n\nPackage Import Errors:\n\nVerify environment contents: conda list\nEnsure environment was created on a compatible system\nCheck for system-specific dependencies"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#resource-estimation",
    "href": "documentation/2024-10-24-ABC7.html#resource-estimation",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "One tricky part of using SLURM is knowing how many resources to request. Here’s how to figure it out:\n\n\nAfter your job completes, use sacct to see what resources it actually used:\nsacct -j &lt;jobid&gt; --format=JobID,JobName,MaxRSS,MaxVMSize,CPUTime\nYou can also use the extremely useful jobinfo &lt;job_id&lt;&gt; That will show something like\nName                : Myjobname\nUser                : dipe\nAccount             : BioinformaticsCore\nPartition           : normal\nNodes               : s21n42\nCores               : 1\nGPUs                : 0\nState               : PENDING (AssocMaxWallDurationPerJobLimit)\n\n\n\n\nStart with a reasonable estimate (like 4GB)\nCheck MaxRSS from sacct to see actual usage\nAdd 20% buffer to account for variations\nIf your job fails with “OUT OF MEMORY”, increase by 50%\n\n\n\n\n\nMore CPUs ≠ Faster Job (depends on your code)\nCheck CPU efficiency with sacct\nFor most Python/R scripts, start with 1-2 CPUs\nParallel jobs might need 4-16 CPUs\n\n\n\n\nYour job’s position in the queue depends on several factors:\n\n\n\n\nFair share (based on recent usage)\nJob size (smaller jobs might start sooner)\nWait time (priority increases with time)\nProject priority\n\n\n\n\n\nUse appropriate partition\nDon’t request more resources than needed\nSubmit shorter jobs when possible\nUse job arrays for many small tasks"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#common-errors",
    "href": "documentation/2024-10-24-ABC7.html#common-errors",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "Here are two of the common SLURM errors and what they mean:\n\n\nslurmstepd: error: Exceeded step memory limit\n\nYour job needed more memory than requested\nSolution: Increase –mem value\n\n\n\n\nCANCELLED AT 2024-01-01T12:00:00 DUE TO TIME LIMIT\n\nJob didn’t finish in time\nSolution: Increase –time value or optimize code"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#using-screen-tmux",
    "href": "documentation/2024-10-24-ABC7.html#using-screen-tmux",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "Have you ever started a process on genomeDK only to be required to physically move, close your laptop and loose all your progress? Tools like screen and tmux can help manage these issues processes by allowing you to create detachable terminal sessions. I personally use screen so we will talk about that with sbatch and srun. Screen and tmux are already installed for everyone on genomeDK.\nscreen is a terminal multiplexer that enables you to create multiple shell sessions within a single terminal window. You can detach from a session and reattach later, allowing your processes to continue running in the background even if you disconnect from the terminal.\n\n\nBasically everytime you are doing something interactive. Lets say you want to download some file but you also want to keep working on something else. You can open a screen session, start the download there, detach from the session and keep doing whatever else you where doing while the file is downloading in that other session.\n\n\nTechnically everything being done needs at least some resources but these are very minimal and unless you open hundrends of sessions at once (why would you?) there is no problem having multiple sessions at the same time.\n# Create new named session\nscreen -S download-genomes\n\n# Detach from session\nCtrl + A, D\n\n# List sessions\nscreen -ls\n\n# Reattach to session\nscreen -r download-genomes"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#useful-links",
    "href": "documentation/2024-10-24-ABC7.html#useful-links",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "The documentation for genomeDK. When you don’t remember something go there first: https://genome.au.dk/docs/overview/\nA bit more advanced, the slurm website: https://slurm.schedmd.com/overview.html"
  },
  {
    "objectID": "documentation/2024-10-24-ABC7.html#conclusion",
    "href": "documentation/2024-10-24-ABC7.html#conclusion",
    "title": "ABC.7: Practical guide to SLURM and job submissions on HPC",
    "section": "",
    "text": "Now you know the basics of SLURM and how to:\n\nSubmit interactive and batch jobs\nMonitor your jobs\nHandle common issues\nUse screen for session management\n\nRemember:\n\nAlways specify your account\nRequest appropriate resources\nUse screen for long sessions\nCheck job output and errors\n\nTHE END!"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html",
    "href": "documentation/2024-08-20-ABC3.html",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "",
    "text": "Data formats and bulkRNA sequencing exercise\n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html#install-packages",
    "href": "documentation/2024-08-20-ABC3.html#install-packages",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "Install packages",
    "text": "Install packages\nFirst of all you need quite some packages for bulkRNA analysis. The following installations will also help in the fiture analysis tutorial where various different plots are explored. Note how you install some packages with install.packages (from the R default channel) and with BiocManager::install (from the BiocManager channel).”\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"pheatmap\")\ninstall.packages(\"ggrepel\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"readxl\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"pdftools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggridges\")\ninstall.packages(\"forcats\")\ninstall.packages(\"igraph\")\n\n# Install Bioconductor packages\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n    install.packages(\"BiocManager\")\n}\n\nBiocManager::install(\"edgeR\")\nBiocManager::install(\"DESeq2\")\nBiocManager::install(\"ComplexHeatmap\")\nBiocManager::install(\"clusterProfiler\")\nBiocManager::install(\"org.Hs.eg.db\")\nBiocManager::install(\"org.Mm.eg.db\")\nBiocManager::install(\"AnnotationDbi\")\nBiocManager::install(\"pathview\")\nBiocManager::install(\"DOSE\")\nBiocManager::install(\"enrichplot\")\nBiocManager::install(\"ReactomePA\")\nBiocManager::install(\"SummarizedExperiment\")\n\nWe should then load the needed packages\n\nlibrary(edgeR)\nlibrary(DESeq2)\nlibrary(ggplot2)\nlibrary(ComplexHeatmap)\nlibrary(clusterProfiler)\nlibrary(pheatmap)\nlibrary(org.Hs.eg.db)\nlibrary(org.Mm.eg.db)\nlibrary(AnnotationDbi)\nlibrary(ggrepel)\nlibrary(openxlsx)\nlibrary(readxl)\nlibrary(pathview)\nlibrary(tidyverse)\nlibrary(RColorBrewer)\nlibrary(DOSE)\nlibrary(enrichplot)\nlibrary(pdftools)\nlibrary(dplyr)\nlibrary(ggridges)\nlibrary(forcats)\nlibrary(ReactomePA)\nlibrary(igraph)\nlibrary(BiocManager)\n\nHere we download a data matrix which we created in advance. The file is compressed, so we also unzip it (converting it from compressed to normal reading format) before reading.\n\ndownload.file(\"https://github.com/AU-ABC/AU-ABC.github.io/raw/main/documentation/2024-08-20-ABC3/FeatureCountOutput.zip\", destfile = \"./FeatureCountOutput.zip\")\n\n\nunzip(\"./FeatureCountOutput.zip\")\n\n\ncountData &lt;- read.table(\"./FeatureCountOutput.txt\", header = TRUE, sep = \"\\t\")\n\nNow we have created the matrix Countdata and we check the rownames of the countMatrix. The names are simply numbers and not gene IDs or names. So we replace the row names with the countData$Geneid column of the dataset. We use the function “head” to view the first 5 names before and after substitution.\n\nhead(rownames(countData))\nrownames(countData) &lt;- countData$Geneid\nhead(rownames(countData))\n\n\n'1''2''3''4''5''6'\n\n\n\n'DDX11L1''WASH7P''MIR6859-1''MIR1302-2HG''MIR1302-2''FAM138A'\n\n\nIf we look at column names, we see a certain number of names columns. The first ones are gene IDs and genetic informations about chromosome, starting and ending position, strand and length. The rest is sample names.\n\ncolnames(countData)\n  \n\n\n'Geneid''Chr''Start''End''Strand''Length''X1_.bam''X2_.bam''X3_.bam''X4_.bam''X5_.bam''X6_.bam''X7_.bam''X8_.bam''X9_.bam'\n\n\nThe sample names does not make sense for a person who did not do the sequencing of this data, therefore we rename it to the scientific annotation.\n\n\n\n\n\n\nTip\n\n\n\nThe order the colnames(countData) is shown, is the order you have to follow with the renaming for the scientific annotation.\n\n\n\n sampleNames &lt;- c(\"WT_1\", \"WT_2\", \"WT_3\", \"DRUG_1\", \"DRUG_2\", \"DRUG_3\", \"DRUG+SUPPLEMENT_1\", \"DRUG+SUPPLEMENT_2\", \"DRUG+SUPPLEMENT_3\")\n\nThe countmatrix countData contains additional information such as gene name, gene length, exon start and more. These informations are found in columns from 1 to 6. This means, that we have to change the samplenames starting from column number 7. Before we print the column names before and after we do substitution\n\ncolnames(countData)[7:length(colnames(countData))]\ncolnames(countData)[7:length(colnames(countData))] &lt;- sampleNames\ncolnames(countData)\n\n\n'X1_.bam''X2_.bam''X3_.bam''X4_.bam''X5_.bam''X6_.bam''X7_.bam''X8_.bam''X9_.bam'\n\n\n\n'Geneid''Chr''Start''End''Strand''Length''WT_1''WT_2''WT_3''DRUG_1''DRUG_2''DRUG_3''DRUG+SUPPLEMENT_1''DRUG+SUPPLEMENT_2''DRUG+SUPPLEMENT_3'\n\n\nAny sequencing data is prone to noise. The kind of noise we remove here is arguably something of great discussion as to where and when to keep data. We can see there are a lot of genes by counting the rows of countData\n\n  nrow(countData) # This returns the number of genes in the countmatrix before removing noise.\n\n49771\n\n\nWe can also plot a simple histogram of how many counts each gene has in a specific sample, for example WT_1. We use the logarithm log(x+1), where x is the expression to have a readable plot. Note how we can see many genes with 0 expression.\n\nhist(log1p(countData$WT_1), breaks=30, main=\"WT_1 expression values\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is however not a very useful plot, because a gene can be overexpressed under other conditions, and underexpressed in the WT_1 sample. We can try to look at the gene expression over all individuals for each gene using again a histogram.\n\n\nwe still have a number of genes with little or no expression. Those are of very little relevance in any analysis\n\nsumExpr &lt;- rowSums( countData[,7:15] )\n\nhist( log1p(sumExpr), breaks=30, main=\"Sum of gene expression values on all samples\")\n\n\n\n\n\n\n\n\nAs an example, we print the fifth gene to see the number of counts. If none of the samples have 5 or more counts of this gene, then we remove the gene.\n\n  countData[5, 7:ncol(countData)] \n\n\nA data.frame: 1 × 9\n\n\n\nWT_1\nWT_2\nWT_3\nDRUG_1\nDRUG_2\nDRUG_3\nDRUG+SUPPLEMENT_1\nDRUG+SUPPLEMENT_2\nDRUG+SUPPLEMENT_3\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nMIR1302-2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nBelow we use subset to apply the filter. We remove data (genes) contains little to no information. It essentially removes a gene from the countmatrix if the gene has less than 5 counts in all the samples.\n\ncountData &lt;- subset(countData, rowSums(countData[, 7:ncol(countData)] &gt; 5) &gt; 0)\n\nWe removed a lot of genes!\n\nnrow(countData) #Returning the number of genes after removing noise.\n\n21473"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html#condition-matrix",
    "href": "documentation/2024-08-20-ABC3.html#condition-matrix",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "Condition matrix",
    "text": "Condition matrix\nThe samples are defined in some condition like WT, DRUG, DRUG+SUPPLEMENT or some other condition.\n\nconditions &lt;- c(rep(\"WT\", 3), rep(\"DRUG\", 3), rep(\"DRUG+SUPPLEMENT\", 3)) \n \n\nWe create a dataframe with the condition and sample information\n\ncoldata &lt;- data.frame(row.names = colnames(countData[0, 7:ncol(countData)]), conditions)\n\n\ncoldata\n\n\nA data.frame: 9 × 1\n\n\n\nconditions\n\n\n\n&lt;chr&gt;\n\n\n\n\nWT_1\nWT\n\n\nWT_2\nWT\n\n\nWT_3\nWT\n\n\nDRUG_1\nDRUG\n\n\nDRUG_2\nDRUG\n\n\nDRUG_3\nDRUG\n\n\nDRUG+SUPPLEMENT_1\nDRUG+SUPPLEMENT\n\n\nDRUG+SUPPLEMENT_2\nDRUG+SUPPLEMENT\n\n\nDRUG+SUPPLEMENT_3\nDRUG+SUPPLEMENT\n\n\n\n\n\nWe create a subset of the countData so that we have an object only containing only the counts of the samples.\n\ncountData_subset &lt;- countData[, -c(1:6)]\n\n\nhead(countData_subset)\n\n\nA data.frame: 6 × 9\n\n\n\nWT_1\nWT_2\nWT_3\nDRUG_1\nDRUG_2\nDRUG_3\nDRUG+SUPPLEMENT_1\nDRUG+SUPPLEMENT_2\nDRUG+SUPPLEMENT_3\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nWASH7P\n45\n45\n37\n109\n84\n107\n123\n128\n72\n\n\nMIR6859-1\n2\n1\n4\n7\n10\n5\n7\n6\n9\n\n\nLOC729737\n6\n3\n6\n9\n7\n5\n85\n81\n68\n\n\nLOC100996442\n70\n66\n61\n82\n58\n70\n127\n107\n100\n\n\nLOC127239154\n6\n7\n6\n23\n20\n19\n5\n5\n4\n\n\nLOC100132287\n2\n4\n1\n3\n2\n2\n4\n6\n4\n\n\n\n\n\nWe check if the sample names are correct. Both of these should return TRUE, otherwise there is some incorrect naming.\n\nall(colnames(countData_subset) %in% rownames(coldata))\nall(colnames(countData_subset) == rownames(coldata))\n\nTRUE\n\n\nTRUE"
  },
  {
    "objectID": "documentation/2024-08-20-ABC3.html#applying-deseq2",
    "href": "documentation/2024-08-20-ABC3.html#applying-deseq2",
    "title": "ABC.3: R analysis of bulkRNA expression matrix",
    "section": "Applying DeSeq2",
    "text": "Applying DeSeq2\nTime to use the DESeq2 normalization method to analyse your sequencing data. The commands below run DeSeq2 and\n\nbuilds a DeSeq2 object from the data matrix and condition matrix\napplyes DeSeq2\ntransforms the expressions by variance stabilization\n\n\ndds &lt;- DESeqDataSetFromMatrix(count = countData_subset, colData = coldata, design = ~conditions)\ndds &lt;- DESeq(dds)\nvsdata &lt;- vst(dds, blind=FALSE)\n\nWarning message in DESeqDataSet(se, design = design, ignoreRank):\n\"some variables in design formula are characters, converting to factors\"\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nestimating size factors\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nestimating dispersions\n\ngene-wise dispersion estimates\n\nmean-dispersion relationship\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nfinal dispersion estimates\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\nfitting model and testing\n\n\n\nTo compute a variance stabilizing transformation is roughly similar to putting the data on the log2 scale, while also dealing with the sampling variability of low counts. It uses the design formula to calculate the within-group variability (if blind=FALSE) or the across-all-samples variability (if blind=TRUE). It does not use the design to remove variation in the data. It therefore does not remove variation that can be associated with batch or other covariates (nor does DESeq2 have a way to specify which covariates are nuisance and which are of interest).\nThe resulting object is called vsdata, which will be used for further analysis with DESeq2\n\nvsdata\n\nclass: DESeqTransform \ndim: 21473 9 \nmetadata(1): version\nassays(1): ''\nrownames(21473): WASH7P MIR6859-1 ... TRNT TRNP\nrowData names(26): baseMean baseVar ... maxCooks dispFit\ncolnames(9): WT_1 WT_2 ... DRUG+SUPPLEMENT_2 DRUG+SUPPLEMENT_3\ncolData names(2): conditions sizeFactor\n\n\n\n\n\n\n\n\nwrap up\n\n\n\nYou have read and preprocessed a bulk RNA expression matrix and ran an analysis with DeSeq2. Further steps concern visualizing and interpreting the results inferred by DeSeq2 on the data."
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html",
    "href": "documentation/2024-09-19-ABC6.html",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "",
    "text": "Today’s slides\n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html#geo-and-sra-data",
    "href": "documentation/2024-09-19-ABC6.html#geo-and-sra-data",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "GEO and SRA data",
    "text": "GEO and SRA data\nGEO (Gene Expression Omnibus) is a database which contains processed data from biological experiments. For example aligned files and analysis results, and supplementary files of various type. A GEO entry is called Series and contains Samples , Platforms, Supplementary data. Usually, there is an associate SRA number.\nA SRA (Sequence Read Archive) number contains the raw data for the samples contained in a GEO series. The download from GEO can happen by using the various alphanumerical identifiers to write a download link. SRA downloads need the software SRA-tools, and cannot be performed using a simple download link.\n\n\n\nA GEO Series, where highlighted in different colours there are the main data elements. The Series ID (red). the type of platform (green) for creating the samples (blue), and the SRA number for raw data (purple).\n\n\n\nDownload GEO data programmatically\nSometimes you need to download some supplementary files from GEO. This is pretty easy from the command line, and we show it with an example. We consider the GEO series in the figure above - go to the GEO home page and find it using the series number in the search bar of the website.\nNow, we use the guidelines of GEO for programmatic downloads. Here you have examples and descriptions on how to write the FTP1 address of all files in the series (excluding SRA files).\nTo look at the content of the series, use the command line and write\ncurl -l ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/\nnote how the address contains series, the series ID where the last three numbers are changed into nnn, and the series number itself. The output is a list of the content of the series:\nmatrix\nminiml\nsoft\nsuppl\nThe first three are folders containing matrix, miniml and soft files, which are plain text files used to describe the series. The fourth is the folder of supplementary material. Using curl -l we can see that it contains the supplementary file of the series:\ncurl -l ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/suppl\nGSE67303_DEG_cuffdiff.xlsx\nNow we can download the file using the whole URL with wget\nwget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/suppl/GSE67303_DEG_cuffdiff.xlsx\n\n\n\n\n\n\nTip\n\n\n\nDo you want to download a lot of supplementary files without one-by-one download by hand? Then wget -r will download recursively the content of the whole folder, for example:\nwget -r ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67303/suppl/\nSome webpages have so-called robots which avoid a user to download in an autopmated way too many datasets at once, to prevent overloading of the bandwidth. Bulk downloads are identified by, amongst others, how much time it takes to request a download. When a robot rejects your download attempt, you get an error message. In that case, wget can solve the problems with some extra dedicated options. Below you can see a bulk download from the PRIDE database, which uses robots:\nwget --random-wait \\\n     -r -p -e robots=off \\\n     -U mozilla \\\n     ftp.pride.ebi.ac.uk/pride/data/archive/2024/09/PXD056312/\nNote: the command above takes time because the data is quite big and is just for the sake of example, so don’t run it :-)\n\n\nYou can change the URL to show the supplementary data inside samples and inside platforms. In our example you can click on the platform/sample number on the webpage, and see if they contain any supplementary data. It is not the case of this GEO series, but for the sake of the example we will write down how to list the supplementary files inside the related FTP folders.\n\nFor the platforms, we need to substute series with platforms, and the same with the IDs, which will again have nnn for the last 3 digits: curl -l ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL19nnn/GPL19949/ you will notice there are only soft and miniml folders for the platform description, and no supplementary\nFor the samples, it works analogously, using sample IDs curl -l ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM1644nnn/GSM1644066/ you will notice there are only soft and miniml folders for the platform description\n\n\n\nDownload GEO and SRA data with geofetch\nDownloading entire GEO series and related raw data through SRA is always painful. Here comes into play a pretty useful package called geofetch (Khoroshevskyi et al. (2023)). This can be easily installed through conda on the command line\nconda create -n geofetch\nconda activate geofetch\nconda install -c bioconda -c conda-forge geofetch sra-tools\n\nYou can also find a guide to anaconda in our ABC2 tutorial, where the packages geofetch and sra-tools can be installed interactively. geofetch works also with windows, though some options are not available.\n\n\n\n\n\n\n\nsettings for sra-tools\n\n\n\nsra-tools creates very large files in .sra format into your home folder while downloading. This can be detrimental on a cluster, for example on GenomeDK, where your home is limited in 100GB size. sra files are then converted into the desired format. You can decide to download sra files in to a temporary folder or any other folder of your choice. For example you can do what follows to change the folder for sra files into /tmp\nmkdir -p ${HOME}/.ncbi\necho '/repository/user/main/public/root = \"/tmp\"' &gt; ${HOME}/.ncbi/user-settings.mkfg\n\n\n\nDownload\ngeofetch makes it easy to download data - the command is very basic and different options can personalize which content you want to download. The whole download contain also metadata which follows the PEP format, a FAIR-ready way of documenting data for reproducible access by other users (Sheffield et al. (2021), LeRoy et al. (2024)). The basic command has this form:\ngeofetch -i SRA_SERIES_ID\nand one can add other arguments to choose the type of data to be downloaded according to this table:\n\n\n\nArguments to choose what to download with geofetch. The first line with empty arguments corresponds to the basic geofetch command.\n\n\nWe can download metadata and raw sra data for the GEO series we have in the first figure of this tutorial with the basic geofetch command\ngeofetch -i GSE67303\nOnce you are done, you have the following files and folders:\nGSE67303\nGSE67303\n├── GSE67303_GSE.soft\n├── GSE67303_GSM.soft\n├── GSE67303_PEP\n│   ├── GSE67303_PEP_raw.csv\n│   └── GSE67303_PEP.yaml\n├── GSE67303_SRA.csv\n\nGSE67303_GSE.soft is a text file containing series information\nGSE67303_GSM.soft is a text file containing samples information\nGSE67303_SRA.csv is a comma-separated table with the sra file lists who was downloaded\nthe files in the subfolder GSE67303_PEP contains data info following PEP standard\n\nTry to do ls in the folder used as default for the download of sra data, such as\nls /tmp/sra/\nand you should see the downloaded data from the sra archive\nSRR1930183.sra  SRR1930184.sra  SRR1930185.sra  SRR1930186.sra\nNow you can use fasterq-dump to convert the sra data into fastq format. fasterq-dump builds up on the tool fastq-dump, but includes some default settings, such as splitting the fastq files into paired reads and ambiguous reads (only when you actually have paired reads), which is usually the standard.\nIn this example we choose the locally downloaded files in the sra download folder, and we choose the output folder inside where we have the metadata information.\nfasterq-dump /tmp/sra/*.sra -O GSE67303/raw -v\nNow you can see the raw data is also there\nls /tmp/sra/\nGSE67303\n├── GSE67303_GSE.soft\n├── GSE67303_GSM.soft\n├── GSE67303_PEP\n│   ├── GSE67303_PEP_raw.csv\n│   └── GSE67303_PEP.yaml\n├── GSE67303_SRA.csv\n└── raw\n    ├── SRR1930183_1.fastq\n    ├── SRR1930183_2.fastq\n    ├── SRR1930184_1.fastq\n    ├── SRR1930184_2.fastq\n    ├── SRR1930185_1.fastq\n    ├── SRR1930185_2.fastq\n    ├── SRR1930186_1.fastq\n    └── SRR1930186_2.fastq\nIf you want, you can empty the sra download folder\nrm /tmp/sra/*.sra\n\n\n\nDownload SRA data with fasterq-dump\nYou can also use fasterq-dump directly if you do not want any metadata. It can be used with only one sra run at the time, but below we show how you can download entire series runs or sample runs.\n\nOnly one SRA run\nDo you simply need a single raw file? In the webpage of the GEO series, click on the SRA code. A new window will show all the samples: choose one and find the SRA run number, which starts with SRR (in figure below).\n\nIn relation to figure above\nfasterq-dump SRR1930186 -v\n\n\nAll SRA runs of a GEO series\nClick on the SRA link inside the GEO series. When the list with all samples appears, click on Send to on the top-right button, choose File and Accession List. This will download a csv file which you can combine with fasterq-dump to download all runs:\nfasterq-dump `sed 1,1d ./Downloads/SraAccList.csv` -v\n\n\nAll SRA runs of a single sample\nAfter clicking on the SRA code of the GEO series, you can click on a specific sample and use the same method with the Send to button to download all the runs of a specific sample."
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html#fastq-dump-command",
    "href": "documentation/2024-09-19-ABC6.html#fastq-dump-command",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "fastq-dump command",
    "text": "fastq-dump command\nfastq-dump is a tool for downloading sequencing reads from NCBI’s Sequence Read Archive (SRA). These sequence reads will be downloaded as FASTQ files. How these FASTQ files are formatted depends on the fastq-dump options used.\n\nDownloading reads from the SRA using fastq-dump\nIn this example, we want to download FASTQ reads for a mate-pair library.\nfastq-dump --gzip --skip-technical --readids --read-filter pass --dumpbase --split-3 --clip --outdir path/to/reads/ SRR_ID\nIn this command…\n\n--gzip: Compress output using gzip. Gzip archived reads can be read directly by bowtie2.\n--skip-technical: Dump only biological reads, skip the technical reads.\n--readids or -I: Append read ID after spot ID as ‘accession.spot.readid’. With this flag, one sequence gets appended the ID .1 and the other .2. Without this option, pair-ended reads will have identical IDs.\n--read-filter pass: Only returns reads that pass filtering (without Ns).\n--dumpbase or -B: Formats sequence using base space (default for other than SOLiD). Included to avoid colourspace (in which pairs of bases are represented by numbers).\n--split-3 separates the reads into left and right ends. If there is a left end without a matching right end, or a right end without a matching left end, they will be put in a single file.\n--clip or -W: Some of the sequences in the SRA contain tags that need to be removed. This will remove those sequences.\n--outdir or -O: (Optional) Output directory, default is current working directory.\nSRR_ID: This is is the ID of the run from SRA to be downloaded. This ID begins with “SRR” and is followed by around seven digits (e.g. SRA1234567).\n\nOther options that can be used instead of --split-3:\n\n--split-files splits the FASTQ reads into two files: one file for mate 1s (...1), and another for mate 2s (..._2). This option will not mateless pairs into a third file.\n--split-spot splits the FASTQ reads into two (mate 1s and mate 2s) within one file. --split-spot gives you an 8-line fastq format where forward precedes reverse (see https://www.biostars.org/p/178586/#258378)."
  },
  {
    "objectID": "documentation/2024-09-19-ABC6.html#fasterq-dump-command",
    "href": "documentation/2024-09-19-ABC6.html#fasterq-dump-command",
    "title": "ABC.6: Download from online databases, open coding session",
    "section": "fasterq-dump command",
    "text": "fasterq-dump command\nfasterq-dump is a tool for downloading sequencing reads from NCBI’s Sequence Read Archive (SRA). These sequence reads will be downloaded as fastq files. fasterq-dump is a newer, streamlined alternative to fastq-dump; both of these programs are a part of sra-tools.\n\nfasterq-dump vs fastq-dump\nHere are a few of the differences between fastq-dump and fasterq-dump:\n\nIn fastq-dump, the flag --split-3 is required to separate paired reads into left and right ends. This is the default setting in fasterq-dump.\nThe fastq-dump flag --skip-technical is no longer required to skip technical reads in fasterq-dump. Instead, the flag --include-technical is required to include technical reads when using fasterq-dump.\nThere is no --gzip or --bzip2 flag in fasterq-dump to download compressed reads with fasterq-dump. However, FASTQ files downloaded using fasterq-dump can still be subsequently compressed.\n\nThe following commands are equivalent, but will be executed faster using fasterq-dump:\nfastq-dump SRR_ID --split-3 --skip-technical\nfasterq-dump SRR_ID\n\n\nDownloading reads from the SRA using fasterq-dump\nIn this example, we want to download FASTQ reads for a mate-pair library.\nfastq-dump --threads n --progress SRR_ID\nIn this command…\n\n--threads specifies the number (n) processors/threads to be used.\n--progress is an optional argument that displays a progress bar when the reads are being downloaded.\nSRR_ID is the ID of the run from the SRA to be downloaded. This ID begins with “SRR” and is followed by around seven digits (e.g. SRR1234567)."
  },
  {
    "objectID": "documentation/2024-11-28-ABC9.html",
    "href": "documentation/2024-11-28-ABC9.html",
    "title": "ABC.9: Graphical applications on GenomeDK",
    "section": "",
    "text": "Today’s slides\n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2024-11-28-ABC9.html#graphical",
    "href": "documentation/2024-11-28-ABC9.html#graphical",
    "title": "ABC.9: Graphical applications on GenomeDK",
    "section": "What is a graphical application",
    "text": "What is a graphical application\n\n\n\n\n\n\nGraphical application\n\n\n\nA graphical application provides a user interface that allows users to interact with software visually rather than through text-based commands.\n\n\nExamples of graphical applications very popular in bioinformatics are Jupyterlab and Rstudio."
  },
  {
    "objectID": "documentation/2024-11-28-ABC9.html#tunnelling",
    "href": "documentation/2024-11-28-ABC9.html#tunnelling",
    "title": "ABC.9: Graphical applications on GenomeDK",
    "section": "What is tunnelling",
    "text": "What is tunnelling\n\n\n\n\n\n\nDefinition\n\n\n\nSSH tunneling is a method of transporting arbitrary networking data over an encrypted SSH connection\n\n\nYou can transport any type of data, for example when you use a command to transfer a dataset from GenomeDK to your own computer, or vice versa. Tunnelling can also be used to constantly transfer data from graphical applications, so that they can be viewed on your own computer.\n\nApplications rendered locally with X11\nThere are various way of rendering applications running on GenomeDK on your local laptop. You might have encountered X11 (for example when using Rstudio Desktop installed in a conda environment), and noticed it is very laggy and slow, also graphically outdated.\nX11 is based on sending all raw instructions for rendering across the network to your computer. This means that the whole graphics you see (icons, widgets, menus, buttons, …) has to travel over the network to your computer. Whenever you take an action, like opening a menu, the cluster needs to know that, so the menu can be rendered and the instructions to show it sent to your computer. This takes clearly a lot of time!\n\n\n\n\n\n\nX11\n\n\n\nX11 does all the rendering and provides instructions to show the graphical interface. Each change in the interface has to go through the network to be shown locally.\n\n\n\n\nApplications rendered locally with a web interface\nA faster and lighter way of rendering applications is through web-based interfaces. A web server is deployed on the cluster, and it transfer data over the network. Rendering instructions are all executed directly by the browser on the local computer, and do not need to travel over the network.\nWhat travels over the network are simple scripts, which th browser will interpret to visualize and update the graphical interface. Examples of web-based applications are Rstudio Server and Jupyterlab, but also all the interactive browsers that you can find online to explore OMICS data!\n\n\n\n\n\n\nweb based applications\n\n\n\nWeb based applications transmit only the necessary information to the local computer, minimizing bandwidth use compared to raw graphical instructions. Those applications are optimized for remote access, ensuring a more fluid experience."
  },
  {
    "objectID": "documentation/2024-11-28-ABC9.html#container",
    "href": "documentation/2024-11-28-ABC9.html#container",
    "title": "ABC.9: Graphical applications on GenomeDK",
    "section": "Containers for web applications",
    "text": "Containers for web applications\nSome web-based applications are cumbersome to install (Rstudio server being the worst, probably, amongst softwares used by academics and researchers), cannot be deployed through a conda environment (Rstudio server again!), or require some administrator privileges to be installed (Once more, Rstudio server!). Here is where we can use containers: those include all the necessary operating system libraries and the software to make specific applications work. You can simply launch a container from the cluster, and it will be able to execute its installed web-based application without much effort.\n\n\n\n\n\n\nContainer\n\n\n\nA container is an isolated environment that packages a web-based application and all its dependencies (e.g., libraries, runtime, and configuration) so it can run consistently anywhere. On an HPC system without administrator privileges, containers are especially useful because all steps requiring elevated privileges can be avoided.\n\n\nContainers can be created with various softwares, the most famous being Docker and Singularity/Apptainer. The latter can import containers from Docker almost seamlessly, and you will see how it is used to launch a session of RStudio server."
  },
  {
    "objectID": "documentation/2024-11-28-ABC9.html#rstudio",
    "href": "documentation/2024-11-28-ABC9.html#rstudio",
    "title": "ABC.9: Graphical applications on GenomeDK",
    "section": "Launch Rstudio",
    "text": "Launch Rstudio\nThe most practical way to launch Rstudio is through a web application with Rstudio server, deployed from a container, instead of Rstudio desktop deployed with X11 forwarding from a conda environment, which can cause a number of incompatibilities depending on your local computer. Follow these step-by-step instructions with explanations:\n\nLog into GenomeDK as usual. In your own project, create a folder dedicated to Rstudio, so that it is there and usable by you or others in the project. For example do mkdir -p rstudio; cd rstudio\nCreate a conda environment with some R packages. I have made one for you with R v4.4.2, renv, ggplot and tidyverse.\nwget https://raw.githubusercontent.com/AU-ABC/AU-ABC.github.io/refs/heads/main/documentation/2024-11-28-ABC9/ABC9rstudio.yml \nconda env create -f ABC9rstudio.yml \nIf you already have an environment with R and some R packages, do not create the one above, but activate yours.\nCheck your user id. This is specific to you. Note it down, because you will need it.\necho $UID\nOnly once you need to download the container which provides Rstudio. Here we use version 4.2.2 because it has been tested for this tutorial:\nsingularity pull docker://rocker/rstudio:4.2.2\nAt this point you should have a file called something like rstudio_4.2.2.sif.\nDo the steps below only once in the same folder where you have the sif file. This will created folders and configurations which allow Rstudio server to work.\nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nEvery time you want to run Rstudio, open tmux to have a permanent desktop in background and request an interactive job. Your session and interactive job will not close even if you accidentally close the command line interface!. For example below I request a job with 2 cores, 32GB of RAM for 4 hours for the project myProject. Note that you also need a conda environment to be active with an R installation and the needed packages.\ntmux new -t rstudio\nrm -rf ~/.local/share/rstudio/ #remove old cache\nsrun --mem=32g --cores=2 --time=4:00:0 --account=myProject --pty bash\nconda activate ABC9rstudio\nCheck the name of the node and note it down by running\nhostname\nand finally open Rstudio:\nsingularity exec --bind run:/run,var-lib-rstudio-server:/var/lib/rstudio-server,database.conf:/etc/rstudio/database.conf rstudio_4.2.2.sif rserver --www-address=`hostname` --www-port $UID --server-user $USER\nNow you need to create a tunnel between your computer and the cluster. Get ready with your user id and node name. Open a second, new terminal window, and write\nssh -L&lt;USERID&gt;:&lt;HOSTNAME&gt;:&lt;USERID&gt; &lt;USERNAME&gt;@login.genome.au.dk\nYou should see Rstudio if you write the following address in your browser\n\nlocalhost:&lt;USERID&gt;\nwriting your user id instead of &lt;USERID&gt;.\nNow you are ready to use Rstudio and all the packages from your conda environment. Note that the environment contains the package renv, so you can also start creating virtual R environments from Rstudio, as we did in the second ABC tutorial.\n\nWhat to do if I close the terminals or I lost internet connection?\nSimple, open a terminal and recreate a tunnel with\nssh -L&lt;USERID&gt;:&lt;HOSTNAME&gt;:&lt;USERID&gt; &lt;USERNAME&gt;login.genome.au.dk\nThe tmux software has kept your terminal running without interruption, so now Rstudio should be visible in your browser."
  },
  {
    "objectID": "documentation/2024-11-28-ABC9.html#jupyterlab",
    "href": "documentation/2024-11-28-ABC9.html#jupyterlab",
    "title": "ABC.9: Graphical applications on GenomeDK",
    "section": "Web application with jupyterlab",
    "text": "Web application with jupyterlab\n\nLog into GenomeDK as usual.\nCreate a conda environment with some R packages, python, jupyterlab. I have made one for you with R&gt;v4.4.0, python v3.10, the AI plugin for jupyterlab and some R packages.\nwget https://raw.githubusercontent.com/AU-ABC/AU-ABC.github.io/refs/heads/main/documentation/2024-11-28-ABC9/ABC9jupyter.yml \nconda env create -f ABC9jupyter.yml \nCheck your user id. This is specific to you. Note it down, because you will need it.\necho $UID\nEvery time you want to run jupyterlab, open tmux to have a permanent desktop in background and request an interactive job. Your session and interactive job will not close even if you accidentally close the command line interface!. For example below I request a job with 2 cores, 32GB of RAM for 4 hours for the project myProject. Note that we activate the conda environment which contains\ntmux new -t rstudio\nsrun --mem=32g --cores=2 --time=4:00:0 --account=myProject --pty bash\nconda activate ABC9jupyter\nCheck the name of the node and note it down by running\nhostname\nDownload the Mistral LLM open model trained on 7Billion parameters\nmkdir -p /home/`whoami`/.cache/gpt4all/\nwget -nc -O /home/`whoami`/.cache/gpt4all/mistral-7b-openorca.Q4_0.gguf https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q4_0.gguf\nRun jupyterlab\njupyter lab --ip=`hostname` --port=$UID --no-browser --NotebookApp.token='' --NotebookApp.password=''\nNow you need to create a tunnel between your computer and the cluster. Get ready with your user id and node name. Open a second, new terminal window, and write\nssh -L&lt;USERID&gt;:&lt;HOSTNAME&gt;:&lt;USERID&gt; &lt;USERNAME&gt;@login.genome.au.dk\nwriting your user id, hostname and username instead of &lt;USERID&gt;, &lt;HOSTNAME&gt; and &lt;USERNAME&gt;.\nYou should see Rstudio if you write the following address in your browser\n\n127.0.0.1:&lt;USERID&gt;/lab\nwriting your user id instead of &lt;USERID&gt;. Jupyterlab will open!\n\nNow click on the AI plugin on the left-side toolbar with symbol . Open the settings as suggested, and choose the Mistral model and the ChatGPT embedding which allows to use the language model (see choices below), then Save the settings and go back with the arrow on top of the plugin window.\n\n \n\nNow you can ask any question in the plugin. Simply write /ask followed by what you want to know. It requires some resources to use an LLM, so it might be slow withthe resources used in this tutorial.\nUse the launcher to choose if you want to start a notebook with python or R, including the packages installed in the conda environment. The launcher looks similar to this:"
  },
  {
    "objectID": "documentation/2025-02-27-ABC13.html",
    "href": "documentation/2025-02-27-ABC13.html",
    "title": "ABC.13: BulkRNA deconvolution",
    "section": "",
    "text": "Slides\nToday’s slides with topic presentation for BulkRNA deconvolution with scRNA-seq data. \n \n\n Download Slides \n\n \n\n\nTutorial TBA"
  },
  {
    "objectID": "documentation/2024-12-12-ABC10.html",
    "href": "documentation/2024-12-12-ABC10.html",
    "title": "ABC.10: Detailed bulkRNA data alignment and quality control",
    "section": "",
    "text": "Slides\nDetailed introduction from Marie to the tutorial topic\n\n Download Slides \n\n\n\nTutorial\nIn construction, we had some problem with it!\n\n\n\n\n\n\nNote\n\n\n\nsdjkfbasjkasjk h"
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html",
    "href": "documentation/2024-06-13-ABC1.html",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "",
    "text": "Introductory slides about the ABC, and who is behind the Health Data Science Sandbox and the Core Bioinformatics Facility\n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#rstudio-layout",
    "href": "documentation/2024-06-13-ABC1.html#rstudio-layout",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Rstudio Layout",
    "text": "Rstudio Layout\n\nLaunch RStudio.\nFamiliarize yourself with the RStudio interface, which consists of:\n\nSource Pane: Where you write and edit scripts.\nConsole Pane: Where you run commands and see output.\nEnvironment/History Pane: Shows your workspace and command history.\nFiles/Plots/Packages/Help/Viewer Pane: Access your files, view plots, manage packages, get help, and view HTML output."
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#make-a-simple-script",
    "href": "documentation/2024-06-13-ABC1.html#make-a-simple-script",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Make a simple script",
    "text": "Make a simple script\n\nClick on File &gt; New File &gt; R Script to open a new script editor.\nInside the file you created, Write a simple script. Copy for example the script below:\n\n\nprint(\"Hello ABC\")\nx &lt;- 1:10\ny &lt;- x^2\nplot(x, y, type=\"b\", col=\"blue\")\n\n[1] \"Hello ABC\"\n\n\n\n\n\n\n\n\n\n\nTry to run the script (The small Run button). Two things should happen:\n\nAn output is shown in the console (bottom left)\nA plot is created and shown in the plotting window (bottom right)\nthe variables x and y are saved in your environment and can be seen in the variable explorer (top right). These variables can be used again since they exist in your computer’s memory.\n\n\n\n\n\n\n\n\nWorking directory\n\n\n\nEvery time you start working in R, this will be considering a working directory. Such directory is the reference point you are working in. For example, if you want to open a file, you need to know where it is in relation to your working directory, so that you can correctly write where it is. Write the command getwd() in the console and press Enter to see your current working directory."
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#some-basic-operations",
    "href": "documentation/2024-06-13-ABC1.html#some-basic-operations",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Some basic operations",
    "text": "Some basic operations\nIn R you can perform basic math operations by using the appropriate symbol. For example\n\n2+3\n9*2\n5^2\n\n5\n\n\n18\n\n\n25\n\n\nYou can assign variables (“objects”) using the symbol &lt;-. For example\n\nx &lt;- 5\nanothername &lt;- 3\nitCanBeAnything &lt;- \"It can be text\""
  },
  {
    "objectID": "documentation/2024-06-13-ABC1.html#lets-try-some-exercises",
    "href": "documentation/2024-06-13-ABC1.html#lets-try-some-exercises",
    "title": "ABC.1: Install R+Rstudio, first R coding problems/solutions",
    "section": "Lets try some exercises!",
    "text": "Lets try some exercises!\nCreate a new script file or use the console to test some exercises below.\n\n1. Create a data frame\nWe will create a simple data frame. A data frame is nothing more than a table, where both rows and columns have labels, and can be easily accessed and manipulated. To create a small data frame, we can define its columns. We define each column through a vector with the function c(), where we can write values inside separated by a comma. Then we provide all vectors to the function data.frame, where we assign column names (Gene, Control, Treatment1, Treatment2).\n\n\n\n\n\n\nNote\n\n\n\nMake sure your vectors are all of the same length! Also, each vector usually contains values of the same type (for example only numbers or only text)\n\n\n\ngeneExpr &lt;- data.frame(\n  Gene = c(\"GeneA\", \"GeneB\", \"GeneC\"),\n  Control = c(10, 20, 30),\n  Treatment1 = c(15, 25, 35),\n  Treatment2 = c(100, 0, 250)\n)\n\ngeneExpr\n\n\nA data.frame: 3 × 4\n\n\nGene\nControl\nTreatment1\nTreatment2\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nGeneA\n10\n15\n100\n\n\nGeneB\n20\n25\n0\n\n\nGeneC\n30\n35\n250\n\n\n\n\n\nYou should be able to see the small data frame printed in output and also shown in the variable explorer.\n\n\n2. Plot from a data frame\nNow lets try to plot it Treatment 1 versus Treatment 2. The basic function for plotting in R is called plot. It takes as arguments the x axis and the y axis. It has other options which are not mandatory, such as style and color of the plot. Notice how we access the values in the columns using the $ sign.\n\nplot( x = geneExpr$Treatment1, \n      y = geneExpr$Treatment2 , \n      main=\"Expression per Treatment group\", \n      xlab = \"Treatment 1\", \n      ylab = \"Treatment 2\" , \n      col =\"blue\" , \n      type =\"b\")\n\n\n\n\n\n\n\n\nYou should be able to see a plot in the plotting window. In the command above we used many options beyond x and y. Can you see what they match in the plot?\n\n\n3. Summary statistics\nThere are lots of summary statistics already implemented in R. Below we calculate mean, median and standard deviation for the column Treatment1 of the data frame and then we print them.\n\nx &lt;- geneExpr$Treatment1\nmeanTr1 &lt;- mean(x)\nmedianTr1 &lt;- median(x)\nsdTr1 &lt;- sd(x)\n\nprint(\"mean, median and sd:\")\nprint(c(meanTr1, medianTr1, sdTr1))\n\n[1] \"mean, median and sd:\"\n[1] 25 25 10\n\n\nThis was neat! Can you try to calculate the cumulative sum of the difference between Treatment 1 and Treatment 2?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nx &lt;- geneExpr$Treatment1 - geneExpr$Treatment2\ncsum &lt;- cumsum(x)\nprint(\"cumulative sum of Tr1 - Tr2:\")\nprint(csum)\n\n[1] \"cumulative sum of Tr1 - Tr2:\"\n[1]  -85  -60 -275\n\n\n\n\n\n\n\n4. Define your own function!\nAlthough R and the packages you can find have almost everything you will need, sometimes you might need to define your own function. The syntax to do it is very easy: you assign a function to a name, which then you will be able to use. Below, there is a function taking an argument (arg1) and multiplying it by 5. The function commands need to be between the curly brackets, and what we want as output need to be explicit through the return() function.\n\nmyFunction &lt;- function(arg1){\n  \n  # Now arg1 is a variable of the function. \n  # You can write comments inside code blocks with #\n  output &lt;- arg1 * 5\n  return (output)\n\n}\n\nSuch a function works if the argument is a number, but also if it is a vector!\n\nprint(\"with a number only\")\nmyFunction(5)\nprint(\"with a vector\")\nmyFunction(geneExpr$Treatment1)\n\n[1] \"with a number only\"\n[1] \"with a vector\"\n\n\n25\n\n\n\n75125175\n\n\nTry to make a function that takes three vectors, plots the first against the sum of the second and third, and returns the sum of all three vectors. Use the plot command we applied previously for help.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsimpleSumPlot &lt;- function (arg1, arg2, arg3){\n  \n  arg23 &lt;- arg2 + arg3\n  arg123 &lt;- arg1 + arg2 + arg3\n\n  #plotting\n  plot( x = arg1, \n      y = arg23, \n      main=\"Plot with my own function\", \n      xlab = \"arg1\", \n      ylab = \"arg2+arg3\" , \n      col =\"red\", \n      type =\"o\" )\n\n  return (arg123)\n  \n}\n\nNow you can try this on vectors of the same length. We can use the ones in our data frame!\n\nsimpleSumPlot(arg1=geneExpr$Control, \n              arg2=geneExpr$Treatment1, \n              arg3=geneExpr$Treatment2)\n\n\n12545315\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Read files\nMany times we want to read files from excel or other formats. R has many ways to do this and if not there are always packages out there to help you read the format you have. For reading an excel file a great package is the readxl. To read a csv file there is already the R function read.csv().\n\n\n\n\n\n\nNote\n\n\n\nBut wait. what are packages? Each package consist of a set of R and other scripts that meet specific needs for the users of R. For example openxlsx reads from Excel files, which R cannot do on its own. There are thousands of packages out there, ranging all fields of science, and some have become very popular.\n\n\nTry to install the package openxlsx. You can use the command install.packages(\"openxlsx\") in your RStudio console. Otherwise, go on the bottom right panel and click Packages and Install like this\n\nNow you are ready to import an Excel file. To use the package, we can load it with library(openxlsx). Otherwise we need to write the package name before the command to use from it (as done below). You can get a file locally on your computer or from an URL as done in this example.\n\ndf &lt;- openxlsx::read.xlsx(\"https://github.com/AU-ABC/AU-ABC.github.io/raw/main/documentation/2024-06-13-instR/data/data.xlsx\", sheet=1)\ndf\n\n\nA data.frame: 3 × 3\n\n\n\nx\ny\nz\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n10\n30\n-1\n\n\n2\n20\n20\n0\n\n\n3\n30\n10\n1\n\n\n\n\n\nOnce you have a data frame, you can always save it. Remember, the path of the saved file is related to your current working directory! To save your data frame as a csv file, use\n\nwrite.table(geneExpr, \n            file = \"./myFirstDataFrame.csv\", \n            row.names = FALSE , \n            sep=\"\\t\")\n\nwhere we remove the labels for the rows and use the tab separator instead of the comma. To read the file again, simply use\n\ndf2 &lt;- read.csv(\"./myFirstDataFrame.csv\" , sep=\"\\t\")\ndf2\n\n\nA data.frame: 3 × 4\n\n\nGene\nControl\nTreatment1\nTreatment2\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nGeneA\n10\n15\n100\n\n\nGeneB\n20\n25\n0\n\n\nGeneC\n30\n35\n250\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the tab key of your keyboard to see a list of the available paths"
  },
  {
    "objectID": "documentation/2025-01-16-ABC11.html",
    "href": "documentation/2025-01-16-ABC11.html",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "",
    "text": "Today’s slides with topic presentation about local language models with Ollama (from Manuel). \n \n\n Download Slides"
  },
  {
    "objectID": "documentation/2025-01-16-ABC11.html#the-seurat-object",
    "href": "documentation/2025-01-16-ABC11.html#the-seurat-object",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "The seurat object",
    "text": "The seurat object\nThe Seurat object is a list that contains all the information and calculated results about the single-cell experiment. It contains the following slots:\n\nslotNames(sample)\n\n\n'assays''meta.data''active.assay''active.ident''graphs''neighbors''reductions''images''project.name''misc''version''commands''tools'\n\n\n\n\n\nSlot\nFunction\n\n\n\n\nassays\nA list of assays within this object, such as RNAseq\n\n\nmeta.data\nContains cell-level meta data\n\n\nactive.assay\nName of active, or default, assay\n\n\nactive.ident\nIdentity classes for the current object\n\n\nreductions\nA list of DimReduc objects, like PCA or UMAP\n\n\nproject.name\nUser-defined project name (optional)\n\n\n\nYou can always look at any slot using the @ operator. For example, look at what assays are in the object:\n\nsample@assays\n\n$RNA\nAssay (v5) data with 36601 features for 8583 cells\nFirst 10 features:\n MIR1302-2HG, FAM138A, OR4F5, AL627309.1, AL627309.3, AL627309.2,\nAL627309.5, AL627309.4, AP006222.2, AL732372.1 \nLayers:\n counts \n\n\nThere is so far only one assay, with the name RNA. This is the default assay at the beginning of an analysys, and it contains the count matrix only at this time. Each matrix contained in the assay is called a Layer, and you can see there is only counts.\nOther layers (matrices) that are added during an analysis are in the table below. An assay can also contain meta.features, which is a metadata table related to each gene (e.g. gene symbols, biotypes, gene ontology, …).\n\nsample@assays$RNA\n\nAssay (v5) data with 36601 features for 8583 cells\nFirst 10 features:\n MIR1302-2HG, FAM138A, OR4F5, AL627309.1, AL627309.3, AL627309.2,\nAL627309.5, AL627309.4, AP006222.2, AL732372.1 \nLayers:\n counts \n\n\n\n\n\nLayer\nFunction\n\n\n\n\ncounts\nStores unnormalized data such as raw counts or TPMs\n\n\ndata\nNormalized data matrix\n\n\nscale.data\nScaled data matrix\n\n\nmeta.features\nFeature-level meta data\n\n\n\nYou can use the command below to access a specific layer inside the assay which is currently active in the dataset.\n\nGetAssayData(object = sample, layer =\"counts\")[1:10,1:10]\n\n  [[ suppressing 10 column names ‘AAACCTGAGATCCTGT-1’, ‘AAACCTGAGCCTATGT-1’, ‘AAACCTGAGCTTTGGT-1’ ... ]]\n\n\n\n10 x 10 sparse Matrix of class \"dgCMatrix\"\n                               \nMIR1302-2HG . . . . . . . . . .\nFAM138A     . . . . . . . . . .\nOR4F5       . . . . . . . . . .\nAL627309.1  . . . . . . 1 . . .\nAL627309.3  . . . . . . . . . .\nAL627309.2  . . . . . . . . . .\nAL627309.5  . . . . . . . . . .\nAL627309.4  . . . . . . . . . .\nAP006222.2  . . . . . . . . . .\nAL732372.1  . . . . . . . . . .\n\n\nTo see the assay currently active (which means used in the analysis), you can access the slot active.assay:\n\nsample@active.assay\n\n'RNA'\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsually, the default assay changes automatically after performing a normalization, so you do not need to do things manually like this. But it is good to know how to access the slots in the Seurat object to verify the data used.\n\n\n\nMetadata\nAnother important assay is the meta.data slot, which contains cell-level metadata. This is where you can store information about each cell, such as cell type, cell cycle phase, or any other information you have about the cells. For example, we add below the percentage of mitochondrial genes in each cell. This is a common quality control metric, as high percentages of mitochondrial genes can indicate poor-quality cells.\n\nsample[[\"percent.mt\"]] &lt;- PercentageFeatureSet(sample, pattern = \"^MT-\")\n\nAgain, you can access the metadata using the @ operator:\n\nhead(sample@meta.data)\n\n\nA data.frame: 6 × 4\n\n\n\norig.ident\nnCount_RNA\nnFeature_RNA\npercent.mt\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAAACCTGAGATCCTGT-1\nspermatogenesis\n694\n473\n5.04322767\n\n\nAAACCTGAGCCTATGT-1\nspermatogenesis\n6868\n2016\n0.04368084\n\n\nAAACCTGAGCTTTGGT-1\nspermatogenesis\n9188\n3670\n2.18763605\n\n\nAAACCTGAGGATGGTC-1\nspermatogenesis\n4021\n2175\n0.00000000\n\n\nAAACCTGAGTACGACG-1\nspermatogenesis\n3237\n1622\n1.54464010\n\n\nAAACCTGAGTACGCCC-1\nspermatogenesis\n10213\n3253\n0.29374327"
  },
  {
    "objectID": "documentation/2025-01-16-ABC11.html#filtering",
    "href": "documentation/2025-01-16-ABC11.html#filtering",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Filtering",
    "text": "Filtering\nWe filter out cells with low quality, such as cells with low gene counts or high mitochondrial gene content.\nTo have an idea of the distribution of those quality measures in each cell, we can plot them using the VlnPlot function. This function creates violin plots of the distribution of a given feature (e.g. gene counts, mitochondrial gene content) across all cells.\n\noptions(repr.plot.width = 12, repr.plot.height=6)\nVlnPlot(sample, \n        features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), \n        ncol = 3,\n        alpha = .25)\n\nWarning message:\n“Default search for \"data\" layer in \"RNA\" assay yielded no results; utilizing \"counts\" layer instead.”\n\n\n\n\n\n\n\n\n\nNote how there are some cells with very few features (detected genes with at least one transcript) and very few counts (total transcripts associated to a gene). These cells are likely to be low-quality, or empty droplets with contamination, and should be removed from the analysis. The same for the cells with too many genes and transcripts, which are likely to be doublets (two cells captured in the same droplet).\nAnother useful plot is the one giving the relationship between number of transcripts and features in each cell. usually, those two measures grow together. If not, it can be a sign of low-quality cells. We can also notice a few outliers with huge amount of transcripts, but not always as many features. These are likely to be doublets.\n\nFeatureScatter(sample, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") \n\n\n\n\n\n\n\n\nNow we can choose the thresholds for filtering. We will remove cells with less than 2000 genes and more than 6000 genes, counts below 5000 and above 30000, and more than 10% mitochondrial genes.\n\nMIN_COUNT = 5000  #minimum number of transcripts per cell\nMAX_COUNT = 30000 #maximum number of transcripts per cell\nMIN_GENES = 2000   #minimum number of genes per cell\nMAX_GENES = 6000   #maximum number of genes per cell\nMAX_MITO = 10      #mitocondrial percentage treshold)\n\nsample &lt;- subset( sample, \n                  subset = nFeature_RNA &gt; MIN_GENES & \n                           nFeature_RNA &lt; MAX_GENES &\n                           nCount_RNA &gt; MIN_COUNT & \n                           nCount_RNA &lt; MAX_COUNT &\n                           percent.mt &lt; MAX_MITO )\n\nNow the violin plot looks more uniform without a huge bottom peak, which is a good sign.\n\nVlnPlot(sample, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\nFeatureScatter(sample, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") \n\nWarning message:\n“Default search for \"data\" layer in \"RNA\" assay yielded no results; utilizing \"counts\" layer instead.”"
  },
  {
    "objectID": "documentation/2025-01-16-ABC11.html#normalization-i",
    "href": "documentation/2025-01-16-ABC11.html#normalization-i",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Normalization I",
    "text": "Normalization I\nNormalization is the process of removing technical biases from the data. The most common normalization method is the LogNormalize method, which normalizes the data by the total expression, multiplies by a scale factor (10,000 by default), and log-transforms the result. This is done by NormalizeData. Usually the data is scaled using ScaleData to have a mean of 0 and a variance of 1.\n\nsample &lt;- NormalizeData(sample, normalization.method = \"LogNormalize\")\nsample &lt;- ScaleData(sample)\n\nNormalizing layer: counts\n\nCentering and scaling data matrix\n\n\n\nNow look at the RNA assay again. You can see that - data layer has been added, which contains the normalized data - scale.data is the data layer scaled with ScaleData\n\nsample@assays\n\n$RNA\nAssay (v5) data with 36601 features for 3883 cells\nFirst 10 features:\n MIR1302-2HG, FAM138A, OR4F5, AL627309.1, AL627309.3, AL627309.2,\nAL627309.5, AL627309.4, AP006222.2, AL732372.1 \nLayers:\n counts, data, scale.data \n\n\n\n\n\n\n\n\nTip\n\n\n\nWhy do you have default assays and not default layers? When you choose a specific assay, the various algorithms in Seurat will automatically choose the correct layer in the assay. If there is no layer with the correct data, Seurat will throw an error. For example PCA will not work if you do not have a scale.data layer in the assay. All those details are in general taken care of automatically, but you can always have ?@fig-object below as a reference for all the various assays and layers, and to which Seurat functions they are used.\n\n\n\nStructure of Seurat object, with the assays and functions connected to them in the Seurat package. Further assays and functions can exist if you use additional external packages."
  },
  {
    "objectID": "documentation/2025-01-16-ABC11.html#pca-and-umap-visualizations",
    "href": "documentation/2025-01-16-ABC11.html#pca-and-umap-visualizations",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "PCA and UMAP visualizations",
    "text": "PCA and UMAP visualizations\nNow we can plot a dimensionality reduction of our data. Usually, you look for the most variable genes, which are the most meaningful on a biological level. This is done with the FindVariableFeatures function.\nWhat does it do? This function calculates the mean and dispersion of each gene, and then selects the most variable genes based on a threshold. The default is to select the top 2000 most variable genes.\n\nsample &lt;- FindVariableFeatures(sample, nfeatures = 2000)\n\nFinding variable features for layer counts\n\n\n\nIf you look at the assay, you can notice a small change, where it now shows the top variable features instead of the first ten in the expression matrix\n\nsample@assays$RNA\n\nAssay (v5) data with 36601 features for 3883 cells\nTop 10 variable features:\n PTGDS, ACTA2, TMSB4X, TAGLN, B2M, S100A6, MYL9, MGP, DCN, KRT17 \nLayers:\n counts, data, scale.data \n\n\nYou can now run and print the PCA of the data, which will be based on the variable features. The PCA is a way to reduce the dimensionality of the data, and it is useful to visualize the data in 2D or 3D. The PCA is calculated using the RunPCA function.\n\n\n\n\n\n\nTip\n\n\n\nThe first components are the most relevant to the data, and the PCA function prints the genes that influence the most each component. This is useful to understand what the components represent. For example, if one component has a long list of mitochondrial genes, it is likely to represent the mitochondrial content of the cells, which you might want to remove in the future analysis where it could cover for other biological factors that are relevant to you.\n\n\n\nsample &lt;- RunPCA(sample, features = VariableFeatures(object = sample))\nFeaturePlot(sample, reduction = 'pca', features = \"nCount_RNA\")\n\nPC_ 1 \nPositive:  HMGB4, GTSF1L, ACTL7A, TSSK2, GAPDHS, SMCP, FHL5, NUPR2, SPTY2D1OS, CAPZA3 \n       H1FNT, C10orf82, FAM71E1, RNF151, C16orf82, ACTL9, IQCF1, C2orf88, FAM71F1, SPATA19 \n       CCDC196, GNG2, C9orf24, SPERT, CCDC91, CLPB, TSSK1B, TMCO2, LELP1, AC002463.1 \nNegative:  HSP90AA1, NPM1, COX7A2L, MT-CO1, MT-ND2, PTGES3, MT-CO3, MT-ND1, MT-CO2, RPS17 \n       PARK7, MT-ATP6, CALM2, GABARAPL2, MT-ND4, BANF1, TRMT112, H2AFZ, CKS2, SNRPD1 \n       FTH1, RPS12, RPL21, HNRNPA3, EIF5, RBX1, MTX2, RPL18, SBNO1, BUD23 \nPC_ 2 \nPositive:  FTL, SOD1, NDUFA13, MIF, S100A11, S100A10, RPL10, FBL, ANXA2, CNN1 \n       RPS4Y1, LDHB, RPL36A, RPS21, PRDX1, GLUL, RBM3, RPL18A, CIRBP, RPL39 \n       RPS4X, EIF4E, SLC25A5, GPI, SLC25A6, RPL21, RPLP1, HSPB1, NME2, GSTP1 \nNegative:  LDHC, SPINK2, TMEM225B, REXO5, CCDC110, ZPBP2, CCT6B, AL133499.1, ANKRD7, PPP3R2 \n       DYDC1, CETN1, PTTG1, SPAG6, CCDC42, CLGN, LINC01016, DBF4, PBK, ETFRF1 \n       MCHR2-AS1, TBPL1, SPATA22, PPP1R1C, CT66, C15orf61, TMPRSS12, KPNA5, AC002467.1, ZC2HC1C \nPC_ 3 \nPositive:  ENG, BST2, OSR2, MGP, FOS, MYL9, JUNB, TSHZ2, TIMP3, NUPR1 \n       CAV1, SFRP4, VIM, LGALS1, CALD1, IFI6, APOD, SMOC2, BGN, ZFP36 \n       TAGLN, FBLN5, SH3BGRL, SPARC, PRDX2, DPEP1, MYH11, IGFBP7, JUN, TCEAL3 \nNegative:  SMC1B, DPEP3, LY6K, CKS2, CHIC2, CENPH, SPINT2, ZCWPW1, SYCP3, TOP2A \n       TEX101, DNMT1, PRSS21, HENMT1, GINS2, BUD23, SMC3, TPTE, HMGB2, ORC6 \n       MAJIN, SQLE, SMC4, VCX, FABP5, DMRTC2, BTG3, UNK, TEX12, MIS18A \nPC_ 4 \nPositive:  PRM1, AC016747.1, LINC01921, PRM2, LRRD1, TNP2, TNP1, LRRC8C-DT, FAM216A, GIHCG \n       C5orf58, EIF5AL1, TMED10, AC073111.1, TSSK6, CCDC179, NUF2, RANGAP1, AC079385.2, LINC01206 \n       PHOSPHO1, ESS2, LDHAL6B, CETN3, TDRG1, CAP1, AL354707.1, OTUB2, SPATA8, CUTC \nNegative:  CMC1, HMGN1, TJP3, FAM24A, C4orf17, TFDP2, DPP10-AS3, AC022784.5, ABRA, TPRG1 \n       PRSS55, C17orf98, ATP6V1E2, LINC02032, LRRC3B, AL450306.1, TMEM243, LYZL6, GPR85, CLDND2 \n       XKR3, LYZL1, SPACA1, LRRC39, LINC01988, LYZL4, AP002528.1, AFG1L, LYZL2, C17orf50 \nPC_ 5 \nPositive:  C5orf47, TEX101, ARL6IP1, SYCP1, SELENOT, TOP2A, HLA-B, LINC01120, MAJIN, HORMAD1 \n       TDRG1, ANKRD31, TPTE, LY6K, XKR9, HSP90B1, LINC00668, TEX12, DMC1, AC044839.1 \n       PRSS21, DMRTC2, SPO11, SYCP3, SCML1, C1orf146, RAD51AP2, AL138889.1, C5orf58, AC240274.1 \nNegative:  ALKAL2, ANKRD37, GYG1, TUBA1B, CSTF1, RHOXF1, AURKA, KRT18, SOX4, RGCC \n       PAFAH1B3, RABIF, EGFL7, LINC01511, SMS, AL445985.2, HMGA1, RABGGTA, CAVIN3, AC004817.2 \n       TMEM14A, ISOC2, TCF3, SIX1, CDCA7L, PPP1CA, L1TD1, RAC3, KLHL23, ASB9 \n\n\n\n\n\n\n\n\n\n\nThe plot above is coloured by the number of transcripts in each cell. Each dot is a cell. You can see that the PCA separates the cells by the number of transcripts - This is because the number of transcripts is a technical factor, and the simple normalization we did before probably did not remove for its beffect on the data.\nAnother thing to take into account is that the PCA is not the best way to visualize the data, as it is a linear transformation of the data. The UMAP is a non-linear transformation that is more suitable for single-cell data. You can run the UMAP using the RunUMAP function. But before, look at how much variance there is in each PCA component (plot below), and there choose a threshold where the value is low. Usually 10 to 15 is just fine.\n\nElbowPlot(sample)\n\n\n\n\n\n\n\n\nThen run the UMAP selecting the first ten dimensions of the PCA\n\nsample &lt;- RunUMAP(sample, dims = 1:10)\n\nWarning message:\n“The default method for RunUMAP has changed from calling Python UMAP via reticulate to the R-native UWOT using the cosine metric\nTo use Python UMAP via reticulate, set umap.method to 'umap-learn' and metric to 'correlation'\nThis message will be shown once per session”\n14:58:33 UMAP embedding parameters a = 0.9922 b = 1.112\n\n14:58:33 Read 3883 rows and found 10 numeric columns\n\n14:58:33 Using Annoy for neighbor search, n_neighbors = 30\n\n14:58:33 Building Annoy index with metric = cosine, n_trees = 50\n\n0%   10   20   30   40   50   60   70   80   90   100%\n\n[----|----|----|----|----|----|----|----|----|----|\n\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n|\n\n14:58:33 Writing NN index file to temp file /tmp/RtmphwjaOt/file1efcb5fdb2ff7\n\n14:58:33 Searching Annoy index using 1 thread, search_k = 3000\n\n14:58:34 Annoy recall = 100%\n\n14:58:34 Commencing smooth kNN distance calibration using 1 thread\n with target n_neighbors = 30\n\n14:58:35 Initializing from normalized Laplacian + noise (using RSpectra)\n\n14:58:35 Commencing optimization for 500 epochs, with 148538 positive edges\n\n14:58:38 Optimization finished\n\n\n\nPretty! We can visualize some know markers, AKAP4 for maturing spermatozoa and PIWIL1 for meiosis. You can see that the cells are separated by the expression of these genes. This is a good sign that the clustering will work well, and that the UMAP represents biological variation. Since UMAP is based on PCA, the PCA was also a good representation of the data and not only biased by the number of transcripts as technical factor.\n\nFeaturePlot(sample, reduction = \"umap\", features = c(\"AKAP4\",\"PIWIL1\"))"
  },
  {
    "objectID": "documentation/2025-01-16-ABC11.html#normalization-ii",
    "href": "documentation/2025-01-16-ABC11.html#normalization-ii",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Normalization II",
    "text": "Normalization II\nNow, one might want a better normalization method. The one we used before is very simple and does not take into account the technical factors that can be present in the data. For example, we might have found out that the PCA was completely biased by cell cycle markers, and we want to remove this bias. This is done by the SCTransform function, which is a more advanced normalization method that takes into account technical factors in the data. Below we do not insert technical factors, but you can do it by adding them in the vars.to.regress argument. Try to remove the effect of mitochondrial content using vars.to.regress = \"percent.mito\" in the command below (we noticed after all some mitochondrial genes in the first PCA component). One can add also multiple factors, like vars.to.regress = c(\"percent.mito\", \"cell.cycle\").\n\nsample &lt;- SCTransform(sample)\n\nRunning SCTransform on assay: RNA\n\nRunning SCTransform on layer: counts\n\nvst.flavor='v2' set. Using model with fixed slope and excluding poisson genes.\n\nVariance stabilizing transformation of count matrix of size 26386 by 3883\n\nModel formula is y ~ log_umi\n\nGet Negative Binomial regression parameters per gene\n\nUsing 2000 genes, 3883 cells\n\nFound 12 outliers - those will be ignored in fitting/regularization step\n\n\nSecond step: Get residuals using fitted parameters for 26386 genes\n\nComputing corrected count matrix for 26386 genes\n\nCalculating gene attributes\n\nWall clock passed: Time difference of 48.80969 secs\n\nDetermine variable features\n\nCentering data matrix\n\nSet default assay to SCT\n\n\n\nLet’s calculate PCA and UMAP again\n\nsample &lt;- RunPCA(sample, features = VariableFeatures(object = sample))\n\nPC_ 1 \nPositive:  MT-CO2, MT-ND4, MT-CO1, MT-CO3, MT-CYB, RPL21, PTMA, RPL18A, HIST1H4C, MT-ATP6 \n       MALAT1, MT-ND1, HSP90AA1, RPS2, GAGE2A, SMC3, RPSA, CCNI, RPLP1, BUD23 \n       FTH1, RPL10, DNAJB6, RPS12, RPS19, PRDX1, CIRBP, HMGB1, NCL, RNPS1 \nNegative:  HMGB4, TNP1, PRM1, PRM2, LINC01921, CRISP2, SMCP, LINC00919, GTSF1L, LELP1 \n       RNF151, NUPR2, TSACC, C10orf62, TSSK2, GAPDHS, FAM209A, OAZ3, TEX37, CAPZA3 \n       CMTM2, FAM71A, AC002463.1, CATSPERZ, TEX38, C20orf141, FAM71B, TSPAN16, SH3RF2, IRGC \nPC_ 2 \nPositive:  AL133499.1, LDHC, ANKRD7, SPATA8, SPINK2, LYAR, RBAKDN, SPATA22, CCDC42, COPRS \n       ZMYND10, CAVIN3, CCNA1, GIHCG, PPP3R2, CLGN, GTF2A2, ZPBP2, REXO5, TBPL1 \n       MRPL34, H2AFJ, PTTG1, AC004817.2, TMEM225B, CATSPERZ, UBB, APH1B, COX7A2, CCDC110 \nNegative:  PRM2, PRM1, TNP1, HMGB4, LINC01921, LINC00919, LELP1, GLUL, TEX37, FAM71B \n       FAM71A, C10orf62, RNF151, OAZ3, HEMGN, C20orf141, NFKBIB, CEP170, PRSS58, TSPAN16 \n       SPATA42, RPL18A, SH3RF2, TEX44, AC002463.1, IQCF2, OTUB2, SPATA3, TSSK6, RPL10 \nPC_ 3 \nPositive:  PRM1, PRM2, TNP1, TEX101, SYCP3, FMR1NB, LINC01921, LY6K, SMC3, HIST1H4C \n       SMC1B, C5orf58, DPEP3, HMGB2, SYCP1, HORMAD1, TPTE, HMGB4, TOP2A, CALM2 \n       LINC00919, SPINT2, BUD23, ZCWPW1, TDRG1, TEX12, HSP90AA1, CKS2, MAD2L1, NCL \nNegative:  RPS4X, VIM, RPL10, MGP, TMSB4X, MYL9, FOS, LGALS1, CALD1, PTGDS \n       TIMP1, B2M, IFITM3, SPARCL1, OSR2, RPL39, FTL, NUPR1, ENG, BEX3 \n       RPL13A, TPM2, CD63, BST2, DCN, ACTA2, MYL6, CAV1, EEF1A1, JUNB \nPC_ 4 \nPositive:  EGFL7, PAFAH1B3, SMS, RHOXF1, CCNI, RPS12, RPSA, SOX4, LYPLA1, GAGE2A \n       ASB9, TUBA1B, DNAJB6, ZNF428, HMGA1, PPP1CA, C17orf49, YWHAB, RPS2, LINC01511 \n       KRT18, RPS19, AL445985.2, NEIL2, MAGEB2, LIN7B, RNPS1, MAGEA4, TMEM14A, LSM2 \nNegative:  TEX101, MALAT1, CALM2, C5orf58, SYCP3, SELENOT, PTGDS, B2M, MGP, CTSL \n       SYCP1, LY6K, FOS, MYL9, FHL2, TDRG1, HORMAD1, LGALS1, TPTE, TIMP1 \n       SPARCL1, IFITM3, CALD1, DPEP1, BST2, DCN, ENG, NEAT1, OSR2, NUPR1 \nPC_ 5 \nPositive:  PRM2, PRM1, TNP1, LINC01921, SPATA8, AL133499.1, MRPL34, GIHCG, LYAR, TNP2 \n       COPRS, TEX37, SPINK2, GLUL, TSSK6, ISOC2, ZMYND10, FAM71B, CAVIN3, RGCC \n       H2AFZ, AC004817.2, CCNA1, HEMGN, AKAP12, RAN, OTUB2, TEX36-AS1, MT-CO1, ESS2 \nNegative:  SPACA1, EQTN, TJP3, ACRV1, GOLGA6L2, ERICH2, FAM209A, LYZL1, SPACA3, OLAH \n       FAM209B, HMGN1, LYZL2, ACTRT3, FNDC11, FAM24A, PEX5L-AS2, C17orf98, PRSS55, SPACA7 \n       LYZL6, TEX38, SPACA4, LYZL4, DPEP3, FMR1NB, BRI3, SMC3, TMEM190, C17orf50 \n\n\n\n\nElbowPlot(sample)\n\n\n\n\n\n\n\n\n\nsample &lt;- RunUMAP(sample, dims = 1:15)\n\n14:59:41 UMAP embedding parameters a = 0.9922 b = 1.112\n\n14:59:41 Read 3883 rows and found 15 numeric columns\n\n14:59:41 Using Annoy for neighbor search, n_neighbors = 30\n\n14:59:41 Building Annoy index with metric = cosine, n_trees = 50\n\n0%   10   20   30   40   50   60   70   80   90   100%\n\n[----|----|----|----|----|----|----|----|----|----|\n\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n|\n\n14:59:42 Writing NN index file to temp file /tmp/RtmphwjaOt/file1efcb66a5aeb3\n\n14:59:42 Searching Annoy index using 1 thread, search_k = 3000\n\n14:59:43 Annoy recall = 100%\n\n14:59:44 Commencing smooth kNN distance calibration using 1 thread\n with target n_neighbors = 30\n\n14:59:46 Initializing from normalized Laplacian + noise (using RSpectra)\n\n14:59:46 Commencing optimization for 500 epochs, with 147944 positive edges\n\n14:59:49 Optimization finished\n\n\n\nhere it is. Similar to what we had before (this is very nice and unbiased data)\n\nFeaturePlot(sample, reduction = \"umap\", features = c(\"AKAP4\",\"PIWIL1\"))\n\n\n\n\n\n\n\n\n\nPost-normalization Seurat object\nLook at the seurat object now. You can see there is a new active assay, called SCT (as in scTransform). This is the new normalization. counts is the count matrix corrected for biases by the SCTransform statistical model, and scale.data is the scaled data. data is the corrected matrix counts, normalized but not scaled.\nNote also that PCA and UMAP are saved in the assay.\n\nsample\n\nAn object of class Seurat \n62987 features across 3883 samples within 2 assays \nActive assay: SCT (26386 features, 3000 variable features)\n 3 layers present: counts, data, scale.data\n 1 other assay present: RNA\n 2 dimensional reductions calculated: pca, umap\n\n\nYou can also use slotNames to see an overview of the slots in the object. Try to look back at ?@fig-object to see how things match.\n\nslotNames(sample@assays$SCT)\n\n\n'SCTModel.list''counts''data''scale.data''assay.orig''var.features''meta.features''misc''key'"
  },
  {
    "objectID": "documentation/2025-01-16-ABC11.html#clustering",
    "href": "documentation/2025-01-16-ABC11.html#clustering",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Clustering",
    "text": "Clustering\nNow we go straight into clustering. Now it is very popular to pull cluster labels from large cell atlases, but it is good to know how to do it manually by looking at markers. First of all we calculate cell similarities (neighbors) and a clustering. We choose a small resolution parameters to avoid a too fine-grained clustering inthe tutorial. Your clusters might be looking different than in this tutorial!\n\nsample &lt;- FindNeighbors(sample, dims = 1:15)\nsample &lt;- FindClusters(sample, resolution = 0.3)\n\nComputing nearest neighbor graph\n\nComputing SNN\n\n\n\nModularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n\nNumber of nodes: 3883\nNumber of edges: 113438\n\nRunning Louvain algorithm...\nMaximum modularity in 10 random starts: 0.9486\nNumber of communities: 11\nElapsed time: 0 seconds\n\n\nNotice how cluster numbers are now assigned to the active.ident assay, unused until now.\n\nsample@active.ident[1:10]\n\nAAACCTGAGCCTATGT-16AAACCTGAGCTTTGGT-14AAACCTGAGTACGCCC-17AAACCTGAGTGAATTG-15AAACCTGCAGAGCCAA-15AAACCTGGTAGGGTAC-10AAACCTGGTCAGAAGC-19AAACCTGGTCGTGGCT-13AAACCTGTCTCTGTCG-10AAACGGGAGATGTGGC-14\n\n\n    \n        Levels:\n    \n    \n    '0''1''2''3''4''5''6''7''8''9''10'\n\n\n\nYou can plot the clusters as well on UMAP (or PCA)\n\nDimPlot(sample, reduction = 'umap', label=TRUE, label.size = 10)\n\n\n\n\n\n\n\n\nNow let’s look to some broad markers categories for spermatogenesis. This step requires biological knowledge of your data, and you can look at the literature or other resources to find the markers. For example, we can look at markers for spermatogonia, spermatocytes, and spermatids.\n\ncat(\"Spermatogonia Markers\\n\")\nFeaturePlot(sample, features = c('MKI67','DMRT1','STRA8','ID4'))\n\nSpermatogonia Markers\n\n\n\n\n\n\n\n\n\n\ncat(\"Spermatocytes Markers\\n\")\nFeaturePlot(sample, features = c('MEIOB','SYCP1','TEX101','PIWIL1','SPATA16','CLGN'))\n\nSpermatocytes Markers\n\n\n\n\n\n\n\n\n\n\ncat(\"Spermatid Markers\\n\")\nFeaturePlot(sample, features = c('PRM1','PRM2','PRM3','AKAP4','SPATA9','SPAM1'))\n\nSpermatid Markers\n\n\n\n\n\n\n\n\n\nWe also look for some somatic markers\n\ncat(\"Somatic Markers\\n\")\nFeaturePlot(sample, features = c('VIM','CTSL'))\n\nSomatic Markers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how the somatic marker CTSL also ends where we might have another cell type. This happens often as there are markers involved in multiple biological processes! This is why it is important to look at multiple markers and not just one.\n\n\n\nRenaming clusters\nNow, we can choose to rename the clusters to more meaningful names. Simply create a named vector with the old and new names separated by :.\n\n\n\n\n\n\nWarning\n\n\n\nFill out the example vector below yourself!\n\n\n\nnew_names = c(\n    '0'='clusterA',\n    '1'='clusterB',\n    '2'='clusterC')\n\nBelow, you substitute the cluster names with the new names. You can also use the RenameIdents function to rename the clusters.\n\nsample@meta.data$new_clusters = recode(Idents(sample), !!!new_names)\nsample@active.ident = as.factor(sample@meta.data$new_clusters)\nnames(sample@active.ident) = colnames(sample)\n\nNow you are ready to look at your clustering!\n\nDimPlot(sample, reduction = 'umap', label=TRUE, label.size = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNow you are ready to go on with your analysis. You can look at differential expression, cell cycle, and other analyses. Those will come in a second tutorial of the ABC, so keep your eyes on our documentation.\nIn the meanwhile, you can try further tutorials on the Seurat website, or look at the Seurat vignettes for more advanced analyses.\nWe at the ABC also work with single cell analysis regularly, so do not hesitate to ask us for help!"
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html",
    "href": "documentation/2024-06-27-ABC2.html",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "",
    "text": "Conferences in the second half of 2024, introduction to Integrated Development Environments and Virtual Environments.\n \n\n Download Slides \n\n \nThe following tutorials show how to create a package environment and do some basic analysis in both R and python."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#project-creation-and-management-in-rstudio",
    "href": "documentation/2024-06-27-ABC2.html#project-creation-and-management-in-rstudio",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Project creation and management in RStudio",
    "text": "Project creation and management in RStudio\n1. Open RStudioo, go to the menu and select File –&gt; New Project...\n2. Choose New Directory –&gt; New Project (similarly you can create a project in an existing folder). Note that there is a third version including version control: here you can also control the history of all changes in the project files using version control tools (such as Git), but we are not talking about this for now.\n\n3. Name your project and select the directory where you want to save it. Click on use renv for this project. Finally click Create Project\n4. The project is now established. You can see some files are automatically created into the project directory.\n\n\n\n\n\n\nNote\n\n\n\nWhen you want to open your project the next time, you can either: - Open the project by double-clicking on the file in Windows Explorer/OSX Finder (e.g. MyProject.Rproj) - Open RStudio, go to the menu and select File –&gt; Open Project and browser to find and select the project that you want to open - Open RStudio, go to the menu, select File –&gt; Recent Projects, and select the project from the list of most recently opened projects\nSome of the things making it nice to work in projects in RStudio are that when a project is opened:\n\nA new R session (process) is started\nThe .RData file (saved data) in the project’s main directory is loaded\nThe .Rhistory file (command history) in the project’s main directory is loaded\nThe current working directory is set to the project directory\nPackages in the environment are loaded\nAnd other settings are restored to where they were the last time the project was closed\n\n\n\n\nProject structure\nA clean and organised project structure is super important. To ensure this, a good practice is to create subfolders like Data, Scripts, Output, and Docs within your project directory. You can do all this in the RStudio browser (bottom-right corner)."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#using-the-renv-package-to-manage-a-projects-environment",
    "href": "documentation/2024-06-27-ABC2.html#using-the-renv-package-to-manage-a-projects-environment",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Using the renv package to manage a project’s environment",
    "text": "Using the renv package to manage a project’s environment\nThe renv package allows you to create isolated and reproducible environments for your R projects, ensuring that the exact package versions and dependencies are used, ultimately facilitating reproducibility and collaboration. For an extended description, read more here. Dependencies are packages on which others rely on to work.\nKey features:\n\nEach project can have its own set of R packages and versions\nEnsures that the same package versions are used each time the project is run\nManages dependencies by automatically tracking the packages used in your project and their versions\nEnsures the same environment setup when sharing projects with collaborators\n\n\ntracking installed packages with renv\nYou do not need much to track all your packages. Simply create your files with R code and save them inside your project folder. While you code, you will necessarily install some packages which you need to create your program.\nWhen you are done, run in the Rstudio console the program renv::snapshot() - this will update the file renv.lock with a list of all installed packages and dependencies (open the file to see how it looks like). All the packages are installed inside the renv folder and will be loaded from there.\nNote that renv will track only packages that are used in your files with code - other packages which are installed but not used will not be tracked. Pretty smart!\n1. Now, try to install dplyr and ggplot2 using\ninstall.packages(c(\"dplyr\", \"ggplot2\"))\nin the RStudio console.\n2. Snapshot the environment by running\nrenv::snapshot()\nHave a look at the file renv.lock to see how it looks like. It should not show any package beyond renv: the file is updated only with packages used in the scripts you have in the project folder, so we will have to run the snapshot at the end of the tutorial where we use the packages!\n\n\n\n\n\n\nSharing projects\n\n\n\nWhen sharing your project, include the renv.lock and renv directory in the project folder. You can also include just the file renv.lock. Other users can use renv::activate() to activate the environment, and also renv::restore() if they need to install the packages (in case the folder renv is not provided).\nNote that if you provide the folder renv, this must be used in the same operating system. It might not be possible to use the environment folder create, for example, in Linux, on a windows or mac computer."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#kickstart-dplyr-analysis",
    "href": "documentation/2024-06-27-ABC2.html#kickstart-dplyr-analysis",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Kickstart dplyr analysis",
    "text": "Kickstart dplyr analysis\nThe dplyr package is part of the tidyverse, a set of packages all based on consistent and intuitive syntax/grammar for data manipulation, where the fundamental data structure is the data frame. dplyr is one of the most popular packages for data manipulation.\nWe already install the package in our environment and it is ready to use. You can use the following commands in the Console, or create an R script or an R markdown document.\nCreate a small dataset with gene expressions and some patient meta data:\n\n# Load dplyr\nlibrary(dplyr)\n\n# Create the dataframe\ndata &lt;- data.frame(\n  SampleID = c(\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"),\n  Gene1 = c(5.2, 6.3, 4.9, 7.2, 5.8),\n  Gene2 = c(3.8, 2.7, 3.5, 4.1, 3.9),\n  Gene3 = c(7.1, 8.5, 6.8, 9.2, 7.3),\n  Age = c(45, 52, 37, 50, 43),\n  Treatment = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n  Response = c(\"Responder\", \"Non-Responder\", \"Responder\", \"Non-Responder\", \"Responder\")\n)\n\n\ndata\n\n\nA data.frame: 5 × 7\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n\n\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n\n\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n\n\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n\n\n\n\n\nYou can apply many commands very intuitively with dplyr. For example, to select only the gene expressions:\n\nselected_data &lt;- select(data, Gene1, Gene2, Gene3)\n\nselected_data\n\n\nA data.frame: 5 × 3\n\n\nGene1\nGene2\nGene3\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n5.2\n3.8\n7.1\n\n\n6.3\n2.7\n8.5\n\n\n4.9\n3.5\n6.8\n\n\n7.2\n4.1\n9.2\n\n\n5.8\n3.9\n7.3\n\n\n\n\n\nYou can establish any filter based on the columns or combination of them. Filtering only on age of at least 50yo and rate of Gene1 and Gene2 larger than 2 is as below\n\nfiltered_data &lt;- filter(data, Age&gt;50 & Gene1/Gene2&gt;2)\n\nfiltered_data\n\n\nA data.frame: 1 × 7\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\n\n\n\nAdding new columns as a combination of others is also immediate\n\nmutated_data &lt;- mutate(data, GeneTotal = Gene1 + Gene2 + Gene3)\n\nmutated_data\n\n\nA data.frame: 5 × 8\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\nGeneTotal\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n16.1\n\n\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n17.5\n\n\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n15.2\n\n\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n20.5\n\n\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n17.0\n\n\n\n\n\nNow we introduce the pipe symbol %&gt;%. This combines with the dplyr functions such that subsequent operations between pipes reflect the way we enunciate them orally. For example: The dataframe data must be grouped by treatment and for each group summarise the Gene1 by its average. Arrange the result by the average of Gene1 in descending order.\n\n\n\n\n\n\ndataframes and tibbles\n\n\n\nNote how the output is no longer a dataframe, but a tibble. A tibble is a more “modern” version of the dataframe which is used in the tidyverse packages. You cannot really notice much difference when using them. To ensure you are using a tibble , you can always define a dataframe and transform it into a tibble using the command as_tibble. Read more about tibbles at this link.\n\n\n\ngrouped_data &lt;- data %&gt;% \n                group_by(Treatment) %&gt;% \n                summarise(avgGene1 = mean(Gene1)) %&gt;%\n                arrange( desc(avgGene1) )\n\ngrouped_data\n\n\nA tibble: 2 × 2\n\n\nTreatment\navgGene1\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nB\n6.75\n\n\nA\n5.30\n\n\n\n\n\nTo save your table in the Output folder as csv file:\n\nwrite.csv(grouped_data, \"Output/grouped.csv\")\n\nNow, if you ran all the commands in your console, create a new R script (File -&gt; New -&gt; R Script) and paste all the code used until now, then save it. If you instead used a Script or Markdown, save it. Move the script/markdown file into the Code folder, so you keep the files organized! Now run again\nrenv::snapshot()\nYou should be requested for permission to write some packages into renv.lock. Accept and look into the file, which not should be different and including new package names.\n\n\n\n\n\n\nwrap up\n\n\n\nNow you know how to manage a project in RStudio and the packages you need. dplyr and the other packages pivoting around tidyverse have a plethora of useful functionalities. A good place to start from is to use a table you need for your own work, and try out various things you can do. Some good resources for training are available as links below:\n\nA chapter of R for data science from the data carpentry. Go through it and look at other chapters as well, they are very instructive!\nA cheatsheet for all dplyr manipulation and for other important tidyverse packages + Rstudio. Keep some at your desk!"
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#install-anaconda-and-create-an-environment",
    "href": "documentation/2024-06-27-ABC2.html#install-anaconda-and-create-an-environment",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Install Anaconda and create an environment",
    "text": "Install Anaconda and create an environment\n1. Download Anaconda from the download page. Uso also the Sign In button of the webpage to create an account: this will enable you to use an AI-chat showing up while you code, and which can help you doing a lot of things.\n\n\n\n\n\n\nNote\n\n\n\nThis guide has been made with Windows, and works similarly with MacOS. For Linux, you will download a file with .sh extension, which you can execute in the command line with bash installer.sh.\n\n\n2. Once Anaconda is installed, open Anaconda Navigator. Log into Anaconda Cloud from the software, and eventually update Anaconda Navigator if asked.\n3. Your initial window would look similarly as below. What you can see is a suite of softwares. Some of those are installed and they are found in the anaconda environment called base (root), as shown in the red circle. It is good advice not to modify the base environment, because Anaconda itself is installed into it!\n\n4. We create a new environment with only few needed packages for our tutorial. Click on Environments on the toolbar (red circle). You can see the base (root) environment and a long list of packages it contains. Click on Create (blue circle).\n\n5. In the appearing window choose a name for the environment and select Python. Then click on Create. It takes a bit of time to create the environment.\n\n6. Select the environment. It will load a few packages (just Python and its essential dependencies), but we want to install new ones. Open the filtering menu (green circle above) and choose All to view all existing packages. Select the following packages: pandas, numpy, jupyterlab, anaconda-toolbox, seaborn and click Apply. A list of dependencies will be shown, and you have to accept that to continue. Wait for the installations to go through.\n7. Now select the environment again. There are much more packages installed and ready to use. Click on Home in the toolbar: you will see the softwares installed in your environment (choose Installed Application from the dropdown menu to see only the installed ones).\n\n\n\n\n\n\nwrap up\n\n\n\nYou can create various environments to keep specific versions of packages constant in separate data science projects. This ensures a high degree of reproducibility in your projects.\nAn environment can include the softwares needed to code (RStudio, jupyterLab are the most famous, but also others can be installed).\nWe will use the created environment to run some basic commands in python."
  },
  {
    "objectID": "documentation/2024-06-27-ABC2.html#python-basics",
    "href": "documentation/2024-06-27-ABC2.html#python-basics",
    "title": "ABC.2: RStudio projects and Anaconda+Python",
    "section": "Python basics",
    "text": "Python basics\nHere we look at the python basics: variables, arrays and dataframes. We will immediately work with two packages: numpy and pandas, which make the use of arrays and dataframes very flexible.\n\nJupyterLab\nLaunch jupyterlab from the Anaconda Home, using the environment ABC2. You should be able to see Jupyterlab opened in your internet browser. It will look similar to the one below: it has a file browser, can show the opened files on different tabs, and has a coding window where the opened file is shown. When you first open jupyterlab you will instead see a Launcher, which gives you a choice for all the available things to do, usually Notebooks, Console, or Other, with related available languages.\n\nYou do not create projects in JupyterLab, you simply create folders instead.\n\nUse the browser to create a folder wherever you prefere, and organize it into subfolders: Code, Output, Data, Scripts. You can use the small button above the browser to create a folder, or simply the right-click options.\nAgain with a right click, create a notebook inside the Code folder\nWhen asked to choose a kernel, select python 3. A kernel includes the programming language and its packages as we installed them into the environment using Anaconda.\n{fig-align=“center”, width=300px}\n\nA notebook looks like this:\n\nThe gray area is a code cell, where you can write code. Select the left border out of a cell to\n\ntransform it into a Markdown cell where you can write text: key M of your keyboard\nadd a cell below: key B\nrun the cell and add one below: keys Shift+Enter\n\n\n\n\n\n\n\nTip\n\n\n\nA notebook looks a lot like a text file. When you open it, it will show all results and images from your code! You can share it to anyone without the need to run the code again for the recipient, if it is needed to simply show some results.\nThe webpage you are reading now has been created with a notebook!\n\n\nLet’s turn things into practice now. Transform the first cell into markdown (key M) and write a title with\n ## ABC2 is cool\nand press Shift+Enter. You will get a new cell below, and the text will be formatted from Markdown.\nNow we write some code in the new cell and press Shift+Enter to execute:\n\nimport numpy as np\nimport pandas as pd\n\nprint(\"Hello ABC\")\nx = np.arange(1,10)\ny = x**2\nprint(\"x\")\nprint(x)\nprint(\"y\")\nprint(y)\n\nHello ABC\nx\n[1 2 3 4 5 6 7 8 9]\ny\n[ 1  4  9 16 25 36 49 64 81]\n\n\nThings should look as below:\n\nThe little number on the left shows how many code steps you have been running so far. The code imports relevant libraries and shortens their name with np and pd, which makes coding easier and compact. Then we use the library np to define an array of numbers from 1 to 10, and we square this array assigning it to y. Finally we print the two arrays.\nAll variables are assigned with the = symbol and you can do all arithmetic operations:\n\nprint(2+3)\nprint(9*2)\nprint(5**2)\n\n5\n18\n25\n\n\n\n\nPandas dataframes\nwe procees looking into a small dataframe. Let’s create one using the pandas library\n\n# Create the dataframe\ndata = pd.DataFrame({\n    'SampleID': ['S1', 'S2', 'S3', 'S4', 'S5'],\n    'Gene1': [5.2, 6.3, 4.9, 7.2, 5.8],\n    'Gene2': [3.8, 2.7, 3.5, 4.1, 3.9],\n    'Gene3': [7.1, 8.5, 6.8, 9.2, 7.3],\n    'Age': [45, 52, 37, 50, 43],\n    'Treatment': ['A', 'B', 'A', 'B', 'A'],\n    'Response': ['Responder', 'Non-Responder', 'Responder', 'Non-Responder', 'Responder']\n})\n\n# View the dataframe\ndata\n\n\n\n\n\n\n\n\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n\n\n0\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n\n\n1\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\n2\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n\n\n3\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n\n\n4\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n\n\n\n\n\n\n\nYou can plot variables from the dataframe with the package seaborn. For example\n\nimport seaborn as sns\n\nsns.scatterplot(data=data,\n                x=\"Gene1\",\n                y=\"Gene2\",\n                hue=\"Response\",\n                size=\"Age\"\n               )\n\n\n\n\n\n\n\n\nIn the command above we used some options beyond x and y. Can you see what they match in the plot?\n\n\n\n\n\n\nTip\n\n\n\nThe seaborn package webpage has great examples for any kind of plot you desire!\n\n\nThere are lots of summary statistics already implemented in python. Below we calculate mean, median and standard deviation for the column Gene1 of the data frame and then we print them.\n\nx = data.Gene1\nmeanG1 = np.mean(x)\nmedianG1 = np.median(x)\nsdG1 = np.std(x)\n\nprint(\"mean, median and sd:\")\n[meanG1, medianG1, sdG1]\n\nmean, median and sd:\n\n\n[np.float64(5.88), np.float64(5.8), np.float64(0.8182909018191513)]\n\n\nThis was neat! Can you try to calculate the cumulative sum of the difference between Gene1 and Gene2?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answer is\nx = data.Gene1 - data.Gene2\ncsum = np.cumsum(x)\nprint(\"cumulative sum of Gene1 - Gene2:\")\nprint(csum)\n\n\n\n\n\nFunctions\nAlthough python and the packages you can find have almost everything you will need, sometimes you might need to define your own function. The syntax to do it is very easy: you define a function name, which then you will be able to use it. Below, there is a function taking an argument (arg1) and multiplying it by 5. The output needs to be explicit through the return() function.\n\ndef myFunction(arg1):\n    res = arg1 * 5\n    return(res)\n\nSuch a function works if the argument is a number, but also if it is an array!\n\nprint(\"with a number only\")\nprint( myFunction(5) )\nprint(\"with an array\")\nprint( myFunction(data.Gene1) )\n\nwith a number only\n25\nwith an array\n0    26.0\n1    31.5\n2    24.5\n3    36.0\n4    29.0\nName: Gene1, dtype: float64\n\n\nTry to make a function that takes three vectors, plots the first against the sum of the second and third, and returns the sum of all three vectors. Use the plot command we applied previously for help.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answer is\ndef simpleSumPlot(arg1, arg2, arg3):\n    arg23 = arg2 + arg3\n    arg123 = arg1 + arg2 + arg3\n\n    #plotting\n    fig = sns.scatterplot(x = arg1, \n                    y = arg2)\n    fig.set_title(\"Plot with my own function\")\n    fig\n\n    return(arg123)\nNow you can try this on vectors of the same length. We can use the ones in our data frame!\n\nsimpleSumPlot(arg1 = data.Gene1,\n             arg2 = data.Gene2,\n             arg3 = data.Gene3)\n\n\n\n\n\nRead and write files\nSave a dataframe with the command .to_csv\n\ndata.to_csv('../Ouput/data.csv')\n\nAnd read it again using\n\ndata2 = pd.read_csv('../Ouput/data.csv')\n\n\ndata2\n\n\n\n\n\n\n\n\nUnnamed: 0\nSampleID\nGene1\nGene2\nGene3\nAge\nTreatment\nResponse\n\n\n\n\n0\n0\nS1\n5.2\n3.8\n7.1\n45\nA\nResponder\n\n\n1\n1\nS2\n6.3\n2.7\n8.5\n52\nB\nNon-Responder\n\n\n2\n2\nS3\n4.9\n3.5\n6.8\n37\nA\nResponder\n\n\n3\n3\nS4\n7.2\n4.1\n9.2\n50\nB\nNon-Responder\n\n\n4\n4\nS5\n5.8\n3.9\n7.3\n43\nA\nResponder\n\n\n\n\n\n\n\n\n\n\n\n\n\nwrap up\n\n\n\nYou learned how to use jupyterlab notebooks and how to do basic operations and plots with python."
  },
  {
    "objectID": "documentation/2025-01-30-ABC12.html",
    "href": "documentation/2025-01-30-ABC12.html",
    "title": "ABC.12: Pixi and Desktop on genomeDK, R-clusterProfiler analysis",
    "section": "",
    "text": "Today we did not show slides for the topic presentation, but we went through the tutorial below."
  },
  {
    "objectID": "documentation/2025-01-30-ABC12.html#over-representation-analysis-ora",
    "href": "documentation/2025-01-30-ABC12.html#over-representation-analysis-ora",
    "title": "ABC.12: Pixi and Desktop on genomeDK, R-clusterProfiler analysis",
    "section": "Over Representation Analysis (ORA)",
    "text": "Over Representation Analysis (ORA)\nORA answers the question: “Which pathways are overrepresented in my list of significant genes?”\nTo answer this, we need a list of background genes (also referred to as the universe). This is important because overrepresentation is always relative, i.e. overrepresented compared to what? Therefore, we essentially create a contingency table to determine the significance of the pathway’s representation in the gene list.e. For example:\n\nmytable &lt;- data.frame(not_interesting_genes=c(2613, 15310), interesting_genes=c(28, 29))\nrow.names(mytable) &lt;- c(\"In pathway\", \"not in pathway\")\nmytable\n\n\nA data.frame: 2 × 2\n\n\n\nnot_interesting_genes\ninteresting_genes\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nIn pathway\n2613\n28\n\n\nnot in pathway\n15310\n29\n\n\n\n\n\n\nStatistical Test\nTo determine over-representation, we use Fisher’s Exact Test, which is a statistical test commonly applied in contingency table analysis. The test is represented by the following formula:\n\\[ p = 1 - \\sum_{i=0}^{k-1} \\frac{\\binom{M}{i} \\binom{N-M}{n-i}}{\\binom{N}{n}} \\]\nWhere: - ( M ) is the total number of genes in the pathway, - ( N ) is the total number of genes in the universe, - ( n ) is the number of significant genes (those of interest), - ( k ) is the number of significant genes in the pathway.\n\nfisher.test(mytable, alternative = \"greater\")\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  mytable\np-value = 1\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.110242      Inf\nsample estimates:\nodds ratio \n 0.1767937 \n\n\nSince we are testing multiple pathways we should correct for multiple testing.\n\ngene_data &lt;- data.frame(\n  Gene_Name = paste0(\"Gene\", 1:10),\n  LogFC = c(2.5, -1.2, 0.8, -2.3, 3.1, -0.5, 1.8, -1.0, 2.0, -3.0),\n  P_value = c(0.001, 0.04, 0.2, 0.005, 0.0001, 0.3, 0.02, 0.06, 0.0005, 0.00001),\n  Regulation = c(\"UP\", \"DOWN\", \"UP\", \"DOWN\", \"UP\", \"DOWN\", \"UP\", \"DOWN\", \"UP\", \"DOWN\")\n)\n\nprint(gene_data)\n\n   Gene_Name LogFC P_value Regulation\n1      Gene1   2.5   1e-03         UP\n2      Gene2  -1.2   4e-02       DOWN\n3      Gene3   0.8   2e-01         UP\n4      Gene4  -2.3   5e-03       DOWN\n5      Gene5   3.1   1e-04         UP\n6      Gene6  -0.5   3e-01       DOWN\n7      Gene7   1.8   2e-02         UP\n8      Gene8  -1.0   6e-02       DOWN\n9      Gene9   2.0   5e-04         UP\n10    Gene10  -3.0   1e-05       DOWN\n\n\n\n\nORA has some significant drawbacks.\n\nMainly it is highly dependent on what we (subjectively) will deem to be interesting genes. Usually that would be genes that are differentially expressed.\nAdditionally, it is highly dependent on the background genes. If there are lot of non annotated genes in the pathways then the power reduces substantially and also creates very biased results depending on which pathway databases you use.\nIt does not take into account the combination of the p value and the logfc. What if a gene has a fold change of 5 but a p value of 0.051?"
  },
  {
    "objectID": "documentation/2025-01-30-ABC12.html#gene-set-enrichment-analysis",
    "href": "documentation/2025-01-30-ABC12.html#gene-set-enrichment-analysis",
    "title": "ABC.12: Pixi and Desktop on genomeDK, R-clusterProfiler analysis",
    "section": "Gene Set Enrichment Analysis",
    "text": "Gene Set Enrichment Analysis\nThese issues are addressed by Gene Set Enrichment analysis. GSEA answers the question “these changes I see between treatment and control are similar to which known list of changes”. The big difference now is that instead of choosing a list of interesting genes we consider all the genes in the output of our degs analysis and we rank them!\nThe core idea of Gene Set Enrichment Analysis (GSEA) is to assess whether the genes in a predefined gene set (e.g., a pathway) are enriched at the top or bottom of a ranked gene list.\n\nGene Ranking: Genes are ranked by a statistic, such as the log-fold change or p-value, representing their differential expression.\nEnrichment Score (ES): The algorithm traverses the ranked gene list and computes a cumulative running score. When encountering a gene in the pathway, the score is increased proportionally to its rank. For genes not in the pathway, the score is decreased. The enrichment score (ES) is the maximum deviation from zero observed during this traversal.\nSignificance Testing: The observed ES is compared to a null distribution of ES values generated by permuting the gene labels. The significance is quantified as a p-value, and adjustments for multiple testing are applied.\nNormalization: The ES is normalized (NES) to account for variations in gene set size, making results comparable across pathways.\n\nBefore we go any further lets quickly get a degs list using the airway package and deseq2\n\ndata(airway)\ncts  &lt;- assay(airway) # The counts matrix\nmetadata &lt;- as.data.frame(colData(airway)) # Our metadata\nmetadata$condition &lt;- factor(metadata$dex) # factor the dex col in condition\ndds &lt;- DESeqDataSetFromMatrix(\n  countData = cts,\n  colData = metadata,\n  design = ~ condition\n)\ndds &lt;- DESeq(dds)\nres &lt;- results(dds)\nresdf &lt;- as.data.frame(res)\nhead(res)\n\nestimating size factors\n\nestimating dispersions\n\ngene-wise dispersion estimates\n\nmean-dispersion relationship\n\nfinal dispersion estimates\n\nfitting model and testing\n\n\n\nlog2 fold change (MLE): condition untrt vs trt \nWald test p-value: condition untrt vs trt \nDataFrame with 6 rows and 6 columns\n                  baseMean log2FoldChange     lfcSE      stat    pvalue\n                 &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nENSG00000000003 708.602170      0.3788470  0.173141  2.188082 0.0286636\nENSG00000000005   0.000000             NA        NA        NA        NA\nENSG00000000419 520.297901     -0.2037604  0.100599 -2.025478 0.0428183\nENSG00000000457 237.163037     -0.0340428  0.126279 -0.269584 0.7874802\nENSG00000000460  57.932633      0.1171786  0.301237  0.388992 0.6972820\nENSG00000000938   0.318098      1.7245505  3.493633  0.493627 0.6215698\n                     padj\n                &lt;numeric&gt;\nENSG00000000003  0.139308\nENSG00000000005        NA\nENSG00000000419  0.183359\nENSG00000000457  0.930572\nENSG00000000460  0.895441\nENSG00000000938        NA\n\n\nNow we can rank the list. There is not on a correct way to rank the gene lists but we would like the ranking to represent both the significance and the fold change. One suggested formula is:\n\\[\n\\text{Ranking Score} = \\log_{2}(\\text{Fold Change}) \\times -\\log_{10}(\\text{P-value})\n\\]\nAdditionally we can order them by the stat column of the deseq2 output which is the wald statistic. This creates a list where over expressed and significant genes are at the top, under expressed and significant genes in the bottom and combinations of not significant genes in the middle.\n\nresOrdered &lt;- res[order(res$stat),]"
  },
  {
    "objectID": "documentation/2025-01-30-ABC12.html#plots",
    "href": "documentation/2025-01-30-ABC12.html#plots",
    "title": "ABC.12: Pixi and Desktop on genomeDK, R-clusterProfiler analysis",
    "section": "Plots",
    "text": "Plots\nOne of the great things about clusterProfiler is that you can very easily create plots with the enrichplot library and they are very well integrated with ggplot. Here we will show some of them as there are many. You can see more here: https://yulab-smu.top/biomedical-knowledge-mining-book/enrichplot.html\n\nConverting Gene Lists\nWhen working with gene lists, it’s often necessary to convert between different types of gene identifiers. For instance, you may need to convert between ENTREZID, ENSEMBL, and SYMBOL identifiers, which are commonly used in gene annotation databases. The bitr() function from the clusterProfiler package is a useful tool for this task.\n\nbitr(gene, fromType = \"ENTREZID\",\n        toType = c(\"ENSEMBL\", \"SYMBOL\"),\n        OrgDb = org.Hs.eg.db)\n\n'select()' returned 1:many mapping between keys and columns\n\nWarning message in bitr(gene, fromType = \"ENTREZID\", toType = c(\"ENSEMBL\", \"SYMBOL\"), :\n“0.48% of input gene IDs are fail to map...”\n\n\n\nA data.frame: 242 × 3\n\n\n\nENTREZID\nENSEMBL\nSYMBOL\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n4312\nENSG00000196611\nMMP1\n\n\n2\n8318\nENSG00000093009\nCDC45\n\n\n3\n10874\nENSG00000109255\nNMU\n\n\n4\n55143\nENSG00000134690\nCDCA8\n\n\n5\n55388\nENSG00000065328\nMCM10\n\n\n6\n991\nENSG00000117399\nCDC20\n\n\n7\n6280\nENSG00000163220\nS100A9\n\n\n8\n2305\nENSG00000111206\nFOXM1\n\n\n9\n9493\nENSG00000137807\nKIF23\n\n\n10\n1062\nENSG00000138778\nCENPE\n\n\n11\n3868\nENSG00000186832\nKRT16\n\n\n12\n4605\nENSG00000101057\nMYBL2\n\n\n13\n9833\nENSG00000165304\nMELK\n\n\n14\n9133\nENSG00000157456\nCCNB2\n\n\n15\n6279\nENSG00000143546\nS100A8\n\n\n16\n10403\nENSG00000080986\nNDC80\n\n\n17\n8685\nENSG00000019169\nMARCO\n\n\n18\n597\nENSG00000140379\nBCL2A1\n\n\n19\n7153\nENSG00000131747\nTOP2A\n\n\n20\n23397\nENSG00000121152\nNCAPH\n\n\n21\n6278\nENSG00000143556\nS100A7\n\n\n22\n79733\nENSG00000129173\nE2F8\n\n\n23\n259266\nENSG00000066279\nASPM\n\n\n24\n1381\nENSG00000166426\nCRABP1\n\n\n25\n3627\nENSG00000169245\nCXCL10\n\n\n26\n27074\nENSG00000078081\nLAMP3\n\n\n27\n6241\nENSG00000171848\nRRM2\n\n\n28\n55165\nENSG00000138180\nCEP55\n\n\n29\n9787\nENSG00000126787\nDLGAP5\n\n\n30\n7368\nENSG00000174607\nUGT8\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n214\n64499\nENSG00000197253\nTPSB2\n\n\n215\n54829\nENSG00000106819\nASPN\n\n\n216\n54829\nENSG00000292224\nASPN\n\n\n217\n1524\nENSG00000168329\nCX3CR1\n\n\n218\n10234\nENSG00000128606\nLRRC17\n\n\n219\n1580\nENSG00000142973\nCYP4B1\n\n\n220\n10647\nENSG00000124935\nSCGB1D2\n\n\n221\n25893\nENSG00000162722\nTRIM58\n\n\n222\n24141\nENSG00000125869\nLAMP5\n\n\n223\n10351\nENSG00000141338\nABCA8\n\n\n224\n2330\nENSG00000131781\nFMO5\n\n\n225\n5304\nENSG00000159763\nPIP\n\n\n226\n79846\nENSG00000105792\nCFAP69\n\n\n227\n8614\nENSG00000113739\nSTC2\n\n\n228\n2625\nENSG00000107485\nGATA3\n\n\n229\n7021\nENSG00000008196\nTFAP2B\n\n\n230\n7802\nENSG00000163879\nDNALI1\n\n\n231\n79689\nENSG00000127954\nSTEAP4\n\n\n232\n11122\nENSG00000196090\nPTPRT\n\n\n233\n55351\nENSG00000152953\nSTK32B\n\n\n234\n9\nENSG00000171428\nNAT1\n\n\n235\n4239\nENSG00000166482\nMFAP4\n\n\n236\n5241\nENSG00000082175\nPGR\n\n\n237\n10551\nENSG00000106541\nAGR2\n\n\n238\n10974\nENSG00000148671\nADIRF\n\n\n239\n79838\nENSG00000103534\nTMC5\n\n\n240\n79901\nENSG00000071967\nCYBRD1\n\n\n241\n57758\nENSG00000175356\nSCUBE2\n\n\n242\n4969\nENSG00000106809\nOGN\n\n\n243\n4969\nENSG00000291350\nOGN\n\n\n\n\n\nLets see from our examples:\n\nmutate(ego, qscore = -log(p.adjust, base=10)) %&gt;% \n    barplot(x=\"qscore\") + ggtitle(\"Bar plot from the ORA  above\")\n\n\n\n\n\n\n\n\n\nedox &lt;- setReadable(ego, 'org.Hs.eg.db', 'ENTREZID') # Convert to symbol\np &lt;- cnetplot(edox, foldChange=geneList, circular = TRUE, colorEdge = TRUE)\np\n\n\n\n\n\n\n\n\nA very informative plot is a heatplot which you can generate like this:\n\np1 &lt;- heatplot(edox, showCategory=5) # Show category dictates show many genesets you see\np2 &lt;- heatplot(edox, foldChange=geneList, showCategory=5)\ncowplot::plot_grid(p1, p2, ncol=1, labels=LETTERS[1:2])\n\n\n\n\n\n\n\n\nOne of the most useful ones I find to be the upset plot. For over-representation analysis, upsetplot will calculate the overlaps among different gene sets and for GSEA result, it will plot the fold change distributions of different categories.\n\nupsetplot(ego3) \n\n\n\n\n\n\n\n\n\noptions(repr.plot.height=20)\nridgeplot(ego3)\n\nPicking joint bandwidth of 0.223\n\n\n\n\n\n\n\n\n\n\nLastly we have: Running score and preranked list are traditional methods for visualizing the results of Gene Set Enrichment Analysis (GSEA). These visualizations help to understand how a particular gene set is enriched along the ordered gene list, as well as how the enrichment score changes as you progress through the gene list.\n\np1 &lt;- gseaplot(ego3, geneSetID = 1, by = \"runningScore\", title = ego3$Description[1])\np2 &lt;- gseaplot(ego3, geneSetID = 1, by = \"preranked\", title = ego3$Description[1])\n\n\noptions(repr.plot.height=10)\ncowplot::plot_grid(p1, p2, ncol=1, labels=LETTERS[1:3])\n\n\n\n\n\n\n\n\n\n\nRunning Score Plot Interpretation\nThe running score plot illustrates how the enrichment score (ES) accumulates as you move through the ranked gene list. The key features to observe are:\n\nSharp Peaks: A sharp rise indicates that a cluster of genes from the pathway is highly ranked, contributing strongly to enrichment.\nPeak Position: The x-axis location of the peak corresponds to where the pathway’s genes are most enriched within the ranked list.\nRunning Score Return: If the score drops back down, it suggests that the pathway genes are not uniformly distributed across the gene list."
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "Events calendar",
    "section": "",
    "text": "The ABC will routinely post new meetings and related workshops/courses of interest here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABC.14: Accessible Bioinformatics Cafe\n\n\n\nCafe\n\n\nCoding\n\n\nBioinformatics\n\n\nSCP\n\n\nMass Spectrometry\n\n\nSingle cell proteomics\n\n\n\nSingle cell proteomics analysis\n\n\n\nMar 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop: Advanced use of GenomeDK\n\n\n\nWorkshop\n\n\nGenomeDK\n\n\nAdvanced\n\n\n\nWhat more you can get out of GenomeDK\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop: Pipelines on GenomeDK\n\n\n\nWorkshop\n\n\nGenomeDK\n\n\nPipelines\n\n\n\nCreate complex and reproducible pipelines on genomeDK\n\n\n\nJun 4, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Event title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nEvent title\n\n\nDates\n\n\nLocation\n\n\nOrganizers\n\n\n\n\n\n\nABC.13: Accessible Bioinformatics Cafe\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility, DDSA\n\n\n\n\nWorkshop: Introduction to GenomeDK\n\n\n \n\n\n \n\n\nHDS Sandbox, GenomeDK\n\n\n\n\nABC.12: Accessible Bioinformatics Cafe\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility, DDSA\n\n\n\n\nABC.11: Accessible Bioinformatics Cafe\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility, DDSA\n\n\n\n\nABC.10: Accessible Bioinformatics Cafe\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.9: Accessible Bioinformatics Cafe\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.8: Nygen workshop\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility, CellX, Nygen\n\n\n\n\nABC.7: Seventh ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nUse GenomeDK workshop\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility, GenomeDK admin\n\n\n\n\nABC.6: Sixth ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.5: Fifth ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.4: Fourth ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.3: Third ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.2: Second ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\nABC.1: First ABC session\n\n\n \n\n\n \n\n\nHDS Sandbox, Bioinf Core facility\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "binfResources/Databases.html",
    "href": "binfResources/Databases.html",
    "title": "Archive of OMICS databases with open datasets",
    "section": "",
    "text": "In this notebook, we provide a searchable table of periodically updated and daily tested OMICS databases with open access datasets. The table is based on multiple online sources and inputs from our colleagues and ABC participants. The links are tested automatically every day at midnight, and an error in opening the related webpage is signaled in the table. When we verify the malfunctioning of the link, we remove the database from the list after a reasonable grace period.\n\n\n\n\n\n\nNote\n\n\n\nFor any contribution or suggestion, please contact us at abcafe [at] au.dk, or open an issue on the Github repository of this webpage. Any input is considered, especially if you produced your own database and want to share it with the community!!!\n\n\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.5\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n\nNameDescriptionOrganismOrganFormatTypeLinkTestedDate_tested\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.5 from the init_notebook_mode cell...\n(need help?)"
  },
  {
    "objectID": "binfResources/R.html",
    "href": "binfResources/R.html",
    "title": "Useful tips for R",
    "section": "",
    "text": "Useful tips for R\nNothing yet\n\n\n\n\n\n\nthis is my tip\n\n\n\nhello I am Marie"
  },
  {
    "objectID": "news/past/2024-11-07-session8.html",
    "href": "news/past/2024-11-07-session8.html",
    "title": "ABC.8: Nygen workshop",
    "section": "",
    "text": "The eighth session of the ABC (Accessible Bioinformatics Cafe) will be on november 7th, at 13:00-15:00 in building 1253-317\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-11-07-session8.html#agenda",
    "href": "news/past/2024-11-07-session8.html#agenda",
    "title": "ABC.8: Nygen workshop",
    "section": "Agenda",
    "text": "Agenda\nFor this ABC we host a workshop by Nygen Analytics, which will give a live demo of a downstream analysis with their interactive tool for single cell datasets.\nThe tool has a free tier account with up to 5 single cell datasets uploaded."
  },
  {
    "objectID": "news/past/2024-11-07-session8.html#signing-up",
    "href": "news/past/2024-11-07-session8.html#signing-up",
    "title": "ABC.8: Nygen workshop",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up to ensure there is enough cake (and you are pretty sure you will show up): click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2025-01-16-session11.html",
    "href": "news/past/2025-01-16-session11.html",
    "title": "ABC.11: Accessible Bioinformatics Cafe",
    "section": "",
    "text": "The 11th session of the ABC (Accessible Bioinformatics Cafe) will be on january 16th, at 13:00-15:00 in building 1231-114\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2025-01-16-session11.html#agenda",
    "href": "news/past/2025-01-16-session11.html#agenda",
    "title": "ABC.11: Accessible Bioinformatics Cafe",
    "section": "Agenda",
    "text": "Agenda\nFor this ABC Manuel will show how to run locally a language model on your laptop, and Marie will propose a tutorial on bulkRNA data"
  },
  {
    "objectID": "news/past/2025-01-16-session11.html#signing-up",
    "href": "news/past/2025-01-16-session11.html#signing-up",
    "title": "ABC.11: Accessible Bioinformatics Cafe",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up to ensure there is enough cake (and you are pretty sure you will show up): click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2025-02-27-session13.html",
    "href": "news/past/2025-02-27-session13.html",
    "title": "ABC.13: Accessible Bioinformatics Cafe",
    "section": "",
    "text": "The 13th session of the ABC (Accessible Bioinformatics Cafe) will be on february 27th, at 13:00-15:00 in building 1231-114\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2025-02-27-session13.html#signing-up",
    "href": "news/past/2025-02-27-session13.html#signing-up",
    "title": "ABC.13: Accessible Bioinformatics Cafe",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up to ensure there is enough cake (and you are pretty sure you will show up): click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2024-09-03-session4.html",
    "href": "news/past/2024-09-03-session4.html",
    "title": "ABC.4: Fourth ABC session",
    "section": "",
    "text": "The fourth session of the ABC (Accessible Bioinformatics Cafe) will be on september 3rd, at 13:00 in building 1231 room 320 (by the lake, see below).\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-09-03-session4.html#agenda",
    "href": "news/past/2024-09-03-session4.html#agenda",
    "title": "ABC.4: Fourth ABC session",
    "section": "Agenda",
    "text": "Agenda\nWe have a very minimalistic agenda consisting of\n\nWhat’s new\nAccessible topic presentation\nTutorials and Open Coding\n\nThe tutorials we propose are of increasing difficulty (and documented at each meeting. This time we have\n\nintroduction to bash for bioinformatics\n\nYou can also bring your own code and issues and get help from us."
  },
  {
    "objectID": "news/past/2024-09-03-session4.html#signing-up",
    "href": "news/past/2024-09-03-session4.html#signing-up",
    "title": "ABC.4: Fourth ABC session",
    "section": "Signing up",
    "text": "Signing up\nNo signup is really necessary, but if you just want to ensure there is enough cake (and you are pretty sure you will show up), click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign up for cake"
  },
  {
    "objectID": "news/past/2024-12-12-session10.html",
    "href": "news/past/2024-12-12-session10.html",
    "title": "ABC.10: Accessible Bioinformatics Cafe",
    "section": "",
    "text": "The 10th session of the ABC (Accessible Bioinformatics Cafe) will be on december 12th, at 13:00-15:00 in building 1231-114\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/past/2024-12-12-session10.html#agenda",
    "href": "news/past/2024-12-12-session10.html#agenda",
    "title": "ABC.10: Accessible Bioinformatics Cafe",
    "section": "Agenda",
    "text": "Agenda\nFor this ABC Marie will show in detail how to import and preprocess bulkRNA sequencing data."
  },
  {
    "objectID": "news/past/2024-12-12-session10.html#signing-up",
    "href": "news/past/2024-12-12-session10.html#signing-up",
    "title": "ABC.10: Accessible Bioinformatics Cafe",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up to ensure there is enough cake (and you are pretty sure you will show up): click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2025-02-26.html",
    "href": "news/past/2025-02-26.html",
    "title": "Workshop: Introduction to GenomeDK",
    "section": "",
    "text": "Are you a student or academic, current or future user of the GenomeDK high-performance computing (HPC) system? Or are you just curious about HPC and how you can utilize it in your work? Do you need to learn to use GenomeDK, or wish to become a better and more organized user?\nThen join us in this series of GenomeDK workshops where we guide you from a general introduction to advanced usage. Each workshop targets increasing difficulty levels, so you can join the sessions that fit you best. The first workshop is an introductory one.\nThis workshop will be hosted by Samuele Soraggi (BiRC, MBG) and Dan Søndergaard (GenomeDK)."
  },
  {
    "objectID": "news/past/2025-02-26.html#signing-up",
    "href": "news/past/2025-02-26.html#signing-up",
    "title": "Workshop: Introduction to GenomeDK",
    "section": "Signing up",
    "text": "Signing up\nEach workshop is scheduled for four hours, including a 45 min. lunch break. There will be cake, coffee, and tea for attendants, so please remember to sign up! There are 30 seats available and a waiting list.\n \n\n Sign Up!"
  },
  {
    "objectID": "news/past/2024-10-10-usegenomedk.html",
    "href": "news/past/2024-10-10-usegenomedk.html",
    "title": "Use GenomeDK workshop",
    "section": "",
    "text": "You are invited to an introductory workshop on october 10th, at 11:00 in building 1540-K26 (Biology department)."
  },
  {
    "objectID": "news/past/2024-10-10-usegenomedk.html#agenda",
    "href": "news/past/2024-10-10-usegenomedk.html#agenda",
    "title": "Use GenomeDK workshop",
    "section": "Agenda",
    "text": "Agenda\nThe topics for the workshop are all at introductory level, such as\n\nlogin and account setup\nfile system\ncommand line intro\nsoftware management\nprojects on GenomeDK\nfile transfer\njobs\n\nThere are no prerequisites apart from having a working account on GenomeDK with a complete 2-Factor authentication. Also, bring your laptop, so we can try out live some commands and exercises on the cluster.\nThere will be a lunch break 12-13, with lunch not included in the workshop. We will serve coffee and cake.\nThere is time for questions and problem-solving, so come with your issues if you are already a GDK user."
  },
  {
    "objectID": "news/past/2024-10-10-usegenomedk.html#signing-up",
    "href": "news/past/2024-10-10-usegenomedk.html#signing-up",
    "title": "Use GenomeDK workshop",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up through this very short online form:\n \n\n Sign up"
  },
  {
    "objectID": "news/upcoming/2025-04-10.html",
    "href": "news/upcoming/2025-04-10.html",
    "title": "Workshop: Advanced use of GenomeDK",
    "section": "",
    "text": "Are you a student or academic, current or future user of the GenomeDK high-performance computing (HPC) system? Or are you just curious about HPC and how you can utilize it in your work? Do you need to learn to use GenomeDK, or wish to become a better and more organized user?\nThen join us in this series of GenomeDK workshops where we guide you from a general introduction to advanced usage. Each workshop targets increasing difficulty levels, so you can join the sessions that fit you best. The second workshop is for more advanced usage of GenomeDK.\nThis workshop will be hosted by Samuele Soraggi (BiRC, MBG) and Dan Søndergaard (GenomeDK)."
  },
  {
    "objectID": "news/upcoming/2025-04-10.html#signing-up",
    "href": "news/upcoming/2025-04-10.html#signing-up",
    "title": "Workshop: Advanced use of GenomeDK",
    "section": "Signing up",
    "text": "Signing up\nEach workshop is scheduled for four hours, including a 45 min. lunch break. There will be cake, coffee, and tea for attendants, so please remember to sign up! There are 30 seats available and a waiting list.\n \n\n Sign Up!"
  },
  {
    "objectID": "news/upcoming/2025-03-13-session14.html",
    "href": "news/upcoming/2025-03-13-session14.html",
    "title": "ABC.14: Accessible Bioinformatics Cafe",
    "section": "",
    "text": "The 14th session of the ABC (Accessible Bioinformatics Cafe) will be on march 13th, at 13:00-15:00 in building 1231-114\nEveryone is welcome to join independently of coding skills and level."
  },
  {
    "objectID": "news/upcoming/2025-03-13-session14.html#agenda",
    "href": "news/upcoming/2025-03-13-session14.html#agenda",
    "title": "ABC.14: Accessible Bioinformatics Cafe",
    "section": "Agenda",
    "text": "Agenda\nFor this ABC we will talk about an interesting and recent package for single cell and proteomics analysis!"
  },
  {
    "objectID": "news/upcoming/2025-03-13-session14.html#signing-up",
    "href": "news/upcoming/2025-03-13-session14.html#signing-up",
    "title": "ABC.14: Accessible Bioinformatics Cafe",
    "section": "Signing up",
    "text": "Signing up\nPlease sign up to ensure there is enough cake (and you are pretty sure you will show up): click on the button below to give us a heads up of your presence. Don’t do it multiple times to cheat for extra cake!\n \n\n Sign Up!"
  },
  {
    "objectID": "tmp/2025-02-06-ABC13.html",
    "href": "tmp/2025-02-06-ABC13.html",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "",
    "text": "Today’s slides with topic presentation about local language models with Ollama (from Manuel). \n\n Download Slides"
  },
  {
    "objectID": "tmp/2025-02-06-ABC13.html#the-seurat-object",
    "href": "tmp/2025-02-06-ABC13.html#the-seurat-object",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "The seurat object",
    "text": "The seurat object\nThe Seurat object is a list that contains all the information and calculated results about the single-cell experiment. It contains the following slots:\n\nslotNames(sample)\n\n\n'assays''meta.data''active.assay''active.ident''graphs''neighbors''reductions''images''project.name''misc''version''commands''tools'\n\n\n\n\n\nSlot\nFunction\n\n\n\n\nassays\nA list of assays within this object, such as RNAseq\n\n\nmeta.data\nContains cell-level meta data\n\n\nactive.assay\nName of active, or default, assay\n\n\nactive.ident\nIdentity classes for the current object\n\n\nreductions\nA list of DimReduc objects, like PCA or UMAP\n\n\nproject.name\nUser-defined project name (optional)\n\n\n\nYou can always look at any slot using the @ operator. For example, look at what assays are in the object:\n\nsample@assays\n\n$RNA\nAssay (v5) data with 36601 features for 8583 cells\nFirst 10 features:\n MIR1302-2HG, FAM138A, OR4F5, AL627309.1, AL627309.3, AL627309.2,\nAL627309.5, AL627309.4, AP006222.2, AL732372.1 \nLayers:\n counts \n\n\nThere is so far only one assay, with the name RNA. This is the default assay at the beginning of an analysys, and it contains the count matrix only at this time. Each matrix contained in the assay is called a Layer, and you can see there is only counts.\nOther layers (matrices) that are added during an analysis are in the table below. An assay can also contain meta.features, which is a metadata table related to each gene (e.g. gene symbols, biotypes, gene ontology, …).\n\nsample@assays$RNA\n\nAssay (v5) data with 36601 features for 8583 cells\nFirst 10 features:\n MIR1302-2HG, FAM138A, OR4F5, AL627309.1, AL627309.3, AL627309.2,\nAL627309.5, AL627309.4, AP006222.2, AL732372.1 \nLayers:\n counts \n\n\n\n\n\nLayer\nFunction\n\n\n\n\ncounts\nStores unnormalized data such as raw counts or TPMs\n\n\ndata\nNormalized data matrix\n\n\nscale.data\nScaled data matrix\n\n\nmeta.features\nFeature-level meta data\n\n\n\nYou can use the command below to access a specific layer inside the assay which is currently active in the dataset.\n\nGetAssayData(object = sample, layer =\"counts\")[1:10,1:10]\n\n  [[ suppressing 10 column names ‘AAACCTGAGATCCTGT-1’, ‘AAACCTGAGCCTATGT-1’, ‘AAACCTGAGCTTTGGT-1’ ... ]]\n\n\n\n10 x 10 sparse Matrix of class \"dgCMatrix\"\n                               \nMIR1302-2HG . . . . . . . . . .\nFAM138A     . . . . . . . . . .\nOR4F5       . . . . . . . . . .\nAL627309.1  . . . . . . 1 . . .\nAL627309.3  . . . . . . . . . .\nAL627309.2  . . . . . . . . . .\nAL627309.5  . . . . . . . . . .\nAL627309.4  . . . . . . . . . .\nAP006222.2  . . . . . . . . . .\nAL732372.1  . . . . . . . . . .\n\n\nTo see the assay currently active (which means used in the analysis), you can access the slot active.assay:\n\nsample@active.assay\n\n'RNA'\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsually, the default assay changes automatically after performing a normalization, so you do not need to do things manually like this. But it is good to know how to access the slots in the Seurat object to verify the data used.\n\n\n\nMetadata\nAnother important assay is the meta.data slot, which contains cell-level metadata. This is where you can store information about each cell, such as cell type, cell cycle phase, or any other information you have about the cells. For example, we add below the percentage of mitochondrial genes in each cell. This is a common quality control metric, as high percentages of mitochondrial genes can indicate poor-quality cells.\n\nsample[[\"percent.mt\"]] &lt;- PercentageFeatureSet(sample, pattern = \"^MT-\")\n\nAgain, you can access the metadata using the @ operator:\n\nhead(sample@meta.data)\n\n\nA data.frame: 6 × 4\n\n\n\norig.ident\nnCount_RNA\nnFeature_RNA\npercent.mt\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nAAACCTGAGATCCTGT-1\nspermatogenesis\n694\n473\n5.04322767\n\n\nAAACCTGAGCCTATGT-1\nspermatogenesis\n6868\n2016\n0.04368084\n\n\nAAACCTGAGCTTTGGT-1\nspermatogenesis\n9188\n3670\n2.18763605\n\n\nAAACCTGAGGATGGTC-1\nspermatogenesis\n4021\n2175\n0.00000000\n\n\nAAACCTGAGTACGACG-1\nspermatogenesis\n3237\n1622\n1.54464010\n\n\nAAACCTGAGTACGCCC-1\nspermatogenesis\n10213\n3253\n0.29374327"
  },
  {
    "objectID": "tmp/2025-02-06-ABC13.html#filtering",
    "href": "tmp/2025-02-06-ABC13.html#filtering",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Filtering",
    "text": "Filtering\nWe filter out cells with low quality, such as cells with low gene counts or high mitochondrial gene content.\nTo have an idea of the distribution of those quality measures in each cell, we can plot them using the VlnPlot function. This function creates violin plots of the distribution of a given feature (e.g. gene counts, mitochondrial gene content) across all cells.\n\noptions(repr.plot.width = 12, repr.plot.height=6)\nVlnPlot(sample, \n        features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), \n        ncol = 3,\n        alpha = .25)\n\nWarning message:\n“Default search for \"data\" layer in \"RNA\" assay yielded no results; utilizing \"counts\" layer instead.”\n\n\n\n\n\n\n\n\n\nNote how there are some cells with very few features (detected genes with at least one transcript) and very few counts (total transcripts associated to a gene). These cells are likely to be low-quality, or empty droplets with contamination, and should be removed from the analysis. The same for the cells with too many genes and transcripts, which are likely to be doublets (two cells captured in the same droplet).\nAnother useful plot is the one giving the relationship between number of transcripts and features in each cell. usually, those two measures grow together. If not, it can be a sign of low-quality cells. We can also notice a few outliers with huge amount of transcripts, but not always as many features. These are likely to be doublets.\n\nFeatureScatter(sample, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") \n\n\n\n\n\n\n\n\nNow we can choose the thresholds for filtering. We will remove cells with less than 2000 genes and more than 6000 genes, counts below 5000 and above 30000, and more than 10% mitochondrial genes.\n\nMIN_COUNT = 5000  #minimum number of transcripts per cell\nMAX_COUNT = 30000 #maximum number of transcripts per cell\nMIN_GENES = 2000   #minimum number of genes per cell\nMAX_GENES = 6000   #maximum number of genes per cell\nMAX_MITO = 10      #mitocondrial percentage treshold)\n\nsample &lt;- subset( sample, \n                  subset = nFeature_RNA &gt; MIN_GENES & \n                           nFeature_RNA &lt; MAX_GENES &\n                           nCount_RNA &gt; MIN_COUNT & \n                           nCount_RNA &lt; MAX_COUNT &\n                           percent.mt &lt; MAX_MITO )\n\nNow the violin plot looks more uniform without a huge bottom peak, which is a good sign.\n\nVlnPlot(sample, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\nFeatureScatter(sample, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") \n\nWarning message:\n“Default search for \"data\" layer in \"RNA\" assay yielded no results; utilizing \"counts\" layer instead.”"
  },
  {
    "objectID": "tmp/2025-02-06-ABC13.html#normalization-i",
    "href": "tmp/2025-02-06-ABC13.html#normalization-i",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Normalization I",
    "text": "Normalization I\nNormalization is the process of removing technical biases from the data. The most common normalization method is the LogNormalize method, which normalizes the data by the total expression, multiplies by a scale factor (10,000 by default), and log-transforms the result. This is done by NormalizeData. Usually the data is scaled using ScaleData to have a mean of 0 and a variance of 1.\n\nsample &lt;- NormalizeData(sample, normalization.method = \"LogNormalize\")\nsample &lt;- ScaleData(sample)\n\nNormalizing layer: counts\n\nCentering and scaling data matrix\n\n\n\nNow look at the RNA assay again. You can see that - data layer has been added, which contains the normalized data - scale.data is the data layer scaled with ScaleData\n\nsample@assays\n\n$RNA\nAssay (v5) data with 36601 features for 3883 cells\nFirst 10 features:\n MIR1302-2HG, FAM138A, OR4F5, AL627309.1, AL627309.3, AL627309.2,\nAL627309.5, AL627309.4, AP006222.2, AL732372.1 \nLayers:\n counts, data, scale.data \n\n\n\n\n\n\n\n\nTip\n\n\n\nWhy do you have default assays and not default layers? When you choose a specific assay, the various algorithms in Seurat will automatically choose the correct layer in the assay. If there is no layer with the correct data, Seurat will throw an error. For example PCA will not work if you do not have a scale.data layer in the assay. All those details are in general taken care of automatically, but you can always have ?@fig-object below as a reference for all the various assays and layers, and to which Seurat functions they are used.\n\n\n\nStructure of Seurat object, with the assays and functions connected to them in the Seurat package. Further assays and functions can exist if you use additional external packages."
  },
  {
    "objectID": "tmp/2025-02-06-ABC13.html#pca-and-umap-visualizations",
    "href": "tmp/2025-02-06-ABC13.html#pca-and-umap-visualizations",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "PCA and UMAP visualizations",
    "text": "PCA and UMAP visualizations\nNow we can plot a dimensionality reduction of our data. Usually, you look for the most variable genes, which are the most meaningful on a biological level. This is done with the FindVariableFeatures function.\nWhat does it do? This function calculates the mean and dispersion of each gene, and then selects the most variable genes based on a threshold. The default is to select the top 2000 most variable genes.\n\nsample &lt;- FindVariableFeatures(sample, nfeatures = 2000)\n\nFinding variable features for layer counts\n\n\n\nIf you look at the assay, you can notice a small change, where it now shows the top variable features instead of the first ten in the expression matrix\n\nsample@assays$RNA\n\nAssay (v5) data with 36601 features for 3883 cells\nTop 10 variable features:\n PTGDS, ACTA2, TMSB4X, TAGLN, B2M, S100A6, MYL9, MGP, DCN, KRT17 \nLayers:\n counts, data, scale.data \n\n\nYou can now run and print the PCA of the data, which will be based on the variable features. The PCA is a way to reduce the dimensionality of the data, and it is useful to visualize the data in 2D or 3D. The PCA is calculated using the RunPCA function.\n\n\n\n\n\n\nTip\n\n\n\nThe first components are the most relevant to the data, and the PCA function prints the genes that influence the most each component. This is useful to understand what the components represent. For example, if one component has a long list of mitochondrial genes, it is likely to represent the mitochondrial content of the cells, which you might want to remove in the future analysis where it could cover for other biological factors that are relevant to you.\n\n\n\nsample &lt;- RunPCA(sample, features = VariableFeatures(object = sample))\nFeaturePlot(sample, reduction = 'pca', features = \"nCount_RNA\")\n\nPC_ 1 \nPositive:  HMGB4, GTSF1L, ACTL7A, TSSK2, GAPDHS, SMCP, FHL5, NUPR2, SPTY2D1OS, CAPZA3 \n       H1FNT, C10orf82, FAM71E1, RNF151, C16orf82, ACTL9, IQCF1, C2orf88, FAM71F1, SPATA19 \n       CCDC196, GNG2, C9orf24, SPERT, CCDC91, CLPB, TSSK1B, TMCO2, LELP1, AC002463.1 \nNegative:  HSP90AA1, NPM1, COX7A2L, MT-CO1, MT-ND2, PTGES3, MT-CO3, MT-ND1, MT-CO2, RPS17 \n       PARK7, MT-ATP6, CALM2, GABARAPL2, MT-ND4, BANF1, TRMT112, H2AFZ, CKS2, SNRPD1 \n       FTH1, RPS12, RPL21, HNRNPA3, EIF5, RBX1, MTX2, RPL18, SBNO1, BUD23 \nPC_ 2 \nPositive:  FTL, SOD1, NDUFA13, MIF, S100A11, S100A10, RPL10, FBL, ANXA2, CNN1 \n       RPS4Y1, LDHB, RPL36A, RPS21, PRDX1, GLUL, RBM3, RPL18A, CIRBP, RPL39 \n       RPS4X, EIF4E, SLC25A5, GPI, SLC25A6, RPL21, RPLP1, HSPB1, NME2, GSTP1 \nNegative:  LDHC, SPINK2, TMEM225B, REXO5, CCDC110, ZPBP2, CCT6B, AL133499.1, ANKRD7, PPP3R2 \n       DYDC1, CETN1, PTTG1, SPAG6, CCDC42, CLGN, LINC01016, DBF4, PBK, ETFRF1 \n       MCHR2-AS1, TBPL1, SPATA22, PPP1R1C, CT66, C15orf61, TMPRSS12, KPNA5, AC002467.1, ZC2HC1C \nPC_ 3 \nPositive:  ENG, BST2, OSR2, MGP, FOS, MYL9, JUNB, TSHZ2, TIMP3, NUPR1 \n       CAV1, SFRP4, VIM, LGALS1, CALD1, IFI6, APOD, SMOC2, BGN, ZFP36 \n       TAGLN, FBLN5, SH3BGRL, SPARC, PRDX2, DPEP1, MYH11, IGFBP7, JUN, TCEAL3 \nNegative:  SMC1B, DPEP3, LY6K, CKS2, CHIC2, CENPH, SPINT2, ZCWPW1, SYCP3, TOP2A \n       TEX101, DNMT1, PRSS21, HENMT1, GINS2, BUD23, SMC3, TPTE, HMGB2, ORC6 \n       MAJIN, SQLE, SMC4, VCX, FABP5, DMRTC2, BTG3, UNK, TEX12, MIS18A \nPC_ 4 \nPositive:  PRM1, AC016747.1, LINC01921, PRM2, LRRD1, TNP2, TNP1, LRRC8C-DT, FAM216A, GIHCG \n       C5orf58, EIF5AL1, TMED10, AC073111.1, TSSK6, CCDC179, NUF2, RANGAP1, AC079385.2, LINC01206 \n       PHOSPHO1, ESS2, LDHAL6B, CETN3, TDRG1, CAP1, AL354707.1, OTUB2, SPATA8, CUTC \nNegative:  CMC1, HMGN1, TJP3, FAM24A, C4orf17, TFDP2, DPP10-AS3, AC022784.5, ABRA, TPRG1 \n       PRSS55, C17orf98, ATP6V1E2, LINC02032, LRRC3B, AL450306.1, TMEM243, LYZL6, GPR85, CLDND2 \n       XKR3, LYZL1, SPACA1, LRRC39, LINC01988, LYZL4, AP002528.1, AFG1L, LYZL2, C17orf50 \nPC_ 5 \nPositive:  C5orf47, TEX101, ARL6IP1, SYCP1, SELENOT, TOP2A, HLA-B, LINC01120, MAJIN, HORMAD1 \n       TDRG1, ANKRD31, TPTE, LY6K, XKR9, HSP90B1, LINC00668, TEX12, DMC1, AC044839.1 \n       PRSS21, DMRTC2, SPO11, SYCP3, SCML1, C1orf146, RAD51AP2, AL138889.1, C5orf58, AC240274.1 \nNegative:  ALKAL2, ANKRD37, GYG1, TUBA1B, CSTF1, RHOXF1, AURKA, KRT18, SOX4, RGCC \n       PAFAH1B3, RABIF, EGFL7, LINC01511, SMS, AL445985.2, HMGA1, RABGGTA, CAVIN3, AC004817.2 \n       TMEM14A, ISOC2, TCF3, SIX1, CDCA7L, PPP1CA, L1TD1, RAC3, KLHL23, ASB9 \n\n\n\n\n\n\n\n\n\n\nThe plot above is coloured by the number of transcripts in each cell. Each dot is a cell. You can see that the PCA separates the cells by the number of transcripts - This is because the number of transcripts is a technical factor, and the simple normalization we did before probably did not remove for its beffect on the data.\nAnother thing to take into account is that the PCA is not the best way to visualize the data, as it is a linear transformation of the data. The UMAP is a non-linear transformation that is more suitable for single-cell data. You can run the UMAP using the RunUMAP function. But before, look at how much variance there is in each PCA component (plot below), and there choose a threshold where the value is low. Usually 10 to 15 is just fine.\n\nElbowPlot(sample)\n\n\n\n\n\n\n\n\nThen run the UMAP selecting the first ten dimensions of the PCA\n\nsample &lt;- RunUMAP(sample, dims = 1:10)\n\nWarning message:\n“The default method for RunUMAP has changed from calling Python UMAP via reticulate to the R-native UWOT using the cosine metric\nTo use Python UMAP via reticulate, set umap.method to 'umap-learn' and metric to 'correlation'\nThis message will be shown once per session”\n14:58:33 UMAP embedding parameters a = 0.9922 b = 1.112\n\n14:58:33 Read 3883 rows and found 10 numeric columns\n\n14:58:33 Using Annoy for neighbor search, n_neighbors = 30\n\n14:58:33 Building Annoy index with metric = cosine, n_trees = 50\n\n0%   10   20   30   40   50   60   70   80   90   100%\n\n[----|----|----|----|----|----|----|----|----|----|\n\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n|\n\n14:58:33 Writing NN index file to temp file /tmp/RtmphwjaOt/file1efcb5fdb2ff7\n\n14:58:33 Searching Annoy index using 1 thread, search_k = 3000\n\n14:58:34 Annoy recall = 100%\n\n14:58:34 Commencing smooth kNN distance calibration using 1 thread\n with target n_neighbors = 30\n\n14:58:35 Initializing from normalized Laplacian + noise (using RSpectra)\n\n14:58:35 Commencing optimization for 500 epochs, with 148538 positive edges\n\n14:58:38 Optimization finished\n\n\n\nPretty! We can visualize some know markers, AKAP4 for maturing spermatozoa and PIWIL1 for meiosis. You can see that the cells are separated by the expression of these genes. This is a good sign that the clustering will work well, and that the UMAP represents biological variation. Since UMAP is based on PCA, the PCA was also a good representation of the data and not only biased by the number of transcripts as technical factor.\n\nFeaturePlot(sample, reduction = \"umap\", features = c(\"AKAP4\",\"PIWIL1\"))"
  },
  {
    "objectID": "tmp/2025-02-06-ABC13.html#normalization-ii",
    "href": "tmp/2025-02-06-ABC13.html#normalization-ii",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Normalization II",
    "text": "Normalization II\nNow, one might want a better normalization method. The one we used before is very simple and does not take into account the technical factors that can be present in the data. For example, we might have found out that the PCA was completely biased by cell cycle markers, and we want to remove this bias. This is done by the SCTransform function, which is a more advanced normalization method that takes into account technical factors in the data. Below we do not insert technical factors, but you can do it by adding them in the vars.to.regress argument. Try to remove the effect of mitochondrial content using vars.to.regress = \"percent.mito\" in the command below (we noticed after all some mitochondrial genes in the first PCA component). One can add also multiple factors, like vars.to.regress = c(\"percent.mito\", \"cell.cycle\").\n\nsample &lt;- SCTransform(sample)\n\nRunning SCTransform on assay: RNA\n\nRunning SCTransform on layer: counts\n\nvst.flavor='v2' set. Using model with fixed slope and excluding poisson genes.\n\nVariance stabilizing transformation of count matrix of size 26386 by 3883\n\nModel formula is y ~ log_umi\n\nGet Negative Binomial regression parameters per gene\n\nUsing 2000 genes, 3883 cells\n\nFound 12 outliers - those will be ignored in fitting/regularization step\n\n\nSecond step: Get residuals using fitted parameters for 26386 genes\n\nComputing corrected count matrix for 26386 genes\n\nCalculating gene attributes\n\nWall clock passed: Time difference of 48.80969 secs\n\nDetermine variable features\n\nCentering data matrix\n\nSet default assay to SCT\n\n\n\nLet’s calculate PCA and UMAP again\n\nsample &lt;- RunPCA(sample, features = VariableFeatures(object = sample))\n\nPC_ 1 \nPositive:  MT-CO2, MT-ND4, MT-CO1, MT-CO3, MT-CYB, RPL21, PTMA, RPL18A, HIST1H4C, MT-ATP6 \n       MALAT1, MT-ND1, HSP90AA1, RPS2, GAGE2A, SMC3, RPSA, CCNI, RPLP1, BUD23 \n       FTH1, RPL10, DNAJB6, RPS12, RPS19, PRDX1, CIRBP, HMGB1, NCL, RNPS1 \nNegative:  HMGB4, TNP1, PRM1, PRM2, LINC01921, CRISP2, SMCP, LINC00919, GTSF1L, LELP1 \n       RNF151, NUPR2, TSACC, C10orf62, TSSK2, GAPDHS, FAM209A, OAZ3, TEX37, CAPZA3 \n       CMTM2, FAM71A, AC002463.1, CATSPERZ, TEX38, C20orf141, FAM71B, TSPAN16, SH3RF2, IRGC \nPC_ 2 \nPositive:  AL133499.1, LDHC, ANKRD7, SPATA8, SPINK2, LYAR, RBAKDN, SPATA22, CCDC42, COPRS \n       ZMYND10, CAVIN3, CCNA1, GIHCG, PPP3R2, CLGN, GTF2A2, ZPBP2, REXO5, TBPL1 \n       MRPL34, H2AFJ, PTTG1, AC004817.2, TMEM225B, CATSPERZ, UBB, APH1B, COX7A2, CCDC110 \nNegative:  PRM2, PRM1, TNP1, HMGB4, LINC01921, LINC00919, LELP1, GLUL, TEX37, FAM71B \n       FAM71A, C10orf62, RNF151, OAZ3, HEMGN, C20orf141, NFKBIB, CEP170, PRSS58, TSPAN16 \n       SPATA42, RPL18A, SH3RF2, TEX44, AC002463.1, IQCF2, OTUB2, SPATA3, TSSK6, RPL10 \nPC_ 3 \nPositive:  PRM1, PRM2, TNP1, TEX101, SYCP3, FMR1NB, LINC01921, LY6K, SMC3, HIST1H4C \n       SMC1B, C5orf58, DPEP3, HMGB2, SYCP1, HORMAD1, TPTE, HMGB4, TOP2A, CALM2 \n       LINC00919, SPINT2, BUD23, ZCWPW1, TDRG1, TEX12, HSP90AA1, CKS2, MAD2L1, NCL \nNegative:  RPS4X, VIM, RPL10, MGP, TMSB4X, MYL9, FOS, LGALS1, CALD1, PTGDS \n       TIMP1, B2M, IFITM3, SPARCL1, OSR2, RPL39, FTL, NUPR1, ENG, BEX3 \n       RPL13A, TPM2, CD63, BST2, DCN, ACTA2, MYL6, CAV1, EEF1A1, JUNB \nPC_ 4 \nPositive:  EGFL7, PAFAH1B3, SMS, RHOXF1, CCNI, RPS12, RPSA, SOX4, LYPLA1, GAGE2A \n       ASB9, TUBA1B, DNAJB6, ZNF428, HMGA1, PPP1CA, C17orf49, YWHAB, RPS2, LINC01511 \n       KRT18, RPS19, AL445985.2, NEIL2, MAGEB2, LIN7B, RNPS1, MAGEA4, TMEM14A, LSM2 \nNegative:  TEX101, MALAT1, CALM2, C5orf58, SYCP3, SELENOT, PTGDS, B2M, MGP, CTSL \n       SYCP1, LY6K, FOS, MYL9, FHL2, TDRG1, HORMAD1, LGALS1, TPTE, TIMP1 \n       SPARCL1, IFITM3, CALD1, DPEP1, BST2, DCN, ENG, NEAT1, OSR2, NUPR1 \nPC_ 5 \nPositive:  PRM2, PRM1, TNP1, LINC01921, SPATA8, AL133499.1, MRPL34, GIHCG, LYAR, TNP2 \n       COPRS, TEX37, SPINK2, GLUL, TSSK6, ISOC2, ZMYND10, FAM71B, CAVIN3, RGCC \n       H2AFZ, AC004817.2, CCNA1, HEMGN, AKAP12, RAN, OTUB2, TEX36-AS1, MT-CO1, ESS2 \nNegative:  SPACA1, EQTN, TJP3, ACRV1, GOLGA6L2, ERICH2, FAM209A, LYZL1, SPACA3, OLAH \n       FAM209B, HMGN1, LYZL2, ACTRT3, FNDC11, FAM24A, PEX5L-AS2, C17orf98, PRSS55, SPACA7 \n       LYZL6, TEX38, SPACA4, LYZL4, DPEP3, FMR1NB, BRI3, SMC3, TMEM190, C17orf50 \n\n\n\n\nElbowPlot(sample)\n\n\n\n\n\n\n\n\n\nsample &lt;- RunUMAP(sample, dims = 1:15)\n\n14:59:41 UMAP embedding parameters a = 0.9922 b = 1.112\n\n14:59:41 Read 3883 rows and found 15 numeric columns\n\n14:59:41 Using Annoy for neighbor search, n_neighbors = 30\n\n14:59:41 Building Annoy index with metric = cosine, n_trees = 50\n\n0%   10   20   30   40   50   60   70   80   90   100%\n\n[----|----|----|----|----|----|----|----|----|----|\n\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n|\n\n14:59:42 Writing NN index file to temp file /tmp/RtmphwjaOt/file1efcb66a5aeb3\n\n14:59:42 Searching Annoy index using 1 thread, search_k = 3000\n\n14:59:43 Annoy recall = 100%\n\n14:59:44 Commencing smooth kNN distance calibration using 1 thread\n with target n_neighbors = 30\n\n14:59:46 Initializing from normalized Laplacian + noise (using RSpectra)\n\n14:59:46 Commencing optimization for 500 epochs, with 147944 positive edges\n\n14:59:49 Optimization finished\n\n\n\nhere it is. Similar to what we had before (this is very nice and unbiased data)\n\nFeaturePlot(sample, reduction = \"umap\", features = c(\"AKAP4\",\"PIWIL1\"))\n\n\n\n\n\n\n\n\n\nPost-normalization Seurat object\nLook at the seurat object now. You can see there is a new active assay, called SCT (as in scTransform). This is the new normalization. counts is the count matrix corrected for biases by the SCTransform statistical model, and scale.data is the scaled data. data is the corrected matrix counts, normalized but not scaled.\nNote also that PCA and UMAP are saved in the assay.\n\nsample\n\nAn object of class Seurat \n62987 features across 3883 samples within 2 assays \nActive assay: SCT (26386 features, 3000 variable features)\n 3 layers present: counts, data, scale.data\n 1 other assay present: RNA\n 2 dimensional reductions calculated: pca, umap\n\n\nYou can also use slotNames to see an overview of the slots in the object. Try to look back at ?@fig-object to see how things match.\n\nslotNames(sample@assays$SCT)\n\n\n'SCTModel.list''counts''data''scale.data''assay.orig''var.features''meta.features''misc''key'"
  },
  {
    "objectID": "tmp/2025-02-06-ABC13.html#clustering",
    "href": "tmp/2025-02-06-ABC13.html#clustering",
    "title": "ABC.11: Ollama for LLMs on your laptop, Introduction to Seurat, and scRNA-seq preprocessing",
    "section": "Clustering",
    "text": "Clustering\nNow we go straight into clustering. Now it is very popular to pull cluster labels from large cell atlases, but it is good to know how to do it manually by looking at markers. First of all we calculate cell similarities (neighbors) and a clustering. We choose a small resolution parameters to avoid a too fine-grained clustering inthe tutorial. Your clusters might be looking different than in this tutorial!\n\nsample &lt;- FindNeighbors(sample, dims = 1:15)\nsample &lt;- FindClusters(sample, resolution = 0.3)\n\nComputing nearest neighbor graph\n\nComputing SNN\n\n\n\nModularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n\nNumber of nodes: 3883\nNumber of edges: 113438\n\nRunning Louvain algorithm...\nMaximum modularity in 10 random starts: 0.9486\nNumber of communities: 11\nElapsed time: 0 seconds\n\n\nNotice how cluster numbers are now assigned to the active.ident assay, unused until now.\n\nsample@active.ident[1:10]\n\nAAACCTGAGCCTATGT-16AAACCTGAGCTTTGGT-14AAACCTGAGTACGCCC-17AAACCTGAGTGAATTG-15AAACCTGCAGAGCCAA-15AAACCTGGTAGGGTAC-10AAACCTGGTCAGAAGC-19AAACCTGGTCGTGGCT-13AAACCTGTCTCTGTCG-10AAACGGGAGATGTGGC-14\n\n\n    \n        Levels:\n    \n    \n    '0''1''2''3''4''5''6''7''8''9''10'\n\n\n\nYou can plot the clusters as well on UMAP (or PCA)\n\nDimPlot(sample, reduction = 'umap', label=TRUE, label.size = 10)\n\n\n\n\n\n\n\n\nNow let’s look to some broad markers categories for spermatogenesis. This step requires biological knowledge of your data, and you can look at the literature or other resources to find the markers. For example, we can look at markers for spermatogonia, spermatocytes, and spermatids.\n\ncat(\"Spermatogonia Markers\\n\")\nFeaturePlot(sample, features = c('MKI67','DMRT1','STRA8','ID4'))\n\nSpermatogonia Markers\n\n\n\n\n\n\n\n\n\n\ncat(\"Spermatocytes Markers\\n\")\nFeaturePlot(sample, features = c('MEIOB','SYCP1','TEX101','PIWIL1','SPATA16','CLGN'))\n\nSpermatocytes Markers\n\n\n\n\n\n\n\n\n\n\ncat(\"Spermatid Markers\\n\")\nFeaturePlot(sample, features = c('PRM1','PRM2','PRM3','AKAP4','SPATA9','SPAM1'))\n\nSpermatid Markers\n\n\n\n\n\n\n\n\n\nWe also look for some somatic markers\n\ncat(\"Somatic Markers\\n\")\nFeaturePlot(sample, features = c('VIM','CTSL'))\n\nSomatic Markers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how the somatic marker CTSL also ends where we might have another cell type. This happens often as there are markers involved in multiple biological processes! This is why it is important to look at multiple markers and not just one.\n\n\n\nRenaming clusters\nNow, we can choose to rename the clusters to more meaningful names. Simply create a named vector with the old and new names separated by :.\n\n\n\n\n\n\nWarning\n\n\n\nFill out the example vector below yourself!\n\n\n\nnew_names = c(\n    '0'='clusterA',\n    '1'='clusterB',\n    '2'='clusterC')\n\nBelow, you substitute the cluster names with the new names. You can also use the RenameIdents function to rename the clusters.\n\nsample@meta.data$new_clusters = recode(Idents(sample), !!!new_names)\nsample@active.ident = as.factor(sample@meta.data$new_clusters)\nnames(sample@active.ident) = colnames(sample)\n\nNow you are ready to look at your clustering!\n\nDimPlot(sample, reduction = 'umap', label=TRUE, label.size = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNow you are ready to go on with your analysis. You can look at differential expression, cell cycle, and other analyses. Those will come in a second tutorial of the ABC, so keep your eyes on our documentation.\nIn the meanwhile, you can try further tutorials on the Seurat website, or look at the Seurat vignettes for more advanced analyses.\nWe at the ABC also work with single cell analysis regularly, so do not hesitate to ask us for help!"
  }
]